# Augmented Reality-basierte Steuerung von Smart-Home-Ger√§ten √ºber openHAB auf Android

## Kurzbeschreibung:

Im Rahmen dieser Bachelorthesis soll eine Android-App entwickelt werden, die mithilfe von Augmented Reality (AR) Smart-Home-Ger√§te aus openHAB erkennt und interaktiv steuern kann. Die Ger√§te werden im Kamerabild identifiziert und mit einem √ºberlagerten 3D-Interface versehen. √úber dieses Interface soll der Benutzer Aktionen wie Ein-/Ausschalten, Werte einstellen oder Zust√§nde abfragen k√∂nnen. Zudem soll eine Konfigurationsoberfl√§che implementiert werden, um die Verbindung zu openHAB √ºber die REST-API herzustellen und neue Ger√§te mit Referenzbildern manuell einzupflegen.

## Ziele der Arbeit:

* Entwicklung einer Android-App mit AR-Funktionalit√§t
* Integration einer REST-Schnittstelle zu openHAB
* Visualisierung von Ger√§ten mittels 3D-Modellen und Interaktionsmen√ºs im Kamerabild
* Konfigurationsmen√ºs zum Hinzuf√ºgen neuer Ger√§te (per Fotos) und Verwaltung der REST-Verbindung zu openHAB
* Bilderkennung zum Abgleich von Ger√§ten im Raum

## Anforderungen

### Funktionale Anforderungen:

1. Ger√§teerkennung √ºber Kamera:
  * Bildbasierte Erkennung von physischen Ger√§ten
  * Zuordnung zu einem bestimmten openHAB-Item anhand gespeicherter Bilder
2. Augmented Reality UI:
  * Anzeige eines 3D-Objekts (z.‚ÄØB. Schalter, Regler) √ºber dem erkannten Ger√§t
  * √ñffnen eines Men√ºs beim Tippen auf das AR-Objekt
3. openHAB-Anbindung:
  * Konfiguration der REST-API-Adresse
  * Kommunikation mit openHAB zur Steuerung (Commands) und Statusabfrage (States)
4. Konfigurationsmodul:
  * Hinzuf√ºgen neuer Ger√§te
  * Zuweisung von Bildern zur sp√§teren Erkennung
  * Mapping zum openHAB-Itemnamen

### Nicht-funktionale Anforderungen:

* Mobile-first Design (Android)
* AR-Erkennung soll performant im Innenraum arbeiten
* Erweiterbarkeit durch neue Ger√§tetypen

## Technologien & Frameworks (Auswahl):

### üì± AR Frameworks:

* ARCore (Google)
  * Android-nativ
  * Gute Erkennung & Tracking-F√§higkeiten
  * Unterst√ºtzung f√ºr Augmented Images
* Unity + Vuforia
  * Plattform√ºbergreifend, visuell m√§chtig
  * Leichtes Hinzuf√ºgen von 3D-Objekten und UI
  * Gute Unterst√ºtzung f√ºr Marker-basierte Erkennung
* 8thWall (WebAR, optional)
  * AR direkt im Browser
  * K√∂nnte f√ºr prototypisches Web-Frontend genutzt werden

## üåê Smart-Home Integration:

* openHAB REST API
  * JSON-basierte API
  * Authentifizierung beachten (evtl. mit Tokens oder Basic Auth)

## üß† Bilderkennung & ML (optional):

* Firebase ML Kit
  * Offline-Bilderkennung m√∂glich
* TensorFlow Lite
  * F√ºr fortgeschrittenes Matching von Ger√§ten
* OpenCV
  * Klassische Bilderkennung (z.‚ÄØB. Template Matching)

## üß∞ Weitere Technologien:

* Kotlin / Java ‚Äì Native Android-Entwicklung
* Unity C# ‚Äì Bei Unity-basiertem Ansatz
* Jetpack Compose ‚Äì F√ºr modernes Android-UI
* Room / SQLite ‚Äì Lokale Persistenz von Ger√§te- und Konfigurationsdaten
* Retrofit / Volley ‚Äì F√ºr REST-Kommunikation mit openHAB

## üìä Schaubild: System√ºbersicht

<img src="https://raw.githubusercontent.com/Michdo93/SmartHome-Ideen/refs/heads/main/screenshots/ar_openhab.png" alt="ar_openhab" width="50%">

## Systemaufbau

### openHAB Config UI

Eine einfache View, in der man die notwendigen Daten f√ºr die REST API von openHAB speichert. Man muss w√§hlen k√∂nnen zwischen `Basic Authentication` oder `API Token`:

#### Basic Authentication

* Benutzername der openHAB-Instanz
* Passwort der openHAB-Instanz
* URL der openHAB-Instanz

#### API Token

* API-Token der openHAB-Instanz
* URL der openHAB-Instanz

##### Beispielhafte Implementierung

Die Konfigurations-UI ist ein zentraler Bestandteil deiner App, da sie die Kommunikation mit der openHAB-Instanz steuert.

1. Eine **UI-View** in Jetpack Compose (modernes Android UI)
2. Die dazugeh√∂rige **State-Verwaltung**
3. Die **Modellierung der Konfiguration**
4. Eine **Beispiel-Implementierung**, wie du den Header f√ºr REST-Anfragen erzeugst

---

###### üß± 1. Datenmodell

```kotlin
data class OpenHabConfig(
    val url: String = "",
    val username: String = "",
    val password: String = "",
    val apiToken: String = "",
    val useApiToken: Boolean = false
)
```

---

###### üé® 2. Jetpack Compose UI

```kotlin
@Composable
fun OpenHabConfigScreen(
    config: OpenHabConfig,
    onConfigChanged: (OpenHabConfig) -> Unit
) {
    var url by remember { mutableStateOf(config.url) }
    var username by remember { mutableStateOf(config.username) }
    var password by remember { mutableStateOf(config.password) }
    var apiToken by remember { mutableStateOf(config.apiToken) }
    var useApiToken by remember { mutableStateOf(config.useApiToken) }

    Column(modifier = Modifier.padding(16.dp)) {
        Text("Authentifizierungsmethode", style = MaterialTheme.typography.titleMedium)

        Row(verticalAlignment = Alignment.CenterVertically) {
            RadioButton(
                selected = !useApiToken,
                onClick = {
                    useApiToken = false
                    onConfigChanged(
                        config.copy(useApiToken = false)
                    )
                }
            )
            Text("Basic Auth")

            Spacer(modifier = Modifier.width(16.dp))

            RadioButton(
                selected = useApiToken,
                onClick = {
                    useApiToken = true
                    onConfigChanged(
                        config.copy(useApiToken = true)
                    )
                }
            )
            Text("API Token")
        }

        Spacer(modifier = Modifier.height(16.dp))

        OutlinedTextField(
            value = url,
            onValueChange = {
                url = it
                onConfigChanged(config.copy(url = url))
            },
            label = { Text("URL der openHAB-Instanz") },
            singleLine = true,
            modifier = Modifier.fillMaxWidth()
        )

        if (useApiToken) {
            OutlinedTextField(
                value = apiToken,
                onValueChange = {
                    apiToken = it
                    onConfigChanged(config.copy(apiToken = apiToken))
                },
                label = { Text("API Token") },
                singleLine = true,
                modifier = Modifier.fillMaxWidth()
            )
        } else {
            OutlinedTextField(
                value = username,
                onValueChange = {
                    username = it
                    onConfigChanged(config.copy(username = username))
                },
                label = { Text("Benutzername") },
                singleLine = true,
                modifier = Modifier.fillMaxWidth()
            )
            OutlinedTextField(
                value = password,
                onValueChange = {
                    password = it
                    onConfigChanged(config.copy(password = password))
                },
                label = { Text("Passwort") },
                singleLine = true,
                visualTransformation = PasswordVisualTransformation(),
                modifier = Modifier.fillMaxWidth()
            )
        }
    }
}
```

---

###### üîë 3. Header-Generierung f√ºr REST-Anfragen

Nutze diese Funktion, um den passenden Auth-Header abh√§ngig von der gew√§hlten Methode zu erzeugen:

```kotlin
fun getAuthHeader(config: OpenHabConfig): Map<String, String> {
    return if (config.useApiToken) {
        mapOf("Authorization" to "Bearer ${config.apiToken}")
    } else {
        val credentials = "${config.username}:${config.password}"
        val encoded = Base64.encodeToString(credentials.toByteArray(), Base64.NO_WRAP)
        mapOf("Authorization" to "Basic $encoded")
    }
}
```

---

###### üíæ 4. Speicherung in `SharedPreferences` oder Room

Du kannst diese Konfiguration entweder in Room speichern (z.‚ÄØB. als Singleton-Tabelle) **oder einfacher √ºber SharedPreferences**:

```kotlin
fun saveOpenHabConfig(context: Context, config: OpenHabConfig) {
    val prefs = context.getSharedPreferences("openhab_prefs", Context.MODE_PRIVATE)
    prefs.edit().apply {
        putString("url", config.url)
        putString("username", config.username)
        putString("password", config.password)
        putString("apiToken", config.apiToken)
        putBoolean("useApiToken", config.useApiToken)
        apply()
    }
}

fun loadOpenHabConfig(context: Context): OpenHabConfig {
    val prefs = context.getSharedPreferences("openhab_prefs", Context.MODE_PRIVATE)
    return OpenHabConfig(
        url = prefs.getString("url", "") ?: "",
        username = prefs.getString("username", "") ?: "",
        password = prefs.getString("password", "") ?: "",
        apiToken = prefs.getString("apiToken", "") ?: "",
        useApiToken = prefs.getBoolean("useApiToken", false)
    )
}
```

---

###### ‚úÖ Ergebnis

Mit dieser L√∂sung kannst du:

* Zwischen Basic Auth und API Token umschalten
* Die openHAB-Zugangsdaten sicher verwalten
* Diese Daten zur Authentifizierung bei REST-Calls verwenden

### AR-Szene mit Kamerastream und 3D UI-Overlay mit Men√º

Eine View, in der man sieht, was die Kamera des Smartphones zeigt. Sobald man √ºber ein 3D-Objekt f√§hrt, welches bedient werden kann, √∂ffnet sich in dieser View ein Frame mit der Bedienung zu diesem Ger√§t. Das 3D-Objekt kann bspw. mit Unity erstellt sein und durch die Anbindung von Vuforia √ºber ein Mapping erkannt werden. Damit dies funktioniert, m√ºsste man die trainierte Datenbank von Vuforia bei neu hinzugef√ºgten Ger√§te ebenfalls aktualisieren. Ebenfalls zu √ºberlegen ist, dass f√ºr die App grundlegend fertige 3D-Objekte in Unity schon vorgefertigt sind, damit wenn ein Ger√§t auch erkannt wird, dieses 3D-Objekt angezeigt werden kann. In der Ger√§tedatenbank m√ºsste man dann diesem Ger√§t ein 3D-Objekt und Bilder zur Erkennung im Kamerastream hinzuf√ºgen. Als Alternative kann bspw. auch ARCore verwendet werden.

In der AR-Szene mit Kamerastream soll haupts√§chlich ein Ger√§tematching stattfinden. Das bedeutet, die Kamera muss einen Vergleich mit der Ger√§tedatenbank machen. Wenn ein Ger√§t zugeordnet werden kann, dann kann √ºber den dort ebenfalls gespeicherten Itemnamen die openHAB REST API den Status von Items dieses Ger√§tes anzeigen und es gleichzeitig erm√∂glichen, Commands an Items dieses Ger√§tes zu senden. Am besten fragt man das Group-Item ab oder gibt vor, dass nur ein Itemname gespeichert werden kann und dass dieses Item vom Item Type Group sein muss.

√úber die REST-API erh√§lt man au√üerdem dann zu jedem Item in dieser Group, deren Item Types, deren State Description und deren Command Description. Anhand dieser Informationen l√§sst sich ein Fragment erzeugen, welches entsprechend den Item-State wiedergibt und die Command-Bedienm√∂glichkeiten bereit stellt.

Wichtige f√ºr die Bedienung von openHAB Items ist, dass die Item Types immer gleich bedient werden k√∂nnen. Hei√üt hier kann man auch Vorlagen (Templates) und Klassen anlegen, die man dann wieder verwendet.

### Ger√§tedatenbank

Klassischerweise implementiert man hier `CRUD`-Operationen

* CREATE
  * Man hat ein Formular, bei dem man den Namen des Ger√§ts angibt und den Itemnamen f√ºr das openHAB Group Item.
  * Man hat je Ger√§t eine eigene Galerie an Bilder.
    * Ich muss mindestens ein Bild zu jedem Ger√§t schie√üen.
    * Vielleicht macht irgendwo ein Limit von 4-6 Bilder Sinn.
    * Die Bilder m√ºssen f√ºr ein Training verwendet werden k√∂nnen.
* READ
  * Wird beim Ger√§tematching in der AR-Szene ben√∂tigt.
* UPDATE
  * Man kann jederzeit zu dem Ger√§t den Namen anpassen oder auch den Itemnamen √§ndern.
  * Man kann jederzeit zu dem Ger√§t Bilder wieder l√∂schen, neue hinzuf√ºgen oder "ersetzen". 
* DELETE
  * Man kann Ger√§te auch wieder komplett l√∂schen.
    * Hei√üt sowohl die Bilder, als auch der Name und Itemname verschwinden aus der Datenbank. 

Wozu ben√∂tigt man alle CRUD-Methoden?

* In deinem Smart Home kann man immer wieder mal alte Ger√§te durch neuere ersetzen.
* In deinem Smart Home kann man die Namensgebungsstruktur seiner Items anpassen.
* In deinem Smart Home kannst du neue Ger√§te hinzuf√ºgen.
* In deinem Smart Home kannst du alte Ger√§te entfernen, ohne sie durch neuere zu ersetzen.
* Du kannst versehentlich dich auch mal bei einem Ger√§t vertan haben und das falsche fotografiert haben bzw. bei einem fotografierten Ger√§t gedacht haben, dass dessen Name anders sei (z. B. mehrere gleiche Lampen).
* ...

Ein Smart Home System bleibt selten konstant. Man richtet es ja nicht nur ein einziges mal ein und √ºber 20-30 Jahren sind alle Ger√§te gleich!

Das Speichern der Bilder f√ºr deine Ger√§tedatenbank ist ein zentraler Aspekt deines Projekts ‚Äì vor allem, wenn du Ger√§te durch Bildabgleich erkennen willst. Hier ist ein √úberblick √ºber m√∂gliche Optionen, deren Vor- und Nachteile sowie ein Vorschlag f√ºr den Aufbau deiner Ger√§tedatenbank inklusive Bildspeicherung.

#### üì¶ **Wie und wo die Bilder gespeichert werden k√∂nnen**

##### üü¢ **1. Speicherung in SQLite mit Pfad-Referenz (empfohlen)**

**Vorgehen:**

* Du speicherst die Bilder **als Dateien im internen Speicher oder App-spezifischen Speicher** (z.‚ÄØB. `/data/data/<package>/files/devices/`).
* In der SQLite-Datenbank speicherst du **nur den Pfad zum Bild**, zusammen mit anderen Ger√§tedaten.

**Vorteile:**

* Geringere Datenbankgr√∂√üe
* Schnellere Zugriffszeiten
* Einfach zu verwalten und backupf√§hig
* Gute Integration in bestehende Android-Architektur

**Nachteile:**

* Zus√§tzliche File-Management-Logik notwendig

---

##### üî¥ **2. Speicherung direkt als BLOB in SQLite**

**Vorgehen:**

* Das Bild (z.‚ÄØB. JPEG oder PNG) wird als **Byte-Array** direkt in einem BLOB-Feld in SQLite gespeichert.

**Vorteile:**

* Alles in einer Datei (praktisch f√ºr kleine Datenmengen)
* Kein extra File-System-Handling

**Nachteile:**

* Datenbank kann sehr gro√ü und tr√§ge werden
* L√§ngere Ladezeiten bei vielen Ger√§ten/Bildern

‚û° **Nur sinnvoll f√ºr sehr kleine Bildmengen oder Prototyping**

---

##### üü° **3. Speicherung √ºber ContentProvider oder MediaStore (optional/fortgeschritten)**

* F√ºr √∂ffentlich zug√§ngliche Bilder (z.‚ÄØB. wenn du willst, dass der User sie auch in seiner Galerie sieht)
* Aufw√§ndiger und meist nicht notwendig, wenn alles in der App bleibt

---

#### üß© **Vorgeschlagene Datenbankstruktur (SQLite mit Pfad zu Bilddateien)**

```sql
Tabelle: devices

| id  | name         | openhab_item_id | image_paths                     |
|-----|--------------|------------------|---------------------------------|
| 1   | Wohnzimmerlampe | Switch_Light_1 | /files/devices/lamp1_1.jpg;... |
```

**Feldbeschreibung:**

* `id`: Prim√§rschl√ºssel
* `name`: Anzeigename des Ger√§ts
* `openhab_item_id`: Name des Items in der openHAB REST-API
* `image_paths`: Semikolon-getrennte Liste von Pfaden zu gespeicherten Bildern (alternativ: eigene Tabelle f√ºr Bilder)

---

#### üîÑ **Alternative: Normalisierte Struktur**

Falls du mehrere Bilder je Ger√§t hast und du sauber bleiben willst:

```sql
Tabelle: devices
| id  | name         | openhab_item_id |
|-----|--------------|------------------|
| 1   | Wohnzimmerlampe | Switch_Light_1 |

Tabelle: device_images
| id  | device_id | image_path                    |
|-----|-----------|-------------------------------|
| 1   | 1         | /files/devices/lamp1_1.jpg    |
| 2   | 1         | /files/devices/lamp1_2.jpg    |
```

‚û° Diese Struktur ist besser skalierbar und sauberer im Zugriff.

---

#### üîß **Wie speicherst du ein Bild in Kotlin/Android?**

##### Speichern eines Bildes im Dateisystem:

```kotlin
fun saveBitmapToInternalStorage(context: Context, bitmap: Bitmap, filename: String): String {
    val file = File(context.filesDir, filename)
    FileOutputStream(file).use { out ->
        bitmap.compress(Bitmap.CompressFormat.PNG, 100, out)
    }
    return file.absolutePath
}
```

##### Speichern des Pfads in SQLite:

Mit `Room` oder klassisch mit `SQLiteOpenHelper`.

---

#### üìå Empfehlung f√ºr dein Projekt

| Kriterium               | Empfehlung                              |
| ----------------------- | --------------------------------------- |
| Anzahl Bilder < 50      | SQLite mit Pfad oder sogar BLOB m√∂glich |
| Anzahl Bilder > 50‚Äì100+ | SQLite mit Pfad & File-System           |
| Zukunftssicherheit      | Normalisierte Struktur mit Bildtabelle  |
| Performance wichtig     | Pfadbasierte Speicherung                |

---

Gerne! Hier ist ein vollst√§ndiges **Room-Datenbankmodell** f√ºr dein AR/openHAB-Projekt mit Unterst√ºtzung f√ºr:

* Ger√§teverwaltung
* Mehrere Bilder pro Ger√§t (Referenzbilder)
* Speicherung der Bildpfade im Dateisystem

---

#### üì¶ **1. Datenbankstruktur (Room)**

##### üóÇ `DeviceEntity.kt`

```kotlin
import androidx.room.Entity
import androidx.room.PrimaryKey

@Entity(tableName = "devices")
data class DeviceEntity(
    @PrimaryKey(autoGenerate = true) val id: Long = 0,
    val name: String,
    val openhabItemId: String
)
```

---

##### üñº `DeviceImageEntity.kt`

```kotlin
import androidx.room.Entity
import androidx.room.ForeignKey
import androidx.room.PrimaryKey

@Entity(
    tableName = "device_images",
    foreignKeys = [ForeignKey(
        entity = DeviceEntity::class,
        parentColumns = ["id"],
        childColumns = ["deviceId"],
        onDelete = ForeignKey.CASCADE
    )]
)
data class DeviceImageEntity(
    @PrimaryKey(autoGenerate = true) val id: Long = 0,
    val deviceId: Long,
    val imagePath: String // z.‚ÄØB. /data/data/.../files/devices/lamp1_1.jpg
)
```

---

#### üîÅ **2. DAO-Interfaces**

##### `DeviceDao.kt`

```kotlin
import androidx.room.*
import kotlinx.coroutines.flow.Flow

@Dao
interface DeviceDao {

    @Insert(onConflict = OnConflictStrategy.REPLACE)
    suspend fun insertDevice(device: DeviceEntity): Long

    @Query("SELECT * FROM devices")
    fun getAllDevices(): Flow<List<DeviceEntity>>

    @Query("SELECT * FROM devices WHERE id = :id")
    suspend fun getDeviceById(id: Long): DeviceEntity?
}
```

---

##### `DeviceImageDao.kt`

```kotlin
import androidx.room.*

@Dao
interface DeviceImageDao {

    @Insert(onConflict = OnConflictStrategy.REPLACE)
    suspend fun insertImage(image: DeviceImageEntity): Long

    @Query("SELECT * FROM device_images WHERE deviceId = :deviceId")
    suspend fun getImagesForDevice(deviceId: Long): List<DeviceImageEntity>

    @Delete
    suspend fun deleteImage(image: DeviceImageEntity)
}
```

---

#### üß© **3. Datenbank-Klasse**

##### `AppDatabase.kt`

```kotlin
import androidx.room.Database
import androidx.room.RoomDatabase

@Database(entities = [DeviceEntity::class, DeviceImageEntity::class], version = 1)
abstract class AppDatabase : RoomDatabase() {
    abstract fun deviceDao(): DeviceDao
    abstract fun deviceImageDao(): DeviceImageDao
}
```

---

#### üöÄ **4. Nutzung in deiner App (z.‚ÄØB. ViewModel oder Repository)**

```kotlin
val db = Room.databaseBuilder(
    context,
    AppDatabase::class.java,
    "device_database"
).build()

val newDevice = DeviceEntity(name = "Wohnzimmerlampe", openhabItemId = "Light_Livingroom")
val deviceId = db.deviceDao().insertDevice(newDevice)

val imagePath = saveBitmapToInternalStorage(context, bitmap, "lamp1_1.jpg")
db.deviceImageDao().insertImage(DeviceImageEntity(deviceId = deviceId, imagePath = imagePath))
```

---

#### ‚úÖ Vorteile dieser L√∂sung

* Skalierbar: Beliebig viele Bilder pro Ger√§t
* Sicher: Ger√§te und Bilder sind logisch verkn√ºpft
* Kompatibel: Ideal f√ºr AR mit Bildvergleich
* Persistenz: Einfach zu sichern oder exportieren

---

## M√∂gliche AR-Vorgehensweise

### Unity + Vuforia

Wenn du **Unity + Vuforia** verwenden m√∂chtest, um eine **AR-App f√ºr openHAB-Ger√§testeuerung** zu bauen (basierend auf Bild-Tracking + REST-Steuerung), ist hier ein kompletter √úberblick, wie du schrittweise vorgehen solltest:

---

#### üß© **1. Voraussetzungen**

* **Unity Hub** installiert
* **Unity Version** (z.‚ÄØB. 2021.3 LTS oder 2022.x) mit Android Build Support
* **Vuforia Engine** (kostenlos, ben√∂tigt Developer License Key)
* Android-Smartphone zum Testen
* openHAB-Instanz mit aktivierter REST-API (Standard bei openHAB 2/3/4)

---

#### üèóÔ∏è **2. Unity-Projekt einrichten**

##### üîß Unity Setup

1. Neues Unity-Projekt erstellen (3D Template)
2. In Unity:

   * `File` ‚Üí `Build Settings` ‚Üí `Android` ausw√§hlen und `Switch Platform`
   * `Player Settings`:

     * `Minimum API Level`: Android 8.0 oder h√∂her
     * `XR Settings`: Haken bei `Vuforia Augmented Reality Supported`

##### üì¶ Vuforia Engine einbinden

1. √ñffne `Edit > Project Settings > XR Plug-in Management`
2. Aktiviere unter Android den **Vuforia AR Support**
3. Erstelle dir bei [developer.vuforia.com](https://developer.vuforia.com/) ein Konto
4. Erzeuge einen **License Key**
5. In Unity:

   * √ñffne das `Vuforia Configuration` Fenster (`Window > Vuforia Engine > Configuration`)
   * F√ºge deinen License Key ein

---

#### üéØ **3. Bildziel(e) festlegen (Target Images)**

##### Vuforia Target Manager

1. Gehe zu: [Vuforia Target Manager](https://developer.vuforia.com/target-manager)
2. Erstelle eine **Device Database**
3. Lade Referenzbilder (Fotos deiner echten Ger√§te) hoch
4. Lade die Datenbank f√ºr **Unity** als `.unitypackage` herunter und importiere sie

##### In Unity:

1. Ziehe ein **ARCamera**-Prefab in deine Szene (`GameObject > Vuforia Engine > AR Camera`)
2. F√ºge ein **ImageTarget** hinzu (`GameObject > Vuforia Engine > Image Target`)
3. W√§hle dein Bildziel aus der importierten Datenbank aus

---

#### üñºÔ∏è **4. UI & Steuerung √ºber openHAB**

##### a) 3D UI als Men√º √ºber dem Ger√§t

1. F√ºge ein **Canvas** als Child des `ImageTarget` hinzu (z.‚ÄØB. World Space)
2. Erstelle darauf Buttons (z.‚ÄØB. "Licht an", "Licht aus")

##### b) Steuerung √ºber HTTP (UnityWebRequest)

```csharp
using UnityEngine;
using UnityEngine.Networking;
using System.Text;

public class OpenHabController : MonoBehaviour
{
    public string openHabUrl = "http://192.168.1.10:8080";
    public string itemName = "Light_Livingroom";
    public string authToken = ""; // Optional

    public void SwitchOn()
    {
        SendCommand("ON");
    }

    public void SwitchOff()
    {
        SendCommand("OFF");
    }

    void SendCommand(string command)
    {
        StartCoroutine(SendCommandCoroutine(command));
    }

    IEnumerator SendCommandCoroutine(string command)
    {
        var url = $"{openHabUrl}/rest/items/{itemName}";
        var request = new UnityWebRequest(url, "POST");
        byte[] bodyRaw = Encoding.UTF8.GetBytes(command);
        request.uploadHandler = new UploadHandlerRaw(bodyRaw);
        request.downloadHandler = new DownloadHandlerBuffer();
        request.SetRequestHeader("Content-Type", "text/plain");

        if (!string.IsNullOrEmpty(authToken))
            request.SetRequestHeader("Authorization", $"Bearer {authToken}");

        yield return request.SendWebRequest();

        if (request.result == UnityWebRequest.Result.Success)
            Debug.Log("Command sent: " + command);
        else
            Debug.LogError("Error: " + request.error);
    }
}
```

Dann die `SwitchOn()` / `SwitchOff()`-Funktionen √ºber Buttons aufrufen.

---

#### üóÉÔ∏è **5. Ger√§tekonfiguration & Daten speichern**

* Du kannst eine eigene JSON-Datei mit Ger√§ten + Bildzuordnungen speichern
* Oder in Unity z.‚ÄØB. `PlayerPrefs` oder `Application.persistentDataPath` verwenden
* Optional: Externe SQLite-Datenbank (z.‚ÄØB. √ºber [SQLite for Unity](https://assetstore.unity.com/packages/tools/input-management/sqlite-kit-57402))

---

#### üì± **6. App Build auf Android**

1. `File > Build Settings > Android`
2. Szene hinzuf√ºgen und `Build & Run`
3. Android-Ger√§t muss USB-Debugging aktiviert haben

---

#### ‚úÖ Zusammenfassung: Was du brauchst

| Aufgabe                       | Tool / Technik              |
| ----------------------------- | --------------------------- |
| AR-Kamera & Tracking          | Vuforia + ImageTargets      |
| UI zur Steuerung              | Unity Canvas (World Space)  |
| REST-Steuerung openHAB        | UnityWebRequest             |
| Konfiguration speichern       | PlayerPrefs oder JSON       |
| Bilddatenbank f√ºr AR-Tracking | Vuforia Target Manager      |
| Authentifizierung             | HTTP-Header (Token / Basic) |

---

Du kannst **aktuelle Bilder aus deiner App verwenden**, um sie **zur Laufzeit** als **Image Targets in Vuforia** zu verwenden ‚Äì **aber mit Einschr√§nkungen**.

##### üîç Problem: Vuforia unterst√ºtzt **nur zur Compile-Zeit eingebundene Image Targets** direkt

Standardm√§√üig funktioniert Vuforia nur mit **vordefinierten Target-Datenbanken**, die du vorher √ºber den **Vuforia Target Manager** erstellst und in Unity importierst.

---

#### ‚úÖ Optionen zur Nutzung von **aktuellen Bildern aus deiner App**

##### üü° **Option 1: Vuforia Model Targets / Cloud Recognition (nur mit Lizenz & Server)**

Vuforia bietet:

* **Cloud Recognition**: Du kannst Bilder aus der App an Vuforia-Server schicken ‚Üí wird dort mit deiner Cloud-Datenbank abgeglichen
* ‚ûï Funktioniert zur Laufzeit
* ‚ûñ Erfordert Vuforia Cloud Lizenz (kostenpflichtig)

‚û°Ô∏è [Vuforia Cloud Recognition Info](https://library.vuforia.com/articles/Solution/How-To-Use-Cloud-Recognition)

---

##### üü¢ **Option 2: Alternativen mit ARCore + ML/AI Matching (ohne Vuforia)**

Wenn du unbedingt **eigene Bilder zur Laufzeit hinzuf√ºgen willst**, dann w√§re dieser Weg flexibler:

| Baustein                       | Beschreibung                                           |
| ------------------------------ | ------------------------------------------------------ |
| **ARCore**                     | Erkennt reale Umgebung (ohne fixe Targets)             |
| **ML Kit / TensorFlow Lite**   | F√ºr Bildvergleich / Ger√§tematching zur Laufzeit        |
| **Eigene Bilderdatenbank**     | SQLite oder Room (Name + Item-ID + Referenzbilder)     |
| **CameraX / Bitmap-Vergleich** | Fotos aus Kamera mit gespeicherten Bildern vergleichen |

Damit kannst du:

* Ein Ger√§t live fotografieren
* Mit deinen gespeicherten Ger√§ten (und deren Bildern) vergleichen
* Bei √úbereinstimmung ‚Üí UI anzeigen + openHAB steuern

---

##### üî¥ **Was nicht geht (ohne Tricks):**

> Du kannst **nicht** direkt ein `ImageTarget` in Vuforia zur Laufzeit mit einem **neuen Bild aus deiner App** definieren.

---

#### üß† Fazit: Was tun?

| Ziel                                       | L√∂sung                                      | Aufwand               |
| ------------------------------------------ | ------------------------------------------- | --------------------- |
| **Bilder zur Laufzeit erkennen & steuern** | üëâ ARCore + eigenes Matching (z.‚ÄØB. ML Kit) | mittel                |
| **Vordefinierte Ger√§te erkennen**          | üëâ Vuforia + ImageTargets                   | gering                |
| **Cloud-Erkennung mit Bildern**            | üëâ Vuforia Cloud                            | hoch, kostenpflichtig |

Anders ausgedr√ºckt:

Mit Unity + Vuforia kann man zumindest als kostenlose Android App nur eine Anwendung explizit f√ºr ein einziges Smart Home entwickeln. Man muss gezielt diese App ma√üschneidern, dass es genau in diesem Smart Home funktioniert. W√§lzt man die Kosten der Cloud auf den Endnutzer ab, k√∂nnte man zumindest eine App entwickeln, mit der zum Teil w√§hrend der Bedienung der App neue Ger√§te hinzugef√ºgt und trainiert werden k√∂nnten.

---

## ARCore und Machine Learning

Super Entscheidung! üéØ Mit **ARCore & ML (z.‚ÄØB. ML Kit oder TensorFlow Lite)** baust du eine **flexible AR-App**, bei der du:

* eigene Bilder aus der App verwenden kannst (z.‚ÄØB. Ger√§te live fotografieren),
* Ger√§te anhand dieser Bilder wiedererkennst (via Machine Learning),
* und dann in der AR-Szene passende Steuer-UI √ºber das erkannte Objekt legst (z.‚ÄØB. ‚ÄûLicht einschalten‚Äú via openHAB REST).

---

### üß† Architektur√ºberblick

```
+----------------------------+
|        Android App        |
|---------------------------|
|  ARCore (Kamera/Tracking) |
|  CameraX (Live-Foto)      |
|  ML-Modul (Bildvergleich) |
|  Room (Ger√§tebilder)      |
|  openHAB REST-Client      |
+----------------------------+
          |
          ‚Üì
[AR-Szene erkennt Objekt im Raum]
          ‚Üì
[√úbereinstimmung mit Ger√§tedatenbank]
          ‚Üì
[Overlay-Men√º f√ºr Steuerung]
```

---

### üì± Beispiel-Ablauf in deiner App

1. **Ger√§t registrieren**

   * Du machst 3‚Äì5 Fotos vom Ger√§t (z.‚ÄØB. Steckdose)
   * Vergibst Name + openHAB Item-ID
   * Die Bilder + Daten landen in einer `Room`-Datenbank

2. **Beim Durchlaufen des Raumes**

   * ARCore erkennt die Umgebung (Plane Detection + Pose Estimation)
   * Du nimmst laufend Bilder aus der Kamera (CameraX)
   * Die App vergleicht aktuelle Bilder mit gespeicherten Ger√§ten per **Bildvergleich (ML)**

3. **Wenn ein Ger√§t erkannt wurde**

   * Es erscheint ein Men√º-Overlay (z.‚ÄØB. 3D-Knopf in der AR-Szene)
   * Per Klick wird ein openHAB-REST-Befehl gesendet (z.‚ÄØB. ‚ÄûON‚Äú)

---

### üß∞ Technologien, die du brauchst

| Zweck                  | Framework                                      | Hinweise                          |
| ---------------------- | ---------------------------------------------- | --------------------------------- |
| AR                     | [ARCore](https://developers.google.com/ar)     | z.‚ÄØB. Sceneform, ARCore Jetpack   |
| Kamera-Bilder          | CameraX                                        | Modern, leicht in Jetpack Compose |
| Bildvergleich          | ML Kit Image Labeling **oder** TensorFlow Lite | ML Kit einfacher f√ºr Start        |
| Datenbank Ger√§tebilder | Room (SQLite)                                  | Name, Item-ID, Bildpfade          |
| REST API zu openHAB    | Retrofit                                       | Mit Basic/Auth Token              |
| UI/Overlays            | Jetpack Compose + AR Anchor Nodes              | Overlay im Raum rendern           |

---

### üß™ ML-Vergleich: Wie geht das?

#### ‚úÖ **Einfache L√∂sung mit ML Kit (kein eigenes Modell n√∂tig)**

ML Kit kann:

* Label in Bildern erkennen (z.‚ÄØB. ‚ÄûLampe‚Äú, ‚ÄûSteckdose‚Äú)
* Custom Image Classification (mit eigenem Modell oder Aufklebern)

Alternativen:

* **Eigenes TinyML-Modell** trainieren (z.‚ÄØB. mit [Teachable Machine](https://teachablemachine.withgoogle.com/))
* Export als `.tflite`
* Einbinden mit TensorFlow Lite Interpreter

---

### üß© Ger√§temodell in Room (Beispiel)

```kotlin
@Entity(tableName = "devices")
data class DeviceEntity(
    @PrimaryKey(autoGenerate = true) val id: Int = 0,
    val name: String,
    val itemId: String,
    val label: String,
)

@Entity(tableName = "device_images")
data class DeviceImageEntity(
    @PrimaryKey(autoGenerate = true) val id: Int = 0,
    val deviceId: Int,
    val imagePath: String // lokal gespeichertes JPEG
)
```

---

### üß† Workflow in Code (vereinfacht)

```kotlin
// Schritt 1: Bild aufnehmen
val photo = cameraX.takePicture()

// Schritt 2: Durch ML-Kit schicken
val result = mlModel.classify(photo)

// Schritt 3: Pr√ºfen ob Label mit einem bekannten Ger√§t √ºbereinstimmt
val matchedDevice = deviceDao.getByLabel(result.label)

// Schritt 4: Falls Ger√§t erkannt, UI anzeigen
if (matchedDevice != null) {
    showOverlayMenuForDevice(matchedDevice)
}
```

---

### üì¶ Projektstruktur (Android-Kotlin)

```
com.yourapp.aropenhab
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ room/
‚îÇ       ‚îú‚îÄ‚îÄ DeviceDao.kt
‚îÇ       ‚îú‚îÄ‚îÄ DeviceEntity.kt
‚îÇ       ‚îî‚îÄ‚îÄ AppDatabase.kt
‚îÇ
‚îú‚îÄ‚îÄ ar/
‚îÇ   ‚îî‚îÄ‚îÄ ARSceneManager.kt  ‚Üê ARCore & Anchors
‚îÇ
‚îú‚îÄ‚îÄ ml/
‚îÇ   ‚îî‚îÄ‚îÄ DeviceClassifier.kt ‚Üê TensorFlow Lite Wrapper oder ML Kit
‚îÇ
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îú‚îÄ‚îÄ DeviceConfigScreen.kt ‚Üê Compose UI zur Ger√§teverwaltung
‚îÇ   ‚îî‚îÄ‚îÄ AROverlayMenu.kt
‚îÇ
‚îî‚îÄ‚îÄ network/
    ‚îî‚îÄ‚îÄ OpenHabClient.kt  ‚Üê REST-Client mit Retrofit
```

---

### ‚úÖ Vorteile dieses Ansatzes:

* Du kannst **beliebige neue Ger√§te** hinzuf√ºgen (Fotos in der App)
* Kein manuelles Hochladen zu Vuforia n√∂tig
* Du bist **nicht auf vordefinierte Targets beschr√§nkt**
* Kompatibel mit lokalen ML-Modellen (offline m√∂glich!)

---

Gro√üartig! Hier ist dein vollst√§ndiger Leitfaden f√ºr die Erstellung einer **Android AR-App**, die mithilfe von **ARCore**, **Machine Learning (ML Kit oder TensorFlow Lite)** und **Sceneform** Ger√§te erkennt und √ºber **openHAB** steuert.

---

#### üî® 1. Vollst√§ndiges Beispielprojekt

Ein umfassendes Beispielprojekt, das ARCore mit ML Kit integriert, findest du im offiziellen Google-Beispiel:

üëâ [ARCore ML Sample (GitHub)](https://github.com/googlesamples/arcore-ml-sample)

Dieses Projekt demonstriert, wie man ARCore verwendet, um Kamera-Frames zu erfassen, ML Kit zur Objekterkennung einzusetzen und Ergebnisse in der AR-Szene darzustellen. Es ist in Kotlin geschrieben und bietet eine solide Grundlage f√ºr deine Anwendung.

---

#### üß™ 2. Bildvergleich mit ML Kit oder Teachable Machine (TensorFlow Lite)

##### Option A: ML Kit

**ML Kit** bietet eine einfache M√∂glichkeit, Objekte in Bildern zu erkennen:

* **Vorteile**:

  * Einfache Integration in Android-Apps
  * Echtzeit-Erkennung
  * Offline-Funktionalit√§t

* **Implementierung**:

  * Verwende ML Kits Objekterkennungs- und Tracking-API
  * Integriere die API in deine App, um Objekte in Kamera-Frames zu erkennen

üëâ [ML Kit Objekterkennung Codelab](https://codelabs.developers.google.com/mlkit-android-odt)

##### Option B: Teachable Machine mit TensorFlow Lite

**Teachable Machine** erm√∂glicht es dir, ein benutzerdefiniertes Modell zu erstellen:

* **Schritte**:

  1. Gehe zu [Teachable Machine](https://teachablemachine.withgoogle.com/)
  2. Erstelle ein neues Bildprojekt
  3. Lade Bilder deiner Ger√§te hoch und trainiere das Modell
  4. Exportiere das Modell im TensorFlow Lite-Format
  5. Integriere das `.tflite`-Modell in deine Android-App

* **Implementierung**:

  * Verwende TensorFlow Lite Interpreter, um das Modell in deiner App zu nutzen
  * Verarbeite Kamera-Frames und f√ºhre Inferenz durch, um Ger√§te zu erkennen

üëâ [Teachable Machine Android Integration Tutorial](https://medium.com/geekculture/build-a-custom-image-classification-android-app-using-teachable-machine-f60b197eaa90)

---

#### üß± 3. Live-Overlay mit Sceneform oder ARCore Jetpack

**Sceneform** erm√∂glicht es, 3D-Modelle in AR-Szenen darzustellen:

* **Implementierung**:

  * Verwende `ArFragment` f√ºr die AR-Sitzung
  * Erstelle `AnchorNode` und `TransformableNode`, um 3D-Modelle an erkannte Positionen zu binden
  * F√ºge UI-Elemente (z.‚ÄØB. Buttons) hinzu, um Ger√§te √ºber openHAB zu steuern

üëâ [Sceneform Samples](https://developers.google.com/sceneform/develop/samples)

**Hinweis**: Sceneform wird nicht mehr aktiv gepflegt, funktioniert aber weiterhin f√ºr viele Anwendungen. Alternativ kannst du OpenGL oder andere 3D-Rendering-Engines in Betracht ziehen.

---

#### üì¶ Projektstruktur

```
com.yourapp.aropenhab
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ room/
‚îÇ       ‚îú‚îÄ‚îÄ DeviceDao.kt
‚îÇ       ‚îú‚îÄ‚îÄ DeviceEntity.kt
‚îÇ       ‚îî‚îÄ‚îÄ AppDatabase.kt
‚îÇ
‚îú‚îÄ‚îÄ ar/
‚îÇ   ‚îî‚îÄ‚îÄ ARSceneManager.kt  ‚Üê ARCore & Anchors
‚îÇ
‚îú‚îÄ‚îÄ ml/
‚îÇ   ‚îî‚îÄ‚îÄ DeviceClassifier.kt ‚Üê TensorFlow Lite Wrapper oder ML Kit
‚îÇ
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îú‚îÄ‚îÄ DeviceConfigScreen.kt ‚Üê Compose UI zur Ger√§teverwaltung
‚îÇ   ‚îî‚îÄ‚îÄ AROverlayMenu.kt
‚îÇ
‚îî‚îÄ‚îÄ network/
    ‚îî‚îÄ‚îÄ OpenHabClient.kt  ‚Üê REST-Client mit Retrofit
```

## Handy trainiert selbst

Wenn du m√∂chtest, dass dein **Handy selbstst√§ndig trainiert**, dann sprichst du von **On-Device Training**, also:

> üì± Dein Android-Ger√§t **lernt eigenst√§ndig** neue Ger√§te zu erkennen ‚Äì direkt aus der App heraus, ohne externes Training am PC oder Cloud.

Das ist technisch **sehr anspruchsvoll**, aber **machbar** mit modernen ML-Frameworks wie **TensorFlow Lite + Model Personalization**.

---

#### üîç Was bedeutet ‚Äûdas Handy trainiert selbst‚Äú?

Das bedeutet, deine App:

1. üì∑ Nimmt **Fotos von einem neuen Ger√§t** auf
2. üß† **Trainiert ein Modell direkt auf dem Ger√§t** (online learning)
3. üíæ Speichert dieses Modell oder die erlernten Merkmale
4. üîç Nutzt es sp√§ter, um √§hnliche Ger√§te automatisch zu erkennen

---

#### üõ†Ô∏è Technologien daf√ºr

| Aufgabe                     | Framework                                 | Status                     |
| --------------------------- | ----------------------------------------- | -------------------------- |
| On-Device Training          | **TensorFlow Lite + Personalization SDK** | ‚úÖ (offiziell, stabil)      |
| Edge-Training f√ºr Bilder    | **MediaPipe + TFLite**                    | üü° (erfordert Optimierung) |
| Persistente Modellanpassung | Custom Classifier Layer                   | ‚úÖ                          |

---

#### üß± Architektur f√ºr On-Device Learning

```
+---------------------------+
|     Android AR App        |
|---------------------------|
| ARCore (Umgebung)         |
| CameraX (Live-Fotos)      |
| TFLite Model (mobil)      |
| Training-Modul (lokal)    | ‚Üê Nutzer trainiert per Foto
| Ger√§te-Datenbank (Room)   |
| openHAB-REST Integration  |
+---------------------------+
```

---

#### ‚úÖ Vorgehen Schritt f√ºr Schritt

##### 1. üì∏ Fotos aufnehmen vom neuen Ger√§t

```kotlin
// z.B. 5 Bilder aufnehmen
val imageList = takePhotoSeries(deviceName)
```

##### 2. üß† Custom-Training auf dem Ger√§t starten

Du trainierst nicht das komplette Modell, sondern nur die **letzte Klassifikationsschicht** (Transfer Learning).

###### Beispiel mit TensorFlow Lite Model Personalization:

```kotlin
val classifier = PersonalizedImageClassifier(context)
classifier.train(deviceName, imageList)
```

üîß Es nutzt:

* MobileNet oder EfficientNet als Basis
* Die neuen Bilder nur f√ºr Feintuning (Last Layer)

##### 3. üíæ Modell speichern oder erweitern

```kotlin
classifier.saveModel(context, "trained_model.tflite")
```

Oder dynamisch im RAM halten und sp√§ter anpassen.

##### 4. üîç Inferenz: Ger√§t erkennen

```kotlin
val prediction = classifier.predict(liveCameraFrame)
if (prediction.label == "Steckdose") {
   showAROverlay("Steckdose", position)
}
```

---

#### üß™ Tools f√ºr dich

| Tool                                                                                                           | Beschreibung                   | Link |
| -------------------------------------------------------------------------------------------------------------- | ------------------------------ | ---- |
| [TFLite Model Maker](https://www.tensorflow.org/lite/tutorials/model_maker_image_classification)               | Modell trainieren              | ‚úÖ    |
| [TFLite Model Personalization](https://www.tensorflow.org/lite/performance/model_personalization)              | On-Device-Training             | ‚úÖ    |
| [MediaPipe AutoML Training](https://developers.google.com/mediapipe/solutions/vision/image_classifier/android) | Visual Classifier mit Training | üü°   |

---

#### üì¶ Beispielhafte Kotlin-Klassenstruktur

```kotlin
class DeviceTrainer(context: Context) {
    fun trainDevice(name: String, images: List<Bitmap>) { ... }
    fun predictDevice(image: Bitmap): String { ... }
    fun saveModel(name: String): Boolean { ... }
}
```

---

#### Grundger√ºst

Das Grundger√ºst f√ºr die App:

```
// Datei: DeviceTrainer.kt
package com.example.aropenhab.ml

import android.content.Context
import android.graphics.Bitmap
import org.tensorflow.lite.task.vision.classifier.ImageClassifier
import org.tensorflow.lite.support.image.TensorImage
import java.util.*

class DeviceTrainer(private val context: Context) {

    private var classifier: ImageClassifier? = null

    init {
        // Modell laden (z.‚ÄØB. MobileNet V1 via Model Personalization)
        val options = ImageClassifier.ImageClassifierOptions.builder()
            .setMaxResults(1)
            .build()
        classifier = ImageClassifier.createFromFileAndOptions(
            context,
            "mobilenet_v1.tflite", // Dummy-Modell, ersetzbar
            options
        )
    }

    fun predictDevice(bitmap: Bitmap): String {
        val image = TensorImage.fromBitmap(bitmap)
        val results = classifier?.classify(image)
        return results?.firstOrNull()?.categories?.firstOrNull()?.label ?: "Unknown"
    }

    fun trainDevice(name: String, images: List<Bitmap>) {
        // Hier m√ºsstest du ein Personalization-Modul erg√§nzen,
        // z.‚ÄØB. mit TFLite Model Maker (mobilen Support bauen)
        // Pseudocode f√ºr Erweiterung:
        // model.updateWithImages(name, images)
    }

    fun saveModel(name: String): Boolean {
        // Speicher-Logik implementieren, z.‚ÄØB. exportiertes tflite-Modell
        return false
    }
}
```

* Kotlin-Klasse `DeviceTrainer.kt`
* Ladefunktion f√ºr ein Basis-Modell (MobileNet V1)
* Bildklassifizierung per `ImageClassifier`
* Platzhalter f√ºr On-Device-Training (erweiterbar mit Model Personalization)

---

## üîú N√§chste Schritte:

1. üìÇ Projektstruktur (Android Studio)
2. üì∑ Kamera-Integration (CameraX)
3. üß† TFLite Model Maker/Personalization f√ºr dynamisches Training
4. üåê openHAB REST-Aufrufe zur Steuerung

Soll ich nun auch:

* Die `CameraX`-Integration einbauen?
* Das Live-Bildklassifizierungs-Overlay mit Jetpack Compose oder Sceneform machen?
* Eine UI f√ºr das Ger√§tekonfigurations-Training erstellen?

Sag einfach z.‚ÄØB. ‚ÄûJa, bitte Kamera & UI hinzuf√ºgen‚Äú.

