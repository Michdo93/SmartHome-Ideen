# Bachelorarbeit: Entwicklung eines modularen, datenschutzfreundlichen und lokalen Sprachassistenten

---

## MÃ¶gliche TitelvorschlÃ¤ge

- "Entwicklung eines datenschutzfreundlichen Sprachassistenten auf Basis freier Software"
- "Vergleich von ASR-Engines fÃ¼r Offline-Sprachassistenten"
- "Modularer Sprachassistent fÃ¼r den lokalen Einsatz: Architektur und Prototyp"
- "Sprachverarbeitung ohne Cloud: Konzeption und Evaluation eines lokalen Sprachassistenten"
- "Custom Wakeword Detection in Embedded Systems: Architektur, Training und Einsatz"
- "Open Source Sprachassistent: Ein systemischer Vergleich freier Komponenten"
- ...

---

## Hintergrund

Sprachassistenten wie Alexa, Siri oder Google Assistant haben sich im Alltag vieler Menschen etabliert. Allerdings sind diese Systeme oft cloudgebunden, datenschutzrechtlich bedenklich und schwer individualisierbar. Dies erÃ¶ffnet spannende MÃ¶glichkeiten fÃ¼r die Entwicklung eines eigenen, lokal laufenden Sprachassistenten mit freier Wahl der Komponenten.

---

### Problemstellung

- Cloudverbindungen kÃ¶nnen auch Ã¶fter mal gestÃ¶rt oder abgebrochen sein.
  - Ein lokaler Sprachassistent wÃ¼rde zumindest lokal funktionierende Befehle bearbeiten und lokal verfÃ¼gbare Informationen zur VerfÃ¼gung stellen kÃ¶nnen.
- Datenschutzrechtlich hÃ¶rt ein Sprachassistent dauerhaft mit und wartet, bis ein sog. Hotword (auch trigger word oder wake word) erkannt wird.
  - Ist wirklich garantiert, dass nur Daten aufgezeichnet und an eine Cloud gesendet werden, wenn ein Hotword verwendet wurde oder lauscht das System dauerhaft mit?
  - Anmerkung: Deutsche Begriffe sind Aktivierungswort, Aufwachwort, Aufwachbefel, Triggerwort, usw.
- Sprachassistenten sind oft nur beschrÃ¤nkt individualisierbar, Anwendungen werden eingestellt oder eingeschrÃ¤nkt.
  - Google Nest ermÃ¶glicht es oft bspw. Smart Home GerÃ¤te zwar anzubinden, diese kÃ¶nnen darÃ¼ber aber nicht gesteuert werden, sondern meist nur ZustÃ¤nde abgefragt werden.
  - Amazon hat bspw. ihre Policy geÃ¤ndert und seitdem wird das openHAB Skill aus SicherheitsgrÃ¼nden nicht mehr unterstÃ¼tzt, damit wÃ¤re ein Smart Home, welches Ã¼ber openHAB gesteuert wurde nicht mehr bedienbar.

---

## Ziel der Arbeit

Ziel der Arbeit ist die Entwicklung und prototypische Umsetzung eines lokalen Sprachassistenzsystems auf Basis freier Software und Hardware. Der Fokus liegt auf ModularitÃ¤t, Erweiterbarkeit und Datenschutzfreundlichkeit. Es sollen verschiedene Komponenten (Spracherkennung, Hotword-Erkennung, Intent-Parsing, Text-to-Speech) integriert und ggf. evaluiert werden. Es muss eine Anbindung zur openHAB REST API mÃ¶glich sein.

---

## Teilziele

- Aufbau einer modularen Architektur fÃ¼r Sprachassistenz
- Integration von Hotword-Detection, ASR (Automatic Speech Recognition, hier: Speech-to-Text), NLU (Natural Language Understanding, hier: Intent-Erkennung ist eine Teilmenge des Natural Language Processings), TTS (Text-to-Speech)
- (Optional) Vergleich verschiedener frei verfÃ¼gbarer ASR/NLU/TTS-Frameworks hinsichtlich:
  - Latenz
  - Genauigkeit
  - Ressourcenverbrauch
- Anpassbarkeit durch neue TriggerwÃ¶rter und Intents
- Betrachtung datenschutzrechtlicher Implikationen (lokal vs. Cloud)
- Anbindung an die openHAB REST API
  - Triggersatz soll bspw.
    - Command an ein openHAB Item schicken und darÃ¼ber dann ein GerÃ¤t steuern
    - State von einem openHAB Item abfragen und per Text-to-Speech diesen Systemzustand ausgeben
- eigene REST API zur VerfÃ¼gung stellen
  - ein Endpunkt kÃ¶nnte zum Beispiel genutzt werden, dass man einen Text schickt, der dann per Text-to-Speech ausgegeben wird.
  - ...
- (Optional) Einbindung einfacher Aktionen (z.â€¯B. Timer/Wecker setzen, Terminerinnerungen speichern & Termine vorlesen, Wetter abfragen, "das Internet durchsuchen")-
- (Optional) Einbindung von GerÃ¤ten unabhÃ¤ngig von Smart Home Systemen
  - Zum Beispiel die Python Library `SoCo` (Sonos Controller)
    - Man kÃ¶nnte die `play_uri`-Funktion nutzen, dass die Text-to-Speech-Ausgabe nicht Ã¼ber einen Lautsprecher am entwickelten GerÃ¤t erfolgt, sondern Ã¼ber einen Sonos-Lautsprecher. Entsprechend muss dies konfigurierbar und eintsellbar sein.

---

## MÃ¶gliche Forschungsschwerpunkte

- Vergleich von Offline-STT-Engines: `Whisper`, `Vosk`, `DeepSpeech`
- Evaluation von NLU-Systemen: `Rasa`, `Snips`, Transformer-basierte LÃ¶sungen
- Training eigener Hotword-Modelle mit `Porcupine`, `Mycroft Precise`
- Einfluss der Hardware auf Performance (z.â€¯B. Raspberry Pi vs. Jetson Nano)
- Kombination regelbasierter und ML-basierter Dialogmodelle
- Lokaler Datenschutz vs. Cloud-Performance

---

## Eingesetzte Technologien (Auswahl)

- Programmiersprache: **Python**
- Hotword-Erkennung: `Porcupine`, `Mycroft Precise`
- Speech-to-Text: `Whisper`, `Vosk`, `DeepSpeech`
- NLU: `Rasa`, `Snips`, eigene ML-Modelle mit `transformers`
- TTS: `Coqui TTS`, `eSpeak`, `Festival`
- Plattform: **Raspberry Pi 5**, ggf. Mini-PC

(Unten sieht man, dass es hier verschiedene Alternativen zur Auswahl gibt. Auch mit Erfahrungen wÃ¤hrend der Bearbeitung darf hier natÃ¼rlich die Technologien getauscht werden.)

---

## Voraussetzungen

- Interesse an Sprachverarbeitung, KI und Embedded Systems
- Gute Kenntnisse in Python
- GrundverstÃ¤ndnis maschinellen Lernens und Linux-Systemen
- Motivation zur Arbeit mit Open-Source-Technologien

---

## Betreuung & Rahmen

- Diese Arbeit eignet sich besonders fÃ¼r Studierende der Informatik, Medieninformatik oder verwandter StudiengÃ¤nge.
- Umfang: Bachelorarbeit (ca. 16 Wochen Vollzeit)
- Bei Interesse ist eine Weiterentwicklung zur Masterarbeit denkbar.

---

## Beispielaufbau & Vorgehensweise

Ein eigener Sprachassistent ist ein spannendes Projekt â€“ technisch anspruchsvoll, aber mit vielen Freiheiten in der Gestaltung. Unten findest du eine systematische Ãœbersicht Ã¼ber alle benÃ¶tigten **Komponenten**: Hardware, Software, KI-Module und die empfohlene Toolchain.

---

### ğŸ”§ **1. Hardware**

#### âœ… *Mindestanforderungen (fÃ¼r lokale Verarbeitung)*:

* **Mini-PC oder Einplatinenrechner**:

  * *Raspberry Pi 5 (2GB/4GB/8GB/16GB)* â€“ Einsteigerfreundlich, fÃ¼r einfache Anwendungen ausreichend.
  * *Alternativen*: NVIDIA Jetson Nano (wenn du KI lokal trainieren willst), oder ein Mini-PC wie Intel NUC.
* **Mikrofon**:

  * *USB-Mikrofon* mit guter AufnahmequalitÃ¤t (z.â€¯B. Samson Go Mic, ReSpeaker 2-Mic HAT fÃ¼r Raspberry Pi).
  * *Array-Mikrofone* (fÃ¼r bessere Erkennung und GerÃ¤uschunterdrÃ¼ckung).
* **Lautsprecher**:

  * Per Klinke oder USB anschlieÃŸbar. Auch kleine Bluetooth-Speaker sind mÃ¶glich.

> ğŸ” Empfehlung: Beginne mit einem USB-Mikrofon + Pi 5 oder Mini-PC zum Testen, bevor du in Mikrofon-Arrays oder Jetson investierst.

---

### ğŸ§  **2. Software-Komponenten & Architektur**

Die Sprachassistenz umfasst mehrere Stufen:

| **Schritt**                   | **Funktion**                           | **Empfohlene Tools/Libs**                                        |
| ----------------------------- | -------------------------------------- | ---------------------------------------------------------------- |
| **1. Hotword Detection**      | Erkennung des Aktivierungswortes       | `Porcupine`, `Snowboy`, `Mycroft Precise`, `Picovoice`           |
| **2. Spracherkennung (ASR)**  | Umwandlung von Sprache in Text         | `Vosk`, `Whisper`, `DeepSpeech`, Google STT (Cloud)              |
| **3. NLU (Intent-Erkennung)** | Bedeutung/Text in Aktion umwandeln     | `Rasa`, `Snips NLU`, eigene Modelle mit `Transformers`           |
| **4. Antwortgenerierung**     | Aktion ausfÃ¼hren oder Antwort erzeugen | Eigene Logik / GPT-Anbindung / Regelbasiert                      |
| **5. Text-zu-Sprache (TTS)**  | Text in Sprache umwandeln              | `Coqui TTS`, `Festival`, `eSpeak`, Google TTS, `ResponsiveVoice` |

Neben der Sprachassistenz benÃ¶tigt man meiner Meinung nach `Flask` fÃ¼r die `REST API` und fÃ¼r eine `WeboberflÃ¤che` des Sprachassistenzen. Wahrscheinlich konfiguriert man Ã¼ber die WeboberflÃ¤che TriggersÃ¤tze. Entsprechend wird auch eine Datenbank (`SQLite` mit `SQLAlchemy`-Anbindung) fÃ¼r PasswÃ¶rter und Regeln benÃ¶tigt.

#### Beispielvergleich

Die Auswahl von verschiedenen Libraries kann ja Vor- und Nachteile hervorheben.

##### Hotword Detection

###### âœ… Vergleich: **Lokale Hotword-Erkennungssysteme**

| **Library / System** | **Sprache / API** | **Lokal** | **Cloudfrei**     | **Status**           | **Besonderheiten**                                      |
| -------------------- | ----------------- | --------- | ----------------- | -------------------- | ------------------------------------------------------- |
| ğŸ”¹ `Porcupine`       | Python 3 / C      | âœ…         | âœ…\*               | âœ… aktiv              | Sehr effizient, Wakeword-Modelle mit Lizenz generierbar |
| ğŸ”¹ `Snowboy`         | Python 2 / C++    | âœ…         | âš ï¸ (fÃ¼r Training) | âŒ eingestellt (2020) | Exzellente Erkennung, kein Custom Training mehr         |
| ğŸ”¹ `Mycroft Precise` | Python 3          | âœ…         | âœ…                 | âœ… aktiv (2024 Forks) | Open-Source, trainierbar mit eigenem Datensatz          |
| ğŸ”¸ `Picovoice`       | Python 3 / Web    | âš ï¸ teils  | âŒ                 | âš ï¸ deprecated        | Picovoice war Suite, nun nur noch Porcupine aktiv       |

---

###### ğŸ§  Klarstellungen:

* **Porcupine** (by Picovoice):

  * Sehr **effizient**, lÃ¤uft sogar auf Mikrocontrollern.
  * Man kann eigene Wakewords **lokal erzeugen**, aber dazu braucht man evtl. die **Picovoice Console** (Web).
  * Die Laufzeit selbst ist **vollstÃ¤ndig offline**.
  * Lizenzmodell: Kostenlos fÃ¼r Einzelpersonen / nicht-kommerzielle Zwecke.

* **Snowboy**:

  * FrÃ¼her sehr populÃ¤r fÃ¼r lokale Wakeword-Erkennung.
  * Leider **nicht mehr gepflegt**.
  * Eigene Wakewords waren nur Ã¼ber eine Web-OberflÃ¤che trainierbar, die inzwischen offline ist.

* **Mycroft Precise**:

  * **Open Source und lokal trainierbar** mit eigenem Datensatz.
  * Python 3-kompatibel, kann auf Linux / Raspberry Pi laufen.
  * Aktive Community / Forks, auch 2024 noch in Benutzung.
  * Funktioniert gut fÃ¼r DIY- oder Datenschutzprojekte.

* **Picovoice SDK**:

  * Die **gesamte SDK-Suite (Speech-to-Text etc.)** wurde eingeschrÃ¤nkt, empfohlen wird jetzt direkt **Porcupine**.
  * **Nicht mehr aktiv entwickelt** als Komplettpaket.

---

###### ğŸ“Œ Fazit / Empfehlung:

| Einsatzziel                              | Empfehlung                         |
| ---------------------------------------- | ---------------------------------- |
| Lokaler Assistent mit Custom Hotword     | ğŸ”¹ **Mycroft Precise**             |
| Minimaler Stromverbrauch (z.â€¯B. Pi Zero) | ğŸ”¹ **Porcupine**                   |
| Forschung / Training eigener Wakewords   | ğŸ”¹ **Mycroft Precise**             |
| Veraltete Tools vermeiden                | âŒ Kein Snowboy, kein Picovoice SDK |

---

Dies ist jetzt nur eine kleine Ãœbersicht, was man machen kann im Vergleich. Am besten Quellen ranziehen (z. B. Webseite, Doku, usw.). Man vergleicht am Besten `Python 3 / Python 2` (`Python 2` ist ein Ausschlusskriterium). In der auf NodeJS-basierenden Software `MagicMirror` wird fÃ¼r die Spracherkennung `Snowboy` eingesetzt. Dort ist als `Hotword` bereits `MagicMirror` vortrainiert. Ein neues Hotword kann nicht mehr trainiert werden, weil `Snowboy` deprecated ist und der Service fÃ¼r das Training meines Wissens sogar eingestellt wurde. SelbstverstÃ¤ndlich schaut man sich an, ob etwas cloudbasiert funktioniert, lokal oder teilweise cloudbasiert ist. Ebenfalls wichtig ist ja, ob etwas kostenlos ist oder nicht. Hin und wieder gibt es ja auch Abomodelle, dass man vielleicht 1000 Requests kostenlos hat oder CentbetrÃ¤ge fÃ¼r einzelne Requests zahlt, usw. Manche Libraries nutzen `C`, `C++` im Background und es wird nur ein `Python-Wrapper` verwendet. Dies bedeutet, dass das Training performanter und hardwarenÃ¤her ist.

##### ASR/TTS

Es gibt extrem viele Libraries fÃ¼r `Text-to-Speech`. Die meisten benÃ¶tigen eine Cloud. Also wirklich fast alle. Dies ist ein klarer Nachteil. Manche sind immer noch auf Basis von `Python 2`. Einige unterstÃ¼tzen sowohl `Python 2`, als auch `Python 3`. Manche gehen natÃ¼rlich nur mit `Python 3`. Ergo kann man auch hier alles was `Python 3` nutzt als Vorteil ansehen. Das Ergebnis, bzw. die SprachqualitÃ¤t von cloudbasierten Libraries ist ganz klar besser. Die lokal laufenden Systeme klingen im Deutschen so, als wÃ¼rde ein Amerikaner Deutsch reden und dann klingt es sehr technisch, mechanisch nach einem Roboter, wÃ¤hrend cloudbasierte Ergebnisse meist einem menschlichen Sprecher sehr nahe kommt. Je nachdem dauert die Umwandlung lange. Bei cloudbasierten LÃ¶sungen gibt es welche, die sehr schnell gute Ergebnisse zurÃ¼ckliefern. Cloudbasiert und damit viel Rechenleistung bedeutet aber tatsÃ¤chlich nicht zwangslÃ¤ufig, dass es schnell geht, bis die Datei generiert wird. Auch hier gibt es mÃ¶glicherweise Latzenzen. Lokal laufende LÃ¶sungen haben zwar keine Latzen zwecks Netzwerkauslastung oder schlechter Internetverbindung, sind aber oft auch langsam, weil die RechenkapazitÃ¤t geringer ist, als in einer Cloud. Man kann vergleichen, welche Datenformate generiert werden (`wav`, `mp3`, usw.). Ich denke fÃ¼r die meisten Szenarien ist es egal, welches Audioformat generiert wird. Wenn man aber bspw. Sonos-Lautsprecher verwenden mÃ¶chte, dann kann es sein, dass nicht jedes Audioformat unterstÃ¼tz wird. Vor- und Nachteile in der Bewertung kann es sein, ob Sprecherstimmen (verschiedene Frauen- oder MÃ¤nnerstimmen) konfigurierbar sind, ob Sprachen konfigurierbar sind oder ob `Speech Synthesis Markup Language` (`SSML`) unterstÃ¼tzt wird. `SSML` ist ein ganz klarer Vorteil und gerade im Vergleich zu `Amazon Alexa`, wÃ¤re es vielleicht sogar sehr sinnvoll, da `Alexa` dies unterstÃ¼tzt. 

Es kÃ¶nnen auch weitere Kritieren noch fÃ¼r einen Vergleich herangezogen werden.

Anmerkung: Es gibt verschiedene Libraries, die im Hintergrund aber bspw. denselben Service wie `Google STT` verwenden. Das heiÃŸt, viele Libraries stellen zu manch einer Library kaum eine echte Alternative dar.

##### NLP

SelbstverstÃ¤ndlich kommt hier auch wieder `Python 2` vs. `Python 3` vor, wenn man die Libraries vergleicht. Da es hier um KI geht, ist auch entscheidend, ob `CPU` oder `GPU` verwendet werden kann. Gemessen an der schwachen Leistung des EndgerÃ¤ts (z. B. Raspberry Pi), wÃ¼rde es hier auch Sinn ergeben, ob man eine `TPU`-UnterstÃ¼tzung oder andere Coprozessoren verwenden kÃ¶nnte. Es gibt auch KIs, die Ã¼ber eine Cloud trainiert werden. Bei manchen KIs, in einer Cloud lÃ¤dt man Daten hoch, trainiert sie, lÃ¤dt Daten herunter und kann die trainierten Daten zu seiner Anwendung hinzufÃ¼gen. Dies wÃ¤re dann zumindest bedingt offline und lokalfÃ¤hig, weil man nur beim Training eine Internetverbindung zwingend benÃ¶tigt hat. Es gibt KIs, die sind leichtgewichtig und besonders gut fÃ¼r `Raspberry Pi's` geeignet.

##### Antwortgenerierung

Ich denke dies kann man mehrschichtig aufbauen. Als erstes nutzt man z. B. die regelbasierten Antworten, wenn dort keine Regel auftaucht kÃ¶nnen Websuche oder KIs angebunden sein, usw. Hier kann man auch Vor- und Nachteile diskutieren. Eine eigene KI zu trainieren edeutet, dass man sehr vielseitig trainieren mÃ¼sste und immens viele Daten brÃ¤uchte. Ein `GPT` (`Generative Pre-trained Transformer`) bedeutet ja in der Regel immer eine Cloud-LÃ¶sung (es gibt auch Docker-Container, die man lokal laufen lassen und trainieren kann).

### Speech-to-Text

Nicht selten sind es auch dieselben Cloud-Dienstleister, wie bei `Text-to-Speech`. Ich behaupte, es sind auch sehr vegleichbare und Ã¤hnliche Kriterien, sowie dieselben Vor- und Nachteile.

---

### ğŸ§  **3. KI-Modelle: WofÃ¼r und wie trainieren?**

#### Du brauchst KI fÃ¼r:

* **Hotword Detection** (Custom Wakewords)
* **Intent-Erkennung** (z.â€¯B. â€œWie wird das Wetter morgen?â€)
* (Optional) **Dialog-Management** (Kontext verstehen)
  * Man kann dies Regelbasiert machen. Empfehlenswert wÃ¤re eine OberflÃ¤che, bei der ich TriggersÃ¤tze hinzufÃ¼ge. Sogenannte `WENN-DANN`-Regel sind im Smart Home Gang und GÃ¤be. Man wÃ¤hlt bspw. `WENN` aus und schreibt einen Triggersatz, klickt dann auf `DANN` und wÃ¤hlt bspw. `openHAB Command` und sendet einen `Command` an ein openHAB Item oder man wÃ¤hlt bspw. `openHAB State` und macht eine Statusabfrage von einem openHAB Item.
  * GrundsÃ¤tzlich kann man es so aufbauen, dass sobald ein Kontext nicht verstanden wird, eine KI versucht zu verstehen, was gemeint ist oder eine Websuche machen, sobald etwas erkannt wird, was in keiner Regel hinterlegt ist.
  * Man kann auch im Regelsystem direkt integrieren, dass z. B. Termineerinnerungen gesetzt werden sollen, das Web durchsucht werden soll, usw. Man kann ja auch absichtlich TriggersÃ¤tze bauen, die nicht zwangslÃ¤ufig mit Smart Home zu tun haben mÃ¼ssen.

##### Modelle:

* Hotword-Erkennung â†’ kleine CNNs oder vortrainierte Modelle (Snowboy, Porcupine).
* Intent-Erkennung â†’ NLP-Modelle wie BERT oder einfache Klassifikatoren (z.â€¯B. `scikit-learn`, `spaCy`, `Rasa`).
* FÃ¼r Antwortgenerierung kannst du GPT lokal oder per API (OpenAI, HuggingFace) einbinden.

##### Training:

* Hotword-Erkennung: ca. 20â€“50 Audiobeispiele pro Wakeword.
* Intent-Erkennung: Beispiel-Intents in YAML/JSON, evtl. Feintuning mit `Rasa` oder `Transformers`.

> ğŸ’¡ FÃ¼r viele AnwendungsfÃ¤lle ist kein eigenes Training notwendig, da es gute **pretrained Modelle** gibt.

---

### ğŸ’» **4. Entwicklungsumgebung**

#### ğŸ”¤ **Programmiersprache:**

* **Python**: Klare Empfehlung, da fast alle relevanten Frameworks hier existieren.
* **Alternative** (falls Performance kritisch ist): Rust oder C++ (z.â€¯B. fÃ¼r Low-Level Audio-Handling).

#### ğŸ§° **Empfohlene Frameworks**:

* `speech_recognition` (Wrapper fÃ¼r verschiedene STT-Systeme)
* `Rasa` (NLU/Dialogmanagement)
* `transformers` von HuggingFace (fÃ¼r moderne NLP)
* `PyTorch` oder `TensorFlow` (fÃ¼r eigene Modelle)
* `Flask` oder `FastAPI`
  * Ich empfehle `Flask`, da man hier nicht nur eine `REST API` bauen kann, sondern auch eine WeboberflÃ¤che. Ãœber die WeboberflÃ¤che kann man ja bspw. TriggersÃ¤tze konfigurieren. 
* `pyaudio` oder `sounddevice` (fÃ¼r Mikrofonzugriff)
* `python-openhab-rest-client` (fÃ¼r openHAB)
* `SQLAlchemy` (fÃ¼r Datenbanken)

Es wird noch eine genauere Planung benÃ¶tigt, wie ein solches System funktonieren soll.

---

### ğŸ§± **5. ArchitekturÃ¼bersicht (Ablauf)**

```plaintext
[Mikrofon] â†’ [Hotword Detection] â†’ [Spracherkennung (ASR)] â†’ [Intent-Erkennung (NLU)]
            â†’ [Antwort (Aktion/Dialog)] â†’ [Text-To-Speech] â†’ [Lautsprecher]
```

Was hier in der Architektur nicht drinnen ist, ist was mit den Intents passiert. Mit diesen kann ich ja zum Beispiel openHAB steuern oder ZustÃ¤nde aus openHAB abfragen. Ich kÃ¶nnte ja auch andere Systeme anbinden. AuÃŸerdem muss ja gar keine Antwort verbal erfolgen. Bei einer `WENN-DANN`-Regel hÃ¤tte ich hier, dass auf eine spezielle Spracheingabe eine spezielle Sprachausgabe erfolgen wÃ¼rde. Ich kann ja auch etwas anderes machen und erhalte dann eine Sprachausgabe oder ich habe eine Spracheingabe ohne Sprachausgabe und mache damit irgendetwas vÃ¶llig anderes (z. B. GerÃ¤te bedienen).

Es fehlt zum Beispiel, ob es eine BenutzeroberflÃ¤che gibt, mit der man `WENN-DANN`-Regel konfiguriert. Es fehlt eine Architekturbetrachtung, ob Datenbanken oder andere Systeme angebunden sind. Es fehlt eine Architekturbetrachtung, wie ein KI-Model angebunden ist. Wie wird was Ã¼berhaupt trainiert?

Dies bedarf noch einer genaueren Planung.

#### Beispiel-Stack (lokal lauffÃ¤hig):

* **Hotword**: Porcupine (Wakeword: â€Hey Kiroâ€œ)
* **ASR**: Vosk (lokal) oder Whisper
* **NLU**: Rasa (mit YAML-Intents)
* **TTS**: Coqui TTS
* **Logik**: Python-Controller, der Aktionen oder Antworten triggert

---

### ğŸ§ª **6. ZusÃ¤tzliche Tipps**

* **Modular entwickeln**: Trenne jeden Schritt in eigene Komponenten (Hotword, STT, NLU, TTS).
* **Offline-fÃ¤higkeit prÃ¼fen**: Whisper, Vosk, Rasa, Coqui kÃ¶nnen lokal laufen.
* **Sicherheit**: Wenn du eine Cloud-LÃ¶sung einsetzt (z.â€¯B. OpenAI API), achte auf Datenschutz.
* **Trigger/Custom Intents**: Rasa erlaubt einfache Erweiterung mit neuen â€Storiesâ€œ oder â€Intentsâ€œ ohne tiefes KI-Training.

Anmerkung: Cloud- oder Web-Anbindungen kÃ¶nnen ja das System jederzeit erweitern. In Abgrenzung zu gÃ¤ngigen Sprachassistenzen lÃ¤uft dieser ja trotzdem lokal. Sollte es Internetprobleme geben, dann verzichtet man nur auf einen Teil der FunktionalitÃ¤ten, aber die Basisfunktionen gehen noch, wÃ¤hrend bei einem typischen Sprachassistent ohne Internetverbindung nichts mehr funktionieren wÃ¼rde.

---

### ğŸš€ Einstiegsvorschlag (fÃ¼r Prototypen)

1. **Raspberry Pi 5 + USB-Mikrofon**
2. Hotword: `Porcupine` (kostenloser Wakeword-Engine)
3. STT: `Whisper` oder `Vosk`
4. NLU: `Rasa` (mit einfachen Intents)
5. TTS: `Coqui TTS` oder Google TTS API
6. Alles orchestriert mit `Python`

---

## Hintergrundwissen

### Unterschied Chatbot vs. Sprachassistent

Prinzipiell gilt:

```
Chateingabe â‰™ Spracheingabe (Speech-to-Text)
Chatausgabe â‰™ Sprachausgabe (Text-to-Speech)
```

Entspricht der Aufbau von einem Chatbot wirklich dem Aufbau eines Sprachassistenten? Nicht wirklich. Sie funktionieren sehr Ã¤hnlich. Ein GroÃŸteil der Architektur kann gleich aufgebaut werden. Man kann auch `Fuzzy Matching` verwenden. Es wird `NLP` in beiden Systemen angewandt. Auch ein Sprachassistent benÃ¶tigt dringend ein `Intent`. Am Beispiel `Fuzzy Matching` wird deutlich, dass dies in einem Chatbot sehr viel notwendiger sein wird, als in einem Sprachassistenten. Tippfehler kann man bspw. mit `Fuzzy Matching` korrigieren, nicht gehÃ¶rte oder verrauschte Worte, kÃ¶nnen damit jedoch nicht korrigiert werden. Bei einem Chatbot kann es mal entstehen, dass einzelne Buchstaben in einem Wort fehlen oder verdreht sind, bei einer Spracherkennung hingegen wird ein Wort entweder erkannt oder nicht erkannt.

### Fuzzy Matching

Das **Fuzzy Matching** kann man im deutschen als unscharfe Suche betiteln. Es ist eine Klasse von String-Matching-Algorithmen, mit der man bestimmte Zeichenketten (Strings) in einer lÃ¤ngeren Zeichenkette oder einem Text suchen bzw. finden kÃ¶nnen sollte.  Es wird daher auch oft als Fuzzy-Suche oder Fuzzy-String-Suche betitelt.

Aus:

[https://www.klippa.com/de/blog/informativ/fuzzy-matching-de/](https://www.klippa.com/de/blog/informativ/fuzzy-matching-de/)

Mit Fuzzy-Matching kann ich darauf reagieren, wenn z. B. nur 80% eines Wortes/Satzes richtig erkannt wurde. Hier mal Beispiele.

```
Gesucht: Bestellnummer

Richtig wÃ¤ren:
Bestellnr.
Bestelnummer
Bestellnumer
Bestellnummmer
...
```

Also mÃ¶gliche Tippfehler oder auch AbkÃ¼rzungen sollen ja darufhin deuten, dass ein- und dasselbe Worte gemeint ist. In Zusammenhang mit Synonymen wÃ¼rde man durch Fuzzy Matching dem Benutzer so extrem viele EingabemÃ¶glichkeiten ermÃ¶glichen. Man formuliert ja nicht nur SÃ¤tze um, man vertippt sich auch mal oder kÃ¼rzt einzelne WÃ¶rter ab.

**Fuzzy Matching** nimmt also Korrekturen vor. Dies kÃ¶nnten im allgemeinen die nachfolgenden sein:

* **EinfÃ¼gen** â€“ HinzufÃ¼gen eines Buchstabens zur VervollstÃ¤ndigung des Wortes (z. B. `Rechnun` wird zu `Rechnung`)
* **LÃ¶schen** â€“ Entfernen eines Buchstabens aus einem Wort (z. B. `Rechnnung` wird zu `Rechnung`)
* **Substitution** â€“ Vertauschen eines Buchstabens, um ein Wort zu korrigieren (z. B. `Technung` wird zu `Rechnung`)
* **Transposition** â€“ Vertauschen von Buchstaben, um ein Wort zu korrigieren (z. B. `Rehcnung` wird zu `Rechnung`)

Jeder Korrektur, die durchgefÃ¼hrt werden muss, wird eine â€Bearbeitungsdistanzâ€œ von 1 zugeschrieben. Die Bearbeitungsdistanzen beeinflussen die oben erwÃ¤hnte Trefferquote. Wenn Sie beispielsweise eine Zeichenfolge mit 11 Zeichen haben und 2 Korrekturen vornehmen mÃ¼ssen, betrÃ¤gt die endgÃ¼ltige Trefferquote **81,81 %**.

```
Berechnung: 100%- 2 / 11= 81.81%  
```

Neben diesen Korrekturen kann **Fuzzy Matching** auch verwendet werden, um Zeichensetzungen, zusÃ¤tzliche WÃ¶rter und fehlende Leerzeichen in Zeichenketten oder Texten zu korrigieren.

---

#### Fuzzy-Matching-Algorithmen

Fuzzy Matching fÃ¤llt in die Kategorie der Methoden, fÃ¼r die es keinen spezifischen Algorithmus gibt, der alle Szenarien und AnwendungsfÃ¤lle abdeckt. Daher werden wir einige der am hÃ¤ufigsten verwendeten und zuverlÃ¤ssigsten Fuzzy-Matching-Algorithmen fÃ¼r die Suche nach ungefÃ¤hren DatenÃ¼bereinstimmungen behandeln:

* Levenshtein-Distanz (LD)
* Hamming-Distanz (HD)
* Damerau-Levenshtein

---

##### Levenshtein-Distanz

Die **Levenshtein-Distanz (LD)** ist eine Fuzzy-Matching-Technik, die zwei Zeichenfolgen beim Vergleich und der Suche nach einer Ãœbereinstimmung berÃ¼cksichtigt. Je hÃ¶her der Wert der Levenshtein-Distanz ist, desto weiter sind die beiden Zeichenfolgen oder â€Begriffeâ€œ von einer identischen Ãœbereinstimmung entfernt.

Wie erhalten wir nun den Wert der Levenshtein-Distanz? Die LD zwischen den beiden Zeichenfolgen entspricht der Anzahl der Ã„nderungen, die erforderlich sind, um eine Zeichenfolge in die andere umzuwandeln. FÃ¼r die LD gelten das EinfÃ¼gen, LÃ¶schen und Ersetzen eines einzelnen Zeichens als Bearbeitungsoperationen.

Nehmen wir an, Sie mÃ¶chten die LD zwischen â€Rechnungsnummerâ€œ und â€Rechnungs-Nr.â€œ messen. Der Abstand zwischen den beiden Begriffen ist â€1 x uâ€œ, â€2 x mâ€œ und â€1 x eâ€œ, was einem Abstand von 4 entsprechen wÃ¼rde. Warum? Weil Sie diese Zeichen hinzufÃ¼gen mÃ¼ssten, um eine Ãœbereinstimmung zu erreichen. Siehe die Beispiele unten.

---

###### Levenshtein-Abstand Beispiel

> **Rechnungnummer** â†’ Rechnung**s**nummer (EinfÃ¼gung von â€**s**â€œ) â€“ Abstand: 1  
> **Rechnung numr** â†’ Rechnungsnu**m**m**e**r (EinfÃ¼gung von â€**m**â€œ & â€**e**â€œ) â€“ Abstand: 2  
> **Rechnung nr** â†’ Rechnungsn**u****m****m****e**r (EinfÃ¼gung von â€**u, m, m, e**â€œ) â€“ Abstand: 4

---

##### Hamming-Distanz

Die **Hamming-Distanz (HD)** unterscheidet sich nicht allzu sehr von der Levenshtein-Distanz. Die Hamming-Distanz wird hÃ¤ufig verwendet, um den Abstand zwischen zwei gleich langen Textabschnitten zu berechnen.

Die HD-Methode basiert auf der **ASCII**-Tabelle (American Standard Code for Information Interchange). Zur Berechnung des Abstandswertes verwendet der Hamming-Distanz-Algorithmus die Tabelle, um den BinÃ¤rcode zu bestimmen, der jedem Buchstaben in den Zeichenketten zugeordnet ist.

---

###### Hamming-Abstand-Beispiel

Nehmen wir die folgenden Textzeichenfolgen â€Numberâ€œ und â€Lumberâ€œ als Beispiel. Wenn wir versuchen, den HD zwischen den Zeichenfolgen zu bestimmen, ist der Abstand nicht 1, wie es mit dem Levenshtein-Algorithmus der Fall wÃ¤re. Stattdessen wÃ¼rde er 10 betragen. Das liegt daran, dass die ASCII-Tabelle einen BinÃ¤rcode von **(1001110)** fÃ¼r den Buchstaben **N** und **(1001100)** fÃ¼r den Buchstaben **L** anzeigt.

Beispielrechnung:

> **D** = N â€“ L = 1001110 â€“ 1001100 = **10**

---

##### Damerau-Levenshtein

Das Damerau-Levenshtein-Verfahren misst auch den Abstand zwischen zwei WÃ¶rtern, indem es die erforderlichen Ã„nderungen misst, die vorgenommen werden mÃ¼ssen, um ein Wort an das andere anzupassen. Diese Ã„nderungen hÃ¤ngen von der Anzahl der Operationen ab, wie z. B. EinfÃ¼gung, LÃ¶schung oder Ersetzung eines einzelnen Zeichens oder Transposition zweier benachbarter Zeichen.

Hier unterscheidet sich die Damerau-Levenshtein-Distanz von der regulÃ¤ren Levenshtein-Distanz, da sie zusÃ¤tzlich zu den Einzelzeichen-Editieroperationen, auch Transpositionen berÃ¼cksichtigt, um eine ungefÃ¤hre Ãœbereinstimmung zu finden (Fuzzy Match).

---

###### Damerau-Levenshtein Beispiel

> **Zeichenfolge 1:** Re<strong>ch</strong>nun<strong>g</strong>  
> **Zeichenfolge 2:** Re<strong>hc</strong>nun  
>   
> **Operation 1:** Transposition â†’ Vertauschen der Zeichen â€**h**â€œ und â€**c**â€œ  
> **Operation 2:** EinfÃ¼gen eines â€**g**â€œ am Ende der Zeichenfolge 2


Da zwei Operationen erforderlich waren, um die beiden WÃ¶rter identisch zu gestalten, **betrÃ¤gt der Abstand 2**. Vereinfacht ausgedrÃ¼ckt zÃ¤hlt jede Operation wie EinfÃ¼gung, LÃ¶schung, Transposition usw. als ein Abstand von â€1â€œ. Mit der Levenshtein-Distanz mÃ¼ssten Sie jedoch drei Korrekturen vornehmen, was einem Abstand von 3 entspricht.

Alle oben genannten Fuzzy-Matching-Algorithmen unterscheiden sich natÃ¼rlich in der Art und Weise, wie die Bearbeitungsdistanz berechnet wird. Dies ist der Grund, warum es keinen FM-Algorithmus gibt, der fÃ¼r alle geeignet ist. Von den drei vorgestellten Algorithmen ist die Levenshtein-Distanz jedoch der am hÃ¤ufigsten verwendete FM-Algorithmus in der Datenverwaltung und Datenwissenschaft.

Empfehlung: In Python kann man die [fuzzywuzzy-Bibliothek](https://github.com/seatgeek/fuzzywuzzy) testen oder einen eigenen Algorithmus implementieren.

---

#### ğŸ§  **Wird Fuzzy Matching fÃ¼r einen Sprachassistenten benÃ¶tigt?**

##### âœ… **Kurzantwort:**

**Ja, in bestimmten Komponenten eines Sprachassistenten kann Fuzzy Matching nÃ¼tzlich sein â€“ aber es ist nicht zwingend erforderlich** und wird **nicht immer eingesetzt**, insbesondere dann nicht, wenn du moderne NLU- oder KI-Modelle verwendest.

---

#### ğŸ“Œ **Wann ist Fuzzy Matching sinnvoll?**

Fuzzy Matching kommt dann zum Einsatz, wenn:

* **Benutzereingaben ungenau, variabel oder fehlerhaft** sind (z.â€¯B. Tippfehler, unterschiedliche Formulierungen).
* **Keine trainierten oder semantischen Modelle verfÃ¼gbar** sind, etwa bei regelbasierten Assistenten.
* Du **einfach strukturierte Sprachbefehle mit statischen Antworten** hast.

##### Typische Einsatzszenarien:

| Anwendung                                                   | Fuzzy Matching notwendig? | Alternative bei moderner Architektur          |
| ----------------------------------------------------------- | ------------------------- | --------------------------------------------- |
| MenÃ¼systeme / Chatbots mit festen Befehlen                  | âœ… Ja                      | Regelbasierte Intenterkennung                 |
| Sprachassistent mit NLU-Modell (z.â€¯B. Rasa)                 | âŒ Eher nein               | Verwendung von ML-basierten Parsers           |
| Wakeword-Erkennung                                          | âŒ Nein                    | Hier nutzt man z.â€¯B. CNNs, kein Text-Matching |
| Dateinamen oder Orte erkennen (â€Spiel *Bohemian Rhapsody*â€œ) | âœ… Ja (optional)           | Levenshtein, Jaro-Winkler, etc.               |

---

#### ğŸ”„ **Beispiel: Fuzzy Matching vs. NLU**

##### Ohne NLU:

```python
if "spiel musik" in user_input:
    do_music()
elif "spiel musick" in user_input:  # Tippfehler!
    do_music()  # geht nicht ohne Fuzzy Matching
```

##### Mit Fuzzy Matching:

```python
from fuzzywuzzy import fuzz
if fuzz.partial_ratio("spiel musik", user_input) > 80:
    do_music()
```

##### Mit NLU (z.â€¯B. Rasa):

```yaml
- intent: play_music
  examples: |
    - spiel musik
    - spiel bitte musik
    - spiele ein lied
    - mach musik an
```

â†’ NLU erkennt die Absicht, auch wenn der Text variiert oder leicht fehlerhaft ist. Kein Fuzzy Matching nÃ¶tig.

---

#### ğŸ§© **Fazit**

| Wenn du ...                            | Dann ...                                      |
| -------------------------------------- | --------------------------------------------- |
| ... nur einfache Textvergleiche machst | Fuzzy Matching kann helfen                    |
| ... moderne NLU-Modelle nutzt          | Fuzzy Matching ist **nicht nÃ¶tig**            |
| ... Intents regelbasiert zuweist       | Fuzzy Matching kann Fehler abfedern           |
| ... mit Sprache (nicht Text) arbeitest | ASR-Fehler kÃ¶nnen durch NLU abgefangen werden |

---

#### ğŸ’¡ Empfehlung fÃ¼r deine Bachelorarbeit:

* **Nutze Fuzzy Matching nur bei Bedarf**, z.â€¯B. fÃ¼r regelbasierte Systeme oder spezifische Trigger (z.â€¯B. Namen, Ortslisten).
* Bei **modernem Aufbau mit Rasa, transformers oder GPT-basierten Komponenten**: Verzichte darauf und verlasse dich auf semantische Modelle.

Ich denke in einer gut ausgearbeiteten Abschlussarbeit erlÃ¤utert man diesen Verzicht auch, weil es sehr viel darÃ¼ber sagt, was in der Theorie ein System alles kÃ¶nnen muss.

### Intent, Entity, Confidence Score

Aus:

[https://www.melibo.de/blog/was-sind-intent-und-entity](https://www.melibo.de/blog/was-sind-intent-und-entity)

---

#### Intent

Intents, zu Deutsch â€Absichtenâ€œ, sind Zwecke oder Ziele, die in den Eingaben eines Kunden zum Ausdruck kommen, um z.B. eine Frage zu einer Retoure zu stellen. Durch die Erkennung der Absicht, die sich in der Kundeneingabe ausdrÃ¼ckt, versucht der KI-Chatbot den richtigen Dialog zu finden und die passende Ausgabe zu wÃ¤hlen. DafÃ¼r nutzen KI-Chatbots maschinelles Lernen, um in natÃ¼rlicher Sprache die vorher definierte Absicht (Intent) zu erkennen. [1] Einfach gesagt, Intents sind Fragen der User:innen, die dem Chatbot zu einem speziellen Thema gestellt werden und der Versuch des KI-Chatbots, die passende Antwort zu erkennen, um das Problem zu lÃ¶sen bzw. die Frage zu beantworten.

---

##### Wie funktionieren Intents?

Bestehende Anbieter wie unter anderem der IBM Watson Assistant [1], Rasa [2] oder Microsoft LUIS [3] basieren alle meist auf dem Prinzip der Intent-Ausgabe. Bevor der Chatbot Intents erkennen kann, mÃ¼ssen erst mal alle Absichten der User:innen definiert werden. HierfÃ¼r ist es wichtig, dass man seine Kundenanfragen erst mal identifiziert und seinen Use-Case richtig versteht. Nachdem der Intent-Katalog erstellt und der Bot online genommen wurde, werden die User:innen dem Chatbot Fragen stellen. Jede Anfrage der User:innen durchlÃ¤uft das sogenannte Intent-Matching, also der Zuordnung der Anfrage aus den gesamten Inhalten des Chatbots. Dabei wird anhand von NLP (Natural Language Processing) ein Confidence-Score berechnet, um anhand von Wahrscheinlichkeiten die passende Antwort auszugeben.

---

#### Confidence-Score

Ein kurzer Exkurs zum Thema Confidence-Scores. Die Confidence-Scores liegen zwischen 0 und 1 und geben an, zu wie viel Prozent der Chatbot ein Intent erkannt hat. Zu jeder gestellten Frage der User:innen berechnet der Chatbot also einen Confidence-Score und versucht auf dieser Grundlage, durch Wahrscheinlichkeiten, die richtige Antwort an die User:innen auszugeben. Die Confidence-Scores sind meistens voreingestellt und liegen zwischen 0,6 und 0,7. Das heiÃŸt, dass der Chatbot Antworten nur dann ausgibt, wenn die Erkennungswahrscheinlichkeit bei mindestens 60 % liegt. Nehmen wir nun als Beispiel an, dass User:innen die Frage stellen â€Was kannst du so?â€œ, um zu erfahren, welche Themen der Chatbot Ã¼berhaupt beantworten kann. Der KI-Chatbot erkennt zu 89 % Prozent den Intent â€Was kannst du?â€œ. In diesem Beispiel gibt der Chatbot die passende Antwort aus und beantwortet somit die Frage.

---

##### Bestandteile eines Intents

Ein Intent besteht also aus dem: Intentnamen, dem User-Input, dem Confidence-Score und einer Antwort. AbhÃ¤ngig von der genutzten Technologie kÃ¶nnen noch weitere Bestandteile dazu kommen, wie Variablen, Actions und Entities.

Ein Beispiel fÃ¼r Intents

(1) User-Input: Frage eines Users

ğŸ™â€â™‚ï¸: â€Vorteile eines Chatbotsâ€œ
ğŸ™â€â™‚ï¸: â€Was kÃ¶nnen Chatbots besonders gut?â€œ
ğŸ™â€â™‚ï¸: â€Warum sollte ich einen Chatbot holen?â€œ

(2) Confidence-Score und (3) Intentnamen â€Vorteile Chatbotâ€œ: Berechnung der Wahrscheinlichkeit anhand vom User-Input

ğŸ§® : 100 % Erkennung des Intents â€Vorteile Chatbotâ€œ

(4) Antwort des Chatbots auf die Frage â€Vorteile Chatbotsâ€œ

ğŸ¤– : Zu den Vorteilen von Chatbots gehÃ¶ren unter anderem und abhÃ¤ngig von der Branche: Automatisierung von Prozessen, wodurch Fehler beim Support reduziert sowie Zeit und Geld eingespart werden kÃ¶nnen. VerkÃ¼rzte Wartezeiten fÃ¼r den Kunden. 24/7 Kundensupport. Effizientere Strukturen. Weniger manueller Aufwand fÃ¼r dich.

---

#### Entity

Im Unterschied zu Intents dienen Entitys oder auch Entities dazu, Informationen der User:innen aus der natÃ¼rlichen Sprache zu extrahieren. Jedes Entity verfÃ¼gt Ã¼ber eine Reihe von Eigenschaften, die mit ihr verbunden sind. Dabei kannst du auf Informationen deines Entities zugreifen. Wie bei einem Intent gibt der Chatbot an, wie hoch der Confidence-Score liegt. Im Unterschied zu Intents liegt der Confidence-Score, aber bei 0 oder 1. UnabhÃ¤ngig davon, ob und wie die Erkennung des Entitys eingestellt ist, haben sogenannte System-Entites immer eine Erkennung von 1. Jedes Entity besitzt einen Wert, einen sogenannten EntitÃ¤tswert. Bei der Erstellung von Entities ist es notwendig, dass neben dem Wert auch Typen definiert werden. Unter Typen versteht man allgemein Synonyme, also WÃ¶rter, die sich ebenfalls auf dasselbe Vorhaben beziehen, es nur anders umschreiben. Je mehr Synonyme ein Entity hat, desto besser die Erkennung des Chatbots [4].

GrundsÃ¤tzlich unterscheiden wir zwischen System-Entities und Customize-Entities. Die System-Entites sind voreingestellt, das heiÃŸt im System bereits enthalten. Darunter fallen etwa Zahlen, Uhrzeiten oder Adressen. Diese Entities sind besonders beliebt und wurden in der Vergangenheit besonders hÃ¤ufig verwendet. Die Customize-Entities dagegen sind selbst definierte Werte, die auf den jeweiligen Use-Case angepasst werden.

---

##### Ein Beispiel fÃ¼r Entities in der Praxis

ğŸ™â€â™‚ï¸: â€Wann kommt mein Produkt Chatbot-Experte in der LandstraÃŸe 5 an?â€œ

---

##### Entities in diesem Beispiel:

Produkt Chatbot Experte (Customize-Entity product_type)
LandstraÃŸe 5 (Sytem-Entity street_adress)

---

##### Vorteile von Entites:

Mit Entities kann man User:innen durch Chat Flows in Form von Buttons navigieren, schnell Synonyme definieren und mehrere Anfragen auf einmal verarbeiten. Die System-Entitys helfen auÃŸerdem dabei, die beliebten Anfragen zum Thema Ort, Zeit und Adresse abzufangen.

[1]: https://cloud.ibm.com/docs/assistant?topic=assistant-intents
[2]: https://rasa.com/open-source/
[3]: https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-utterance
[4]: https://cloud.ibm.com/docs/assistant?topic=assistant-expression-language

---

#### Speech Synthesis Markup Language (SSML)

**SSML (Speech Synthesis Markup Language)** ist eine XML-basierte Auszeichnungssprache, mit der du **Text-to-Speech (TTS)**-Ausgaben prÃ¤zise steuern kannst â€“ also wie ein Sprachsynthesizer den gesprochenen Text betonen, pausieren oder variieren soll.

---

##### ğŸ§  Was kann man mit SSML machen?

Mit SSML kannst du steuern:

* ğŸ—£ **Aussprache** (`<phoneme>`)
* â¸ **Pausen** (`<break>`)
* ğŸµ **TonhÃ¶he, LautstÃ¤rke, Sprechgeschwindigkeit** (`<prosody>`)
* ğŸ“¢ **Sprachstil / Emphasis** (`<emphasis>`, `<voice>`)
* ğŸŒ **Sprache und Stimme** (`<lang>`, `<voice>`)
* ğŸ” **Audioeinspielung** (`<audio>` â€“ je nach Engine)

---

##### âœ… Einfaches Beispiel

```xml
<speak>
  Hallo! <break time="500ms"/> Wie kann ich dir helfen?
</speak>
```

â± FÃ¼gt eine **halbe Sekunde Pause** ein.

---

##### ğŸ“Œ HÃ¤ufig genutzte SSML-Tags (Auswahl)

| Tag          | Funktion                                | Beispiel                                              |
| ------------ | --------------------------------------- | ----------------------------------------------------- |
| `<speak>`    | Wurzel jeder SSML-Ausgabe               | `<speak>Text hier</speak>`                            |
| `<break>`    | FÃ¼gt Pause ein                          | `<break time="300ms"/>`                               |
| `<prosody>`  | Steuert TonhÃ¶he, LautstÃ¤rke, Tempo      | `<prosody pitch="+10%" rate="slow">...</prosody>`     |
| `<emphasis>` | Betonung auf Wort/Satz                  | `<emphasis level="strong">wichtig</emphasis>`         |
| `<phoneme>`  | Definiert Lautschrift                   | `<phoneme alphabet="ipa" ph="ËˆhÃ¦loÊŠ">Hallo</phoneme>` |
| `<lang>`     | Schaltet auf andere Sprache             | `<lang xml:lang="en-US">Hello!</lang>`                |
| `<audio>`    | Spielt Audiodatei ab (nur in Cloud-TTS) | `<audio src="sound.mp3"/>`                            |

Anmerkung: Gute Entwickler kÃ¶nnen auch Mechanismen entwickeln, wie man solche SSML-Tags selbst in Libraries umsetzt, die dies nicht von sich aus unterstÃ¼tzen. SÃ¤tze mÃ¼ssen dann dann unter UmstÃ¤nden aufgeteilt werden, mehrere Audiodateien erstellt und am Ende zusammengefÃ¼gt werden. Dies wÃ¤re aber durchaus ein extrem komplexes und durchaus nicht sicheres und instabiles Prozedere.

---

##### ğŸ™ Beispiel: Mehrsprachiger Text mit Betonung

```xml
<speak>
  Willkommen bei deinem Sprachassistenten. <break time="300ms"/>
  <emphasis level="strong">Heute</emphasis> ist ein schÃ¶ner Tag.
  <lang xml:lang="en-US">The weather is sunny and warm.</lang>
</speak>
```

---

##### âš™ï¸ KompatibilitÃ¤t

| TTS-Engine            | SSML-UnterstÃ¼tzung | Hinweis                               |
| --------------------- | ------------------ | ------------------------------------- |
| **Google TTS**        | âœ… VollstÃ¤ndig      | auch `<audio>`-Einbindung mÃ¶glich     |
| **Amazon Polly**      | âœ… Sehr gut         | unterstÃ¼tzt viele `<voice>`-Stile     |
| **Coqui TTS**         | âš ï¸ Teilweise       | `<break>`, `<prosody>` oft verfÃ¼gbar  |
| **eSpeak / Festival** | âŒ Kaum             | meist keine native SSML-UnterstÃ¼tzung |

---

##### ğŸ›  Tipps fÃ¼r die Praxis

* SSML hilft, die **NatÃ¼rlichkeit und VerstÃ¤ndlichkeit** von Sprachausgaben zu steigern.
* Nutze es gezielt bei langen Dialogen, AufzÃ¤hlungen oder multilingualem Content.
* FÃ¼r lokale Assistenten mit z.â€¯B. `pyttsx3` ist SSML oft **nicht direkt nutzbar** â€“ dort musst du ggf. Pausen und TonhÃ¶he manuell steuern.

---

Ja, **es gibt mehrere Text-to-Speech-LÃ¶sungen, die vollstÃ¤ndig lokal arbeiten**, also **ohne Cloudverbindung** funktionieren. Das ist besonders wichtig fÃ¼r Datenschutz, OfflinefÃ¤higkeit und Embedding in Systeme wie deinen Sprachassistenten.

---

##### ğŸ—£ï¸ **Lokale TTS-Libraries â€“ ohne Cloud**

Hier ist eine Ãœbersicht:

| Library / Tool | Sprache | Lokal | QualitÃ¤t | SSML-UnterstÃ¼tzung     | Bemerkung                              |
| -------------- | ------- | ----- | -------- | ---------------------- | -------------------------------------- |
| **Coqui TTS**  | Python  | âœ…     | â­â­â­â­â˜†    | Teilweise (SSML-light) | Moderne TTS, sehr anpassbar            |
| **eSpeak NG**  | C/C++   | âœ…     | â­â­â˜†â˜†â˜†    | âŒ                      | Extrem leichtgewichtig, viele Sprachen |
| **Festival**   | C++     | âœ…     | â­â­â˜†â˜†â˜†    | âŒ                      | Veraltet, aber stabil                  |
| **RHVoice**    | C++     | âœ…     | â­â­â­â˜†â˜†    | âŒ                      | Gute QualitÃ¤t, Open Source             |
| **pyttsx3**    | Python  | âœ…     | â­â­â˜†â˜†â˜†    | âŒ                      | Wrapper um native Engines              |
| **MaryTTS**    | Java    | âœ…     | â­â­â­â˜†â˜†    | âœ…                      | Alte, aber robuste Plattform mit SSML  |
| **OpenTTS**    | Docker  | âœ…     | â­â­â­â­â˜†    | Ja (je nach Backend)   | Aggregiert Coqui, MaryTTS, etc.        |

---

##### ğŸ” Kurze Details zu den wichtigsten:

###### ğŸ”¸ **Coqui TTS**

* Modernes, Deep-Learning-basiertes System (Fork von Mozilla TTS)
* Offline trainierbar oder fertige Modelle nutzbar
* CLI & Python API
* Stimmen fÃ¼r viele Sprachen, inkl. Deutsch
* EingeschrÃ¤nkte SSML-Ã¤hnliche Features (`<break>`, `rate`, `pitch` per CLI/Args)

ğŸ‘‰ Empfohlen fÃ¼r dein Projekt, wenn du **QualitÃ¤t + LokalitÃ¤t** brauchst

---

###### ğŸ”¸ **eSpeak NG**

* Extrem leicht (\~2MB)
* UnterstÃ¼tzt 100+ Sprachen, aber Robot-Ã¤hnliche Stimme
* Ideal fÃ¼r embedded / low-resource-Systeme

ğŸ‘‰ FÃ¼r Prototypen oder Systeme mit **minimaler Leistung**

---

###### ğŸ”¸ **pyttsx3**

* Python-Wrapper um native TTS-Engines (Windows: SAPI5, macOS: NSSpeechSynthesizer, Linux: espeak)
* Einfach zu benutzen, aber begrenzte Kontrolle Ã¼ber Stimme und Stil
* Keine SSML-UnterstÃ¼tzung, nur rudimentÃ¤res Timing

ğŸ‘‰ Gut fÃ¼r **schnelle Demos** oder klassische Desktop-Sprachsynthese

---

###### ğŸ”¸ **MaryTTS**

* Java-basiert, mit Webserver & REST-API
* UnterstÃ¼tzt echtes SSML
* Deutsche Stimme enthalten
* LÃ¤ngere Startzeit, komplexer Aufbau

ğŸ‘‰ FÃ¼r **akademische oder stabile serverseitige Anwendungen**

---

##### âœ… Empfehlung fÃ¼r Bachelorarbeit

| Ziel                                     | Empfehlung                         |
| ---------------------------------------- | ---------------------------------- |
| Moderne TTS, lokale Nutzung, erweiterbar | **Coqui TTS**                      |
| Einfache Integration, geringe Ressourcen | **pyttsx3** oder **eSpeak NG**     |
| VollstÃ¤ndige SSML-Verarbeitung lokal     | **MaryTTS** oder Coqui (teilweise) |

---

## ğŸ“‘ **Gliederungsvorschlag fÃ¼r die Bachelorarbeit**

Hier ist ein **Vorschlag fÃ¼r die Gliederung deiner Bachelorarbeit**, passend zum Thema *â€Entwicklung eines lokalen Sprachassistentenâ€œ*. Die Gliederung ist modular, logisch aufgebaut und erlaubt sowohl technische Tiefe als auch theoretische Reflexion.

### 1. **Einleitung**

* Motivation und Relevanz des Themas
* Zielsetzung der Arbeit
* Abgrenzung und Umfang
* Aufbau der Arbeit

---

### 2. **Theoretische Grundlagen**

* Sprachassistenten: Definition und Bestandteile
* Ãœberblick Ã¼ber aktuelle Systeme (Alexa, Siri, Mycroft, etc.)
  * Vor- und Nachteile (Datenschutz, Cloudanbindung, keine Smart Home Integration wie bspw. openHAB, usw.)
* Komponenten im Detail:

  * Hotword Detection
  * Automatic Speech Recognition (ASR)
    * ErklÃ¤re ebenfalls am besten STT 
  * Natural Language Understanding (NLU)
    * ErklÃ¤re ebenfalls am besten NLP und warum NLU eine Teilmenge ist. 
  * Text-to-Speech (TTS)
* Datenschutz & Offline-Verarbeitung
* Einordnung in bestehende Arbeiten (State of the Art)

Was nicht schaden kann ist, wenn man hier noch KI-Grundlagen irgendwie erlÃ¤utert und erklÃ¤rt. Man hat ja verschiedene BerÃ¼hrungspunkte, weil man ja auch Modelle trainiert. Eventuell muss man ja auch Begriffe wie `Reinforcement Learning` beschreiben. Spracherkennungssysteme neigen meist zu `Ãœbertraining` (`overfitting`), was wÃ¤re dies denn Ã¼berhaupt erst? Da kann man mit theoretische Grundlagen sehr tief ins Thema einsteigen.

---

### 3. **Anforderungsanalyse**

* Funktionale Anforderungen (z.â€¯B. Hotword, STT, NLU)
* Nicht-funktionale Anforderungen (z.â€¯B. Offline-Betrieb, ModularitÃ¤t)
* Zielplattform (z.â€¯B. Raspberry Pi vs. Mini-PC)
* Auswahlkriterien fÃ¼r Frameworks und Komponenten
* Smart Home Integration (z. B. openHAB)

---

### 4. **Konzept und Architektur**

* Architekturentwurf des Sprachassistenten
* Beschreibung der Systemkomponenten

  * Audioaufnahme & Verarbeitung
  * Modulinteraktion
* Kommunikationsschnittstellen
* Datenflussdiagramm

---

### 5. **Implementierung**

* Projektstruktur & Technologie-Stack
* Beschreibung der wichtigsten Module:

  * Hotword-Erkennung (z.â€¯B. Porcupine)
  * Spracherkennung (z.â€¯B. Whisper/Vosk)
  * Intent-Erkennung (z.â€¯B. Rasa/Regex)
  * Textausgabe (z.â€¯B. Coqui TTS)
* Erweiterbarkeit & Custom Intents
* BeispielablÃ¤ufe / Interaktionen

Ich sage hier nur vielleicht. Man sollte sich nicht zu stark an diesen Libraries orientieren.

---

### 6. **Evaluation**

* Testmethodik (z.â€¯B. definierte Szenarien, Vergleich mit CloudlÃ¶sungen)
* Messkriterien:

  * Erkennungsrate
  * Latenzzeit
  * Ressourcenverbrauch
* Vergleich von Alternativen (z.â€¯B. Whisper vs. Vosk)
* Diskussion der Ergebnisse

Eine Machbarkeitsstudie macht man ja so oder so allgemein. In Bezug auf "Machtbarkeit" kann man ja zeigen, ob ein ehemaliges Szenario von Alexa und openHAB sich nun mit dem neu entwickelten System umsetzen lÃ¤sst. Wenn ja, dann hat man es ja geschafft ein "Konkurrenzprodukt" zu entwickeln, welches cloudunabhÃ¤ngig funktioniert, somit lokal ist, datenschutztechtlich unbedenklich, usw.

---

### 7. **Reflexion und Ausblick**

* Bewertung der Zielerreichung
* Technische und konzeptionelle Herausforderungen
* Verbesserungspotenziale
* Ausblick auf mÃ¶gliche Erweiterungen (z.â€¯B. Dialogmanagement, IoT-Anbindung)

Ich denke eins wird klar sein: Alexa nutzt bspw. verschiedene Skills. Mit diesen Skills lassen sich Apps, Anwendungen, GerÃ¤te von sehr vielen verschiedenen Anbietern integrieren. Auch, weil Skills communitybasiert oder durch Firmen/Organistationen entwickelt werden. HeiÃŸt viele BedienmÃ¶glichkeiten und Szenarien Ã¼ber viele Skills/Anwendungen sind hier natÃ¼rlich nicht integrierbar und am Ende nutzbar. Dies nimmt einen Nutzer auch eine gewisse KomfortabilitÃ¤t und einen gewissen Nutzen. WÃ¤ren aber bspw. manche Skills umsetzbar oder mÃ¼sste man die komplette Architektur des Prototypen umschreiben? KÃ¶nnte man z. B. GerÃ¤te von Somfy, Philips Hue oder Sonos auch direkt anbinden oder geht dies nur Ã¼ber ein Smart Home System wie openHAB?

---

### 8. **Fazit**

* Zusammenfassung der Arbeit
* Wichtigste Erkenntnisse
* PersÃ¶nliche Bewertung

---

### 9. **Anhang**

* CodeauszÃ¼ge
* Konfigurationsdateien
* Screenshots / Ablaufdiagramme
* Testprotokolle

---

### 10. **Literaturverzeichnis**

* Fachliteratur
* Dokumentationen verwendeter Tools
* Onlinequellen mit Zugriffsdatum

---

## Beispielprojekt

Hier ist ein **Beispielprojekt fÃ¼r einen lokalen Sprachassistenten** mit Fokus auf ModularitÃ¤t, VerstÃ¤ndlichkeit und Erweiterbarkeit â€“ ideal als Startpunkt fÃ¼r deine Bachelorarbeit.

Anmerkung: Das ist vom zu entwickelten Prototypen noch meilenweit entfernt.

---

### ğŸ“ Projekt-Setup: `voice_assistant/`

```plaintext
voice_assistant/
â”œâ”€â”€ main.py                       # Einstiegspunkt: orchestration der Komponenten
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml             # Konfiguration (Hotword, STT, NLU, TTS)
â”œâ”€â”€ hotword/
â”‚   â””â”€â”€ detector.py               # Hotword-Erkennung (Porcupine / Precise)
â”œâ”€â”€ stt/
â”‚   â””â”€â”€ transcriber.py            # Speech-to-Text (Whisper / Vosk)
â”œâ”€â”€ nlu/
â”‚   â””â”€â”€ intent_parser.py          # Intent-Erkennung (Rasa / Regex)
â”œâ”€â”€ tts/
â”‚   â””â”€â”€ synthesizer.py            # Text-to-Speech (Coqui TTS)
â”œâ”€â”€ core/
â”‚   â””â”€â”€ actions.py                # Antwortlogik / Aktionshandler
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ audio_utils.py            # Mikrofonaufnahme, Audio-Konvertierung
â”œâ”€â”€ data/
â”‚   â””â”€â”€ intents.yaml              # Trainingsdaten fÃ¼r Intents
â””â”€â”€ requirements.txt              # Python-AbhÃ¤ngigkeiten
```

---

### ğŸ§  KomponentenÃ¼berblick mit Mini-Codebeispielen

#### 1. `main.py`

```python
from hotword.detector import wait_for_hotword
from stt.transcriber import transcribe_audio
from nlu.intent_parser import parse_intent
from tts.synthesizer import speak
from core.actions import handle_intent

def main():
    print("System bereit. Sag das Hotword...")
    while True:
        wait_for_hotword()
        audio = utils.audio_utils.record_input()
        text = transcribe_audio(audio)
        intent = parse_intent(text)
        response = handle_intent(intent)
        speak(response)

if __name__ == "__main__":
    main()
```

---

#### 2. `hotword/detector.py` (z.â€¯B. mit `pvporcupine`)

```python
import pvporcupine
import pyaudio

def wait_for_hotword():
    porcupine = pvporcupine.create(keywords=["computer"])
    audio_stream = pyaudio.PyAudio().open(
        rate=porcupine.sample_rate,
        channels=1,
        format=pyaudio.paInt16,
        input=True,
        frames_per_buffer=porcupine.frame_length,
    )
    while True:
        pcm = audio_stream.read(porcupine.frame_length)
        pcm = list(int.from_bytes(pcm[i:i+2], 'little', signed=True) for i in range(0, len(pcm), 2))
        if porcupine.process(pcm) >= 0:
            break
```

---

#### 3. `stt/transcriber.py` (mit `whisper`)

```python
import whisper

model = whisper.load_model("base")

def transcribe_audio(audio_path="input.wav"):
    result = model.transcribe(audio_path)
    return result["text"]
```

---

#### 4. `nlu/intent_parser.py` (Regel- oder Rasa-basiert)

```python
import yaml

with open("data/intents.yaml", "r") as f:
    intent_data = yaml.safe_load(f)

def parse_intent(text):
    text = text.lower()
    for intent, patterns in intent_data.items():
        for pattern in patterns:
            if pattern in text:
                return intent
    return "unknown"
```

Beispiel fÃ¼r `data/intents.yaml`:

```yaml
play_music:
  - spiel musik
  - mach musik an
  - spiele ein lied
get_time:
  - wie spÃ¤t ist es
  - sag mir die uhrzeit
  - wie viel uhr
```

---

#### 5. `tts/synthesizer.py` (z.â€¯B. mit `pyttsx3` oder `coqui-tts`)

```python
import pyttsx3

engine = pyttsx3.init()

def speak(text):
    engine.say(text)
    engine.runAndWait()
```

---

#### 6. `core/actions.py`

```python
from datetime import datetime

def handle_intent(intent):
    if intent == "get_time":
        return f"Es ist {datetime.now().strftime('%H:%M')} Uhr."
    elif intent == "play_music":
        return "Okay, ich spiele Musik ab."
    else:
        return "Das habe ich leider nicht verstanden."
```

---

### ğŸ“¦ `requirements.txt`

```txt
pvporcupine
pyaudio
whisper
torch
pyttsx3
pyyaml
```

---

### ğŸ› ï¸ Was du jetzt damit machen kannst

* ğŸ”„ Eigene Intents und Aktionen hinzufÃ¼gen
* ğŸ” Komponenten austauschen (z.â€¯B. Whisper â†’ Vosk, pyttsx3 â†’ Coqui TTS)
* ğŸ§ª Performance- oder Vergleichsexperimente implementieren
* ğŸ” Datenschutz-Konzept einbauen (lokale vs. Cloud-Verarbeitung vergleichen)

---
