# Bachelorarbeit: Entwicklung eines modularen, datenschutzfreundlichen und lokalen Sprachassistenten

---

## MÃ¶gliche TitelvorschlÃ¤ge

- "Entwicklung eines datenschutzfreundlichen Sprachassistenten auf Basis freier Software"
- "Entwicklung eines datenschutzfreundlichen Sprachassistenten auf Basis von Open-Source-Software"
- "Vergleich von ASR-Engines fÃ¼r Offline-Sprachassistenten"
- "Modularer Sprachassistent fÃ¼r den lokalen Einsatz: Architektur und Prototyp"
- "Sprachverarbeitung ohne Cloud: Konzeption und Evaluation eines lokalen Sprachassistenten"
- "Custom Wakeword Detection in Embedded Systems: Architektur, Training und Einsatz"
- "Open Source Sprachassistent: Ein systemischer Vergleich freier Komponenten"
- ...

---

## Hintergrund

Sprachassistenten wie Alexa, Siri oder Google Assistant haben sich im Alltag vieler Menschen etabliert. Allerdings sind diese Systeme oft cloudgebunden, datenschutzrechtlich bedenklich und schwer individualisierbar. Dies erÃ¶ffnet spannende MÃ¶glichkeiten fÃ¼r die Entwicklung eines eigenen, lokal laufenden Sprachassistenten mit freier Wahl der Komponenten.

---

### Problemstellung im Detail

- Cloudverbindungen kÃ¶nnen auch Ã¶fter mal gestÃ¶rt oder abgebrochen sein.
  - Ein lokaler Sprachassistent wÃ¼rde zumindest lokal funktionierende Befehle bearbeiten und lokal verfÃ¼gbare Informationen zur VerfÃ¼gung stellen kÃ¶nnen.
- Datenschutzrechtlich hÃ¶rt ein Sprachassistent dauerhaft mit und wartet, bis ein sog. Hotword (auch trigger word oder wake word) erkannt wird.
  - Ist wirklich garantiert, dass nur Daten aufgezeichnet und an eine Cloud gesendet werden, wenn ein Hotword verwendet wurde oder lauscht das System dauerhaft mit?
    - Rein theoretisch kÃ¶nnte man Ã¼ber Wireshark prÃ¼fen, ob ein Sprachassistent wie bspw. Alexa nicht stÃ¤ndig mit ihrer Cloud kommunziert. Die Annahme wÃ¤re natÃ¼rlich sehr groÃŸ, dass wenn dauerhaft kommuniziert wird, dass Amazon abhÃ¶rt und lauscht, nicht nur nach einem Hotword aktiv Daten aufzeichnet.
  - Anmerkung: Deutsche Begriffe sind Aktivierungswort, Aufwachwort, Aufwachbefel, Triggerwort, usw.
- Sprachassistenten sind oft nur beschrÃ¤nkt individualisierbar, Anwendungen werden eingestellt oder eingeschrÃ¤nkt.
  - Google Nest ermÃ¶glicht es oft bspw. Smart Home GerÃ¤te zwar anzubinden, diese kÃ¶nnen darÃ¼ber aber nicht gesteuert werden, sondern meist nur ZustÃ¤nde abgefragt werden.
  - Amazon hat bspw. ihre Policy geÃ¤ndert und seitdem wird das openHAB Skill aus SicherheitsgrÃ¼nden nicht mehr unterstÃ¼tzt, damit wÃ¤re ein Smart Home, welches Ã¼ber openHAB gesteuert wurde nicht mehr bedienbar.

---

## Ziel der Arbeit

Ziel der Arbeit ist die Entwicklung und prototypische Umsetzung eines lokalen Sprachassistenzsystems auf Basis freier Software und Hardware. Der Fokus liegt auf ModularitÃ¤t, Erweiterbarkeit und Datenschutzfreundlichkeit. Es sollen verschiedene Komponenten (Spracherkennung, Hotword-Erkennung, Intent-Parsing, Text-to-Speech) integriert und ggf. evaluiert werden. Es muss eine Anbindung zur openHAB REST API mÃ¶glich sein.

---

## Teilziele

- Aufbau einer modularen Architektur fÃ¼r Sprachassistenz
- Integration von Hotword-Detection, ASR (Automatic Speech Recognition, hier: Speech-to-Text), NLU (Natural Language Understanding, hier: Intent-Erkennung ist eine Teilmenge des Natural Language Processings), TTS (Text-to-Speech)
- (Optional) Vergleich verschiedener frei verfÃ¼gbarer ASR/NLU/TTS-Frameworks hinsichtlich:
  - Latenz
  - Genauigkeit
  - Ressourcenverbrauch
- Anpassbarkeit durch neue TriggerwÃ¶rter und Intents
- Betrachtung datenschutzrechtlicher Implikationen (lokal vs. Cloud)
- Anbindung an die openHAB REST API
  - Triggersatz soll bspw.
    - Command an ein openHAB Item schicken und darÃ¼ber dann ein GerÃ¤t steuern
    - State von einem openHAB Item abfragen und per Text-to-Speech diesen Systemzustand ausgeben
- eigene REST API zur VerfÃ¼gung stellen
  - ein Endpunkt kÃ¶nnte zum Beispiel genutzt werden, dass man einen Text schickt, der dann per Text-to-Speech ausgegeben wird.
  - ...
- (Optional) Einbindung einfacher Aktionen (z.â€¯B. Timer/Wecker setzen, Terminerinnerungen speichern & Termine vorlesen, Wetter abfragen, "das Internet durchsuchen")-
- (Optional) Einbindung von GerÃ¤ten unabhÃ¤ngig von Smart Home Systemen
  - Zum Beispiel die Python Library `SoCo` (Sonos Controller)
    - Man kÃ¶nnte die `play_uri`-Funktion nutzen, dass die Text-to-Speech-Ausgabe nicht Ã¼ber einen Lautsprecher am entwickelten GerÃ¤t erfolgt, sondern Ã¼ber einen Sonos-Lautsprecher. Entsprechend muss dies konfigurierbar und eintsellbar sein.

---

## MÃ¶gliche Forschungsschwerpunkte

- Vergleich von Offline-STT-Engines: `Whisper`, `Vosk`, `DeepSpeech`
- Evaluation von NLU-Systemen: `Rasa`, `Snips`, Transformer-basierte LÃ¶sungen
- Training eigener Hotword-Modelle mit `Porcupine`, `Mycroft Precise`
- Einfluss der Hardware auf Performance (z.â€¯B. Raspberry Pi vs. Jetson Nano)
- Kombination regelbasierter und ML-basierter Dialogmodelle
- Lokaler Datenschutz vs. Cloud-Performance

---

## Eingesetzte Technologien (Auswahl)

- Programmiersprache: **Python**
- Hotword-Erkennung: `Porcupine`, `Mycroft Precise`
- Speech-to-Text: `Whisper`, `Vosk`, `DeepSpeech`
- NLU: `Rasa`, `Snips`, eigene ML-Modelle mit `transformers`
- TTS: `Coqui TTS`, `eSpeak`, `Festival`
- Plattform: **Raspberry Pi 5**, ggf. Mini-PC

(Unten sieht man, dass es hier verschiedene Alternativen zur Auswahl gibt. Auch mit Erfahrungen wÃ¤hrend der Bearbeitung darf hier natÃ¼rlich die Technologien getauscht werden.)

---

## Voraussetzungen

- Interesse an Sprachverarbeitung, KI und Embedded Systems
- Gute Kenntnisse in Python
- GrundverstÃ¤ndnis maschinellen Lernens und Linux-Systemen
- Motivation zur Arbeit mit Open-Source-Technologien

---

## Betreuung & Rahmen

- Diese Arbeit eignet sich besonders fÃ¼r Studierende der Informatik, Medieninformatik oder verwandter StudiengÃ¤nge.
- Umfang: Bachelorarbeit (ca. 16 Wochen Vollzeit)
- Bei Interesse ist eine Weiterentwicklung zur Masterarbeit denkbar.

---

## Beispielaufbau & Vorgehensweise

Ein eigener Sprachassistent ist ein spannendes Projekt â€“ technisch anspruchsvoll, aber mit vielen Freiheiten in der Gestaltung. Unten findest du eine systematische Ãœbersicht Ã¼ber alle benÃ¶tigten **Komponenten**: Hardware, Software, KI-Module und die empfohlene Toolchain.

---

## MÃ¶gliche Fragen

- Sind Ressourcen fÃ¼r einen lokalen Sprachassistenz leistungsstark genug? (Last)
- Wie lange dauert es eine Antwort zu generieren? (Latenz)
- Wie prÃ¤zise kÃ¶nnen Worte/SÃ¤tze erkannt werden? (Genauigkeit)
- Gibt es genÃ¼gend kostenlose und lokale Frameworks und Libraries fÃ¼r die einzelnen Module eines Sprachassistenten (angenommen STT lÃ¤uft lokal, bedeutet dies nicht, dass TTS auch lokal funktionieren kÃ¶nnte)?
- Wie hÃ¤ufig muss trainiert werden? Wird im laufenden Einsatz des Systems trainiert (z. B. `Reinforcement Learning`)?
- Was muss trainiert werden?
- Wie muss trainiert werden? (Zum Beispiel `online` vs. `offline`).
- Wie werden Trigger hinzugefÃ¼gt?
- Wie kann ein Trigger zu einer AusfÃ¼hrung gelangen? (Zum Beispiel einen regelbasierten Ansatz verwenden).
- BenÃ¶tige ich eine Hotword-Recognition?
  - Dies wÃ¼rde ich auch definitiv mit ja beantworten. Selbst wenn dieses System nicht datenschutzrechtlich bedenklich ist, sollte das System nicht dauerhaft zuhÃ¶ren. HÃ¶rt dieses System dauerhaft zu, muss eine oder mehrere KIs die ganze Zeit analysieren, was das System ziemlich sicher Ã¼berlasten wÃ¼rde.
- Wie lange darf eine Spracheingabe dauern?
  - Vielleicht machen 90-120 Sekunden als maximal Sinn, weil wenn ich zu viel rede, ist man einfach nicht prÃ¤zise genug. Eine KI wÃ¼rde nicht mehr verstehen, worauf man hinaus will. Wenn man einen regelbasierten Ansatz hat, kÃ¶nnten aber z. B. 120 Sekunden schon viel zu lange sein, weil man dann in 120 Sekunden ja Wort fÃ¼r Wort denselben Satz sagen muss, wie der, der durch einen Trigger ausgelÃ¶st werden soll. Hier wÃ¤re dann je lÃ¤nger ein Satz ist, es umso schwieriger, dass nach Abzug von Fehlern der richtige Satz noch erkannt wird.
- Braucht eine Spracheingabe eine minimale Dauer?
  - Vielleicht macht es Sinn, dass man mindestens 15-20 Sekunden etwas sagen sollte, ansonsten wird dies verworfen, weil zu wenige Informationen vorhanden wÃ¤ren, als dass man damit etwas anfangen kÃ¶nnte. 
- Brauche ich Abbruchkritierien?
  - Abbruchkriterien kÃ¶nnten Worte wie "Stop" oder "Halt" sein. Eventuell gekoppelt, dass man vorher erneut das Hotword sagen muss.
  - Ein Abbruchkriterium ist natÃ¼rlich, dass es keinen Triggersatz gibt.
  - Weitere Abbruchkriterien sind, dass wenn eine KI analysiert oder eine Websuche gemacht wird, die eine Antwort liefert, dass dies eine ZeitÃ¼berschreitung darstellt. Ich denke niemand wartet 5-6 Minuten auf eine Antwort. WÃ¤re hier also ebenfalls vielleicht 90-120 Sekunden sinnvoll fÃ¼r eine ZeitÃ¼berschreitung als Abbruchkriterium? Eine andere Zeitspanne?
- Sind Abbruchkritieren wie Worte auch eigene Hortwords oder TriggersÃ¤tze?
- Wie lange muss ich Satzpausen zwischen Hotword und Triggerwort einplanen?
- Welche Hotwords eignen sich? Zum Beispiel Smart Home.
- Brauche ich mehrere Hotwords? Wenn ja, wie viele?
  - Ich wÃ¼rde sagen, mehr als 3-4 wÃ¼rden wenig Sinn ergeben. Wenn man aber bspw. Smart Home und openHAB verwendet, kann man individueller seinen Sprachassistenten ansprechen. 
- Muss ich Hotwords selbst wÃ¤hlen und damit verbunden dann neu trainieren kÃ¶nnen?
  - Ja, weil man einen individuellen Sprachassistenten nicht zwangslÃ¤ufig im Smart Home verwenden will, dann wÃ¤re Smart Home mÃ¶glicherweise das falsche Hotword.
  - Ja, weil man vielleicht wenn man openHAB verwendet openHAB als Hotword verwenden will oder wenn man Home Assistant oder ioBroker, usw. verwendet, diese Eigennamen als Hotword verwenden will.
-  Welche Libraries eignen sich?
-  Welche Frameworks eignen sich?
-  Ist die SprachqualitÃ¤t von offline Libraries Ã¼berhaupt zumutbar?
-  Wie fÃ¼ge ich TriggersÃ¤tze hinzu?
-  Wie fÃ¼ge ich Regeln hinzu? (`WENN-DANN`-Regel sind denke ich fÃ¼r einen Teil der Anwendungsszenarieren definitiv notwendig).
-  Wie fÃ¼ge ich FunktionalitÃ¤ten hinzu, die durch Trigger ausgefÃ¼hrt werden sollen.
-  Welche FunktionalitÃ¤ten sollen denn ausgefÃ¼hrt werden kÃ¶nnen?
  - Zum Beispiel openHAB-Anbindung.
  - Zum Beispiel Wecker/Timer setzen.
  - Zum Beispiel Terminerinnerung setzen und an Termine erinnert werden.
- MÃ¼ssen Mehrsprachigkeit berÃ¼cksichtigt werden? (Nein, da es ein Prototyp ist. Man mÃ¼sste sonst viel zu viel trainieren).
- Brauche ich eine REST API? Ja.
- Brauche ich eine WeboberflÃ¤che? Ja.
- Brauche ich eine Datenbank? Ja.
- Wie teste ich das System bzw. welche Tests kann ich machen und planen?
- ...

---

### ğŸ”§ **1. Hardware**

#### âœ… *Mindestanforderungen (fÃ¼r lokale Verarbeitung)*:

* **Mini-PC oder Einplatinenrechner**:

  * *Raspberry Pi 5 (2GB/4GB/8GB/16GB)* â€“ Einsteigerfreundlich, fÃ¼r einfache Anwendungen ausreichend.
  * *Alternativen*: NVIDIA Jetson Nano (wenn du KI lokal trainieren willst), oder ein Mini-PC wie Intel NUC.
* **Mikrofon**:

  * *USB-Mikrofon* mit guter AufnahmequalitÃ¤t (z.â€¯B. Samson Go Mic, ReSpeaker 2-Mic HAT fÃ¼r Raspberry Pi).
  * *Array-Mikrofone* (fÃ¼r bessere Erkennung und GerÃ¤uschunterdrÃ¼ckung).
* **Lautsprecher**:

  * Per Klinke oder USB anschlieÃŸbar. Auch kleine Bluetooth-Speaker sind mÃ¶glich.

> ğŸ” Empfehlung: Beginne mit einem USB-Mikrofon + Pi 5 oder Mini-PC zum Testen, bevor du in Mikrofon-Arrays oder Jetson investierst.

---

### ğŸ§  **2. Software-Komponenten & Architektur**

Die Sprachassistenz umfasst mehrere Stufen:

| **Schritt**                   | **Funktion**                           | **Empfohlene Tools/Libs**                                        |
| ----------------------------- | -------------------------------------- | ---------------------------------------------------------------- |
| **1. Hotword Detection**      | Erkennung des Aktivierungswortes       | `Porcupine`, `Snowboy`, `Mycroft Precise`, `Picovoice`           |
| **2. Spracherkennung (ASR)**  | Umwandlung von Sprache in Text         | `Vosk`, `Whisper`, `DeepSpeech`, Google STT (Cloud)              |
| **3. NLU (Intent-Erkennung)** | Bedeutung/Text in Aktion umwandeln     | `Rasa`, `Snips NLU`, eigene Modelle mit `Transformers`           |
| **4. Antwortgenerierung**     | Aktion ausfÃ¼hren oder Antwort erzeugen | Eigene Logik / GPT-Anbindung / Regelbasiert                      |
| **5. Text-zu-Sprache (TTS)**  | Text in Sprache umwandeln              | `Coqui TTS`, `Festival`, `eSpeak`, Google TTS, `ResponsiveVoice` |

Neben der Sprachassistenz benÃ¶tigt man meiner Meinung nach `Flask` fÃ¼r die `REST API` und fÃ¼r eine `WeboberflÃ¤che` des Sprachassistenzen. Wahrscheinlich konfiguriert man Ã¼ber die WeboberflÃ¤che TriggersÃ¤tze. Entsprechend wird auch eine Datenbank (`SQLite` mit `SQLAlchemy`-Anbindung) fÃ¼r PasswÃ¶rter und Regeln benÃ¶tigt.

#### Beispielvergleich

Die Auswahl von verschiedenen Libraries kann ja Vor- und Nachteile hervorheben.

##### Hotword Detection

###### âœ… Vergleich: **Lokale Hotword-Erkennungssysteme**

| **Library / System** | **Sprache / API** | **Lokal** | **Cloudfrei**     | **Status**           | **Besonderheiten**                                      |
| -------------------- | ----------------- | --------- | ----------------- | -------------------- | ------------------------------------------------------- |
| ğŸ”¹ `Porcupine`       | Python 3 / C      | âœ…         | âœ…\*               | âœ… aktiv              | Sehr effizient, Wakeword-Modelle mit Lizenz generierbar |
| ğŸ”¹ `Snowboy`         | Python 2 / C++    | âœ…         | âš ï¸ (fÃ¼r Training) | âŒ eingestellt (2020) | Exzellente Erkennung, kein Custom Training mehr         |
| ğŸ”¹ `Mycroft Precise` | Python 3          | âœ…         | âœ…                 | âœ… aktiv (2024 Forks) | Open-Source, trainierbar mit eigenem Datensatz          |
| ğŸ”¸ `Picovoice`       | Python 3 / Web    | âš ï¸ teils  | âŒ                 | âš ï¸ deprecated        | Picovoice war Suite, nun nur noch Porcupine aktiv       |

---

###### ğŸ§  Klarstellungen:

* **Porcupine** (by Picovoice):

  * Sehr **effizient**, lÃ¤uft sogar auf Mikrocontrollern.
  * Man kann eigene Wakewords **lokal erzeugen**, aber dazu braucht man evtl. die **Picovoice Console** (Web).
  * Die Laufzeit selbst ist **vollstÃ¤ndig offline**.
  * Lizenzmodell: Kostenlos fÃ¼r Einzelpersonen / nicht-kommerzielle Zwecke.

* **Snowboy**:

  * FrÃ¼her sehr populÃ¤r fÃ¼r lokale Wakeword-Erkennung.
  * Leider **nicht mehr gepflegt**.
  * Eigene Wakewords waren nur Ã¼ber eine Web-OberflÃ¤che trainierbar, die inzwischen offline ist.

* **Mycroft Precise**:

  * **Open Source und lokal trainierbar** mit eigenem Datensatz.
  * Python 3-kompatibel, kann auf Linux / Raspberry Pi laufen.
  * Aktive Community / Forks, auch 2024 noch in Benutzung.
  * Funktioniert gut fÃ¼r DIY- oder Datenschutzprojekte.

* **Picovoice SDK**:

  * Die **gesamte SDK-Suite (Speech-to-Text etc.)** wurde eingeschrÃ¤nkt, empfohlen wird jetzt direkt **Porcupine**.
  * **Nicht mehr aktiv entwickelt** als Komplettpaket.

---

###### ğŸ“Œ Fazit / Empfehlung:

| Einsatzziel                              | Empfehlung                         |
| ---------------------------------------- | ---------------------------------- |
| Lokaler Assistent mit Custom Hotword     | ğŸ”¹ **Mycroft Precise**             |
| Minimaler Stromverbrauch (z.â€¯B. Pi Zero) | ğŸ”¹ **Porcupine**                   |
| Forschung / Training eigener Wakewords   | ğŸ”¹ **Mycroft Precise**             |
| Veraltete Tools vermeiden                | âŒ Kein Snowboy, kein Picovoice SDK |

---

Dies ist jetzt nur eine kleine Ãœbersicht, was man machen kann im Vergleich. Am besten Quellen ranziehen (z. B. Webseite, Doku, usw.). Man vergleicht am Besten `Python 3 / Python 2` (`Python 2` ist ein Ausschlusskriterium). In der auf NodeJS-basierenden Software `MagicMirror` wird fÃ¼r die Spracherkennung `Snowboy` eingesetzt. Dort ist als `Hotword` bereits `MagicMirror` vortrainiert. Ein neues Hotword kann nicht mehr trainiert werden, weil `Snowboy` deprecated ist und der Service fÃ¼r das Training meines Wissens sogar eingestellt wurde. SelbstverstÃ¤ndlich schaut man sich an, ob etwas cloudbasiert funktioniert, lokal oder teilweise cloudbasiert ist. Ebenfalls wichtig ist ja, ob etwas kostenlos ist oder nicht. Hin und wieder gibt es ja auch Abomodelle, dass man vielleicht 1000 Requests kostenlos hat oder CentbetrÃ¤ge fÃ¼r einzelne Requests zahlt, usw. Manche Libraries nutzen `C`, `C++` im Background und es wird nur ein `Python-Wrapper` verwendet. Dies bedeutet, dass das Training performanter und hardwarenÃ¤her ist.

##### ASR/TTS

Es gibt extrem viele Libraries fÃ¼r `Text-to-Speech`. Die meisten benÃ¶tigen eine Cloud. Also wirklich fast alle. Dies ist ein klarer Nachteil. Manche sind immer noch auf Basis von `Python 2`. Einige unterstÃ¼tzen sowohl `Python 2`, als auch `Python 3`. Manche gehen natÃ¼rlich nur mit `Python 3`. Ergo kann man auch hier alles was `Python 3` nutzt als Vorteil ansehen. Das Ergebnis, bzw. die SprachqualitÃ¤t von cloudbasierten Libraries ist ganz klar besser. Die lokal laufenden Systeme klingen im Deutschen so, als wÃ¼rde ein Amerikaner Deutsch reden und dann klingt es sehr technisch, mechanisch nach einem Roboter, wÃ¤hrend cloudbasierte Ergebnisse meist einem menschlichen Sprecher sehr nahe kommt. Je nachdem dauert die Umwandlung lange. Bei cloudbasierten LÃ¶sungen gibt es welche, die sehr schnell gute Ergebnisse zurÃ¼ckliefern. Cloudbasiert und damit viel Rechenleistung bedeutet aber tatsÃ¤chlich nicht zwangslÃ¤ufig, dass es schnell geht, bis die Datei generiert wird. Auch hier gibt es mÃ¶glicherweise Latzenzen. Lokal laufende LÃ¶sungen haben zwar keine Latzen zwecks Netzwerkauslastung oder schlechter Internetverbindung, sind aber oft auch langsam, weil die RechenkapazitÃ¤t geringer ist, als in einer Cloud. Man kann vergleichen, welche Datenformate generiert werden (`wav`, `mp3`, usw.). Ich denke fÃ¼r die meisten Szenarien ist es egal, welches Audioformat generiert wird. Wenn man aber bspw. Sonos-Lautsprecher verwenden mÃ¶chte, dann kann es sein, dass nicht jedes Audioformat unterstÃ¼tz wird. Vor- und Nachteile in der Bewertung kann es sein, ob Sprecherstimmen (verschiedene Frauen- oder MÃ¤nnerstimmen) konfigurierbar sind, ob Sprachen konfigurierbar sind oder ob `Speech Synthesis Markup Language` (`SSML`) unterstÃ¼tzt wird. `SSML` ist ein ganz klarer Vorteil und gerade im Vergleich zu `Amazon Alexa`, wÃ¤re es vielleicht sogar sehr sinnvoll, da `Alexa` dies unterstÃ¼tzt. 

Es kÃ¶nnen auch weitere Kritieren noch fÃ¼r einen Vergleich herangezogen werden.

Anmerkung: Es gibt verschiedene Libraries, die im Hintergrund aber bspw. denselben Service wie `Google STT` verwenden. Das heiÃŸt, viele Libraries stellen zu manch einer Library kaum eine echte Alternative dar.

##### NLP

SelbstverstÃ¤ndlich kommt hier auch wieder `Python 2` vs. `Python 3` vor, wenn man die Libraries vergleicht. Da es hier um KI geht, ist auch entscheidend, ob `CPU` oder `GPU` verwendet werden kann. Gemessen an der schwachen Leistung des EndgerÃ¤ts (z. B. Raspberry Pi), wÃ¼rde es hier auch Sinn ergeben, ob man eine `TPU`-UnterstÃ¼tzung oder andere Coprozessoren verwenden kÃ¶nnte. Es gibt auch KIs, die Ã¼ber eine Cloud trainiert werden. Bei manchen KIs, in einer Cloud lÃ¤dt man Daten hoch, trainiert sie, lÃ¤dt Daten herunter und kann die trainierten Daten zu seiner Anwendung hinzufÃ¼gen. Dies wÃ¤re dann zumindest bedingt offline und lokalfÃ¤hig, weil man nur beim Training eine Internetverbindung zwingend benÃ¶tigt hat. Es gibt KIs, die sind leichtgewichtig und besonders gut fÃ¼r `Raspberry Pi's` geeignet.

##### Antwortgenerierung

Ich denke dies kann man mehrschichtig aufbauen. Als erstes nutzt man z. B. die regelbasierten Antworten, wenn dort keine Regel auftaucht kÃ¶nnen Websuche oder KIs angebunden sein, usw. Hier kann man auch Vor- und Nachteile diskutieren. Eine eigene KI zu trainieren edeutet, dass man sehr vielseitig trainieren mÃ¼sste und immens viele Daten brÃ¤uchte. Ein `GPT` (`Generative Pre-trained Transformer`) bedeutet ja in der Regel immer eine Cloud-LÃ¶sung (es gibt auch Docker-Container, die man lokal laufen lassen und trainieren kann).

##### Speech-to-Text

Nicht selten sind es auch dieselben Cloud-Dienstleister, wie bei `Text-to-Speech`. Ich behaupte, es sind auch sehr vegleichbare und Ã¤hnliche Kriterien, sowie dieselben Vor- und Nachteile.

---

### ğŸ§  **3. KI-Modelle: WofÃ¼r und wie trainieren?**

#### Du brauchst KI fÃ¼r:

* **Hotword Detection** (Custom Wakewords)
* **Intent-Erkennung** (z.â€¯B. â€œWie wird das Wetter morgen?â€)
* (Optional) **Dialog-Management** (Kontext verstehen)
  * Man kann dies Regelbasiert machen. Empfehlenswert wÃ¤re eine OberflÃ¤che, bei der ich TriggersÃ¤tze hinzufÃ¼ge. Sogenannte `WENN-DANN`-Regel sind im Smart Home Gang und GÃ¤be. Man wÃ¤hlt bspw. `WENN` aus und schreibt einen Triggersatz, klickt dann auf `DANN` und wÃ¤hlt bspw. `openHAB Command` und sendet einen `Command` an ein openHAB Item oder man wÃ¤hlt bspw. `openHAB State` und macht eine Statusabfrage von einem openHAB Item.
  * GrundsÃ¤tzlich kann man es so aufbauen, dass sobald ein Kontext nicht verstanden wird, eine KI versucht zu verstehen, was gemeint ist oder eine Websuche machen, sobald etwas erkannt wird, was in keiner Regel hinterlegt ist.
  * Man kann auch im Regelsystem direkt integrieren, dass z. B. Termineerinnerungen gesetzt werden sollen, das Web durchsucht werden soll, usw. Man kann ja auch absichtlich TriggersÃ¤tze bauen, die nicht zwangslÃ¤ufig mit Smart Home zu tun haben mÃ¼ssen.

##### Modelle:

* Hotword-Erkennung â†’ kleine CNNs oder vortrainierte Modelle (Snowboy, Porcupine).
* Intent-Erkennung â†’ NLP-Modelle wie BERT oder einfache Klassifikatoren (z.â€¯B. `scikit-learn`, `spaCy`, `Rasa`).
* FÃ¼r Antwortgenerierung kannst du GPT lokal oder per API (OpenAI, HuggingFace) einbinden.

##### Training:

* Hotword-Erkennung: ca. 20â€“50 Audiobeispiele pro Wakeword.
* Intent-Erkennung: Beispiel-Intents in YAML/JSON, evtl. Feintuning mit `Rasa` oder `Transformers`.

> ğŸ’¡ FÃ¼r viele AnwendungsfÃ¤lle ist kein eigenes Training notwendig, da es gute **pretrained Modelle** gibt.

---

### ğŸ’» **4. Entwicklungsumgebung**

#### ğŸ”¤ **Programmiersprache:**

* **Python**: Klare Empfehlung, da fast alle relevanten Frameworks hier existieren.
* **Alternative** (falls Performance kritisch ist): Rust oder C++ (z.â€¯B. fÃ¼r Low-Level Audio-Handling).

#### ğŸ§° **Empfohlene Frameworks**:

* `speech_recognition` (Wrapper fÃ¼r verschiedene STT-Systeme)
* `Rasa` (NLU/Dialogmanagement)
* `transformers` von HuggingFace (fÃ¼r moderne NLP)
* `PyTorch` oder `TensorFlow` (fÃ¼r eigene Modelle)
* `Flask` oder `FastAPI`
  * Ich empfehle `Flask`, da man hier nicht nur eine `REST API` bauen kann, sondern auch eine WeboberflÃ¤che. Ãœber die WeboberflÃ¤che kann man ja bspw. TriggersÃ¤tze konfigurieren. 
* `pyaudio` oder `sounddevice` (fÃ¼r Mikrofonzugriff)
* `python-openhab-rest-client` (fÃ¼r openHAB)
* `SQLAlchemy` (fÃ¼r Datenbanken)

Es wird noch eine genauere Planung benÃ¶tigt, wie ein solches System funktonieren soll.

---

### ğŸ§± **5. ArchitekturÃ¼bersicht (Ablauf)**

```plaintext
[Mikrofon] â†’ [Hotword Detection] â†’ [Spracherkennung (ASR)] â†’ [Intent-Erkennung (NLU)]
            â†’ [Antwort (Aktion/Dialog)] â†’ [Text-To-Speech] â†’ [Lautsprecher]
```

Was hier in der Architektur nicht drinnen ist, ist was mit den Intents passiert. Mit diesen kann ich ja zum Beispiel openHAB steuern oder ZustÃ¤nde aus openHAB abfragen. Ich kÃ¶nnte ja auch andere Systeme anbinden. AuÃŸerdem muss ja gar keine Antwort verbal erfolgen. Bei einer `WENN-DANN`-Regel hÃ¤tte ich hier, dass auf eine spezielle Spracheingabe eine spezielle Sprachausgabe erfolgen wÃ¼rde. Ich kann ja auch etwas anderes machen und erhalte dann eine Sprachausgabe oder ich habe eine Spracheingabe ohne Sprachausgabe und mache damit irgendetwas vÃ¶llig anderes (z. B. GerÃ¤te bedienen).

Es fehlt zum Beispiel, ob es eine BenutzeroberflÃ¤che gibt, mit der man `WENN-DANN`-Regel konfiguriert. Es fehlt eine Architekturbetrachtung, ob Datenbanken oder andere Systeme angebunden sind. Es fehlt eine Architekturbetrachtung, wie ein KI-Model angebunden ist. Wie wird was Ã¼berhaupt trainiert?

Dies bedarf noch einer genaueren Planung.

#### Beispiel-Stack (lokal lauffÃ¤hig):

* **Hotword**: Porcupine (Wakeword: â€Hey Kiroâ€œ)
* **ASR**: Vosk (lokal) oder Whisper
* **NLU**: Rasa (mit YAML-Intents)
* **TTS**: Coqui TTS
* **Logik**: Python-Controller, der Aktionen oder Antworten triggert

---

### ğŸ§ª **6. ZusÃ¤tzliche Tipps**

* **Modular entwickeln**: Trenne jeden Schritt in eigene Komponenten (Hotword, STT, NLU, TTS).
* **Offline-fÃ¤higkeit prÃ¼fen**: Whisper, Vosk, Rasa, Coqui kÃ¶nnen lokal laufen.
* **Sicherheit**: Wenn du eine Cloud-LÃ¶sung einsetzt (z.â€¯B. OpenAI API), achte auf Datenschutz.
* **Trigger/Custom Intents**: Rasa erlaubt einfache Erweiterung mit neuen â€Storiesâ€œ oder â€Intentsâ€œ ohne tiefes KI-Training.

Anmerkung: Cloud- oder Web-Anbindungen kÃ¶nnen ja das System jederzeit erweitern. In Abgrenzung zu gÃ¤ngigen Sprachassistenzen lÃ¤uft dieser ja trotzdem lokal. Sollte es Internetprobleme geben, dann verzichtet man nur auf einen Teil der FunktionalitÃ¤ten, aber die Basisfunktionen gehen noch, wÃ¤hrend bei einem typischen Sprachassistent ohne Internetverbindung nichts mehr funktionieren wÃ¼rde.

---

### ğŸš€ Einstiegsvorschlag (fÃ¼r Prototypen)

1. **Raspberry Pi 5 + USB-Mikrofon**
2. Hotword: `Porcupine` (kostenloser Wakeword-Engine)
3. STT: `Whisper` oder `Vosk`
4. NLU: `Rasa` (mit einfachen Intents)
5. TTS: `Coqui TTS` oder Google TTS API
6. Alles orchestriert mit `Python`

---

## Hintergrundwissen

### Unterschied Chatbot vs. Sprachassistent

Prinzipiell gilt:

```
Chateingabe â‰™ Spracheingabe (Speech-to-Text)
Chatausgabe â‰™ Sprachausgabe (Text-to-Speech)
```

Entspricht der Aufbau von einem Chatbot wirklich dem Aufbau eines Sprachassistenten? Nicht wirklich. Sie funktionieren sehr Ã¤hnlich. Ein GroÃŸteil der Architektur kann gleich aufgebaut werden. Man kann auch `Fuzzy Matching` verwenden. Es wird `NLP` in beiden Systemen angewandt. Auch ein Sprachassistent benÃ¶tigt dringend ein `Intent`. Am Beispiel `Fuzzy Matching` wird deutlich, dass dies in einem Chatbot sehr viel notwendiger sein wird, als in einem Sprachassistenten. Tippfehler kann man bspw. mit `Fuzzy Matching` korrigieren, nicht gehÃ¶rte oder verrauschte Worte, kÃ¶nnen damit jedoch nicht korrigiert werden. Bei einem Chatbot kann es mal entstehen, dass einzelne Buchstaben in einem Wort fehlen oder verdreht sind, bei einer Spracherkennung hingegen wird ein Wort entweder erkannt oder nicht erkannt.

### Fuzzy Matching

Das **Fuzzy Matching** kann man im deutschen als unscharfe Suche betiteln. Es ist eine Klasse von String-Matching-Algorithmen, mit der man bestimmte Zeichenketten (Strings) in einer lÃ¤ngeren Zeichenkette oder einem Text suchen bzw. finden kÃ¶nnen sollte.  Es wird daher auch oft als Fuzzy-Suche oder Fuzzy-String-Suche betitelt.

Aus:

[https://www.klippa.com/de/blog/informativ/fuzzy-matching-de/](https://www.klippa.com/de/blog/informativ/fuzzy-matching-de/)

Mit Fuzzy-Matching kann ich darauf reagieren, wenn z. B. nur 80% eines Wortes/Satzes richtig erkannt wurde. Hier mal Beispiele.

```
Gesucht: Bestellnummer

Richtig wÃ¤ren:
Bestellnr.
Bestelnummer
Bestellnumer
Bestellnummmer
...
```

Also mÃ¶gliche Tippfehler oder auch AbkÃ¼rzungen sollen ja darufhin deuten, dass ein- und dasselbe Worte gemeint ist. In Zusammenhang mit Synonymen wÃ¼rde man durch Fuzzy Matching dem Benutzer so extrem viele EingabemÃ¶glichkeiten ermÃ¶glichen. Man formuliert ja nicht nur SÃ¤tze um, man vertippt sich auch mal oder kÃ¼rzt einzelne WÃ¶rter ab.

**Fuzzy Matching** nimmt also Korrekturen vor. Dies kÃ¶nnten im allgemeinen die nachfolgenden sein:

* **EinfÃ¼gen** â€“ HinzufÃ¼gen eines Buchstabens zur VervollstÃ¤ndigung des Wortes (z. B. `Rechnun` wird zu `Rechnung`)
* **LÃ¶schen** â€“ Entfernen eines Buchstabens aus einem Wort (z. B. `Rechnnung` wird zu `Rechnung`)
* **Substitution** â€“ Vertauschen eines Buchstabens, um ein Wort zu korrigieren (z. B. `Technung` wird zu `Rechnung`)
* **Transposition** â€“ Vertauschen von Buchstaben, um ein Wort zu korrigieren (z. B. `Rehcnung` wird zu `Rechnung`)

Jeder Korrektur, die durchgefÃ¼hrt werden muss, wird eine â€Bearbeitungsdistanzâ€œ von 1 zugeschrieben. Die Bearbeitungsdistanzen beeinflussen die oben erwÃ¤hnte Trefferquote. Wenn Sie beispielsweise eine Zeichenfolge mit 11 Zeichen haben und 2 Korrekturen vornehmen mÃ¼ssen, betrÃ¤gt die endgÃ¼ltige Trefferquote **81,81 %**.

```
Berechnung: 100%- 2 / 11= 81.81%  
```

Neben diesen Korrekturen kann **Fuzzy Matching** auch verwendet werden, um Zeichensetzungen, zusÃ¤tzliche WÃ¶rter und fehlende Leerzeichen in Zeichenketten oder Texten zu korrigieren.

---

#### Fuzzy-Matching-Algorithmen

Fuzzy Matching fÃ¤llt in die Kategorie der Methoden, fÃ¼r die es keinen spezifischen Algorithmus gibt, der alle Szenarien und AnwendungsfÃ¤lle abdeckt. Daher werden wir einige der am hÃ¤ufigsten verwendeten und zuverlÃ¤ssigsten Fuzzy-Matching-Algorithmen fÃ¼r die Suche nach ungefÃ¤hren DatenÃ¼bereinstimmungen behandeln:

* Levenshtein-Distanz (LD)
* Hamming-Distanz (HD)
* Damerau-Levenshtein

---

##### Levenshtein-Distanz

Die **Levenshtein-Distanz (LD)** ist eine Fuzzy-Matching-Technik, die zwei Zeichenfolgen beim Vergleich und der Suche nach einer Ãœbereinstimmung berÃ¼cksichtigt. Je hÃ¶her der Wert der Levenshtein-Distanz ist, desto weiter sind die beiden Zeichenfolgen oder â€Begriffeâ€œ von einer identischen Ãœbereinstimmung entfernt.

Wie erhalten wir nun den Wert der Levenshtein-Distanz? Die LD zwischen den beiden Zeichenfolgen entspricht der Anzahl der Ã„nderungen, die erforderlich sind, um eine Zeichenfolge in die andere umzuwandeln. FÃ¼r die LD gelten das EinfÃ¼gen, LÃ¶schen und Ersetzen eines einzelnen Zeichens als Bearbeitungsoperationen.

Nehmen wir an, Sie mÃ¶chten die LD zwischen â€Rechnungsnummerâ€œ und â€Rechnungs-Nr.â€œ messen. Der Abstand zwischen den beiden Begriffen ist â€1 x uâ€œ, â€2 x mâ€œ und â€1 x eâ€œ, was einem Abstand von 4 entsprechen wÃ¼rde. Warum? Weil Sie diese Zeichen hinzufÃ¼gen mÃ¼ssten, um eine Ãœbereinstimmung zu erreichen. Siehe die Beispiele unten.

---

###### Levenshtein-Abstand Beispiel

> **Rechnungnummer** â†’ Rechnung**s**nummer (EinfÃ¼gung von â€**s**â€œ) â€“ Abstand: 1  
> **Rechnung numr** â†’ Rechnungsnu**m**m**e**r (EinfÃ¼gung von â€**m**â€œ & â€**e**â€œ) â€“ Abstand: 2  
> **Rechnung nr** â†’ Rechnungsn**u****m****m****e**r (EinfÃ¼gung von â€**u, m, m, e**â€œ) â€“ Abstand: 4

---

##### Hamming-Distanz

Die **Hamming-Distanz (HD)** unterscheidet sich nicht allzu sehr von der Levenshtein-Distanz. Die Hamming-Distanz wird hÃ¤ufig verwendet, um den Abstand zwischen zwei gleich langen Textabschnitten zu berechnen.

Die HD-Methode basiert auf der **ASCII**-Tabelle (American Standard Code for Information Interchange). Zur Berechnung des Abstandswertes verwendet der Hamming-Distanz-Algorithmus die Tabelle, um den BinÃ¤rcode zu bestimmen, der jedem Buchstaben in den Zeichenketten zugeordnet ist.

---

###### Hamming-Abstand-Beispiel

Nehmen wir die folgenden Textzeichenfolgen â€Numberâ€œ und â€Lumberâ€œ als Beispiel. Wenn wir versuchen, den HD zwischen den Zeichenfolgen zu bestimmen, ist der Abstand nicht 1, wie es mit dem Levenshtein-Algorithmus der Fall wÃ¤re. Stattdessen wÃ¼rde er 10 betragen. Das liegt daran, dass die ASCII-Tabelle einen BinÃ¤rcode von **(1001110)** fÃ¼r den Buchstaben **N** und **(1001100)** fÃ¼r den Buchstaben **L** anzeigt.

Beispielrechnung:

> **D** = N â€“ L = 1001110 â€“ 1001100 = **10**

---

##### Damerau-Levenshtein

Das Damerau-Levenshtein-Verfahren misst auch den Abstand zwischen zwei WÃ¶rtern, indem es die erforderlichen Ã„nderungen misst, die vorgenommen werden mÃ¼ssen, um ein Wort an das andere anzupassen. Diese Ã„nderungen hÃ¤ngen von der Anzahl der Operationen ab, wie z. B. EinfÃ¼gung, LÃ¶schung oder Ersetzung eines einzelnen Zeichens oder Transposition zweier benachbarter Zeichen.

Hier unterscheidet sich die Damerau-Levenshtein-Distanz von der regulÃ¤ren Levenshtein-Distanz, da sie zusÃ¤tzlich zu den Einzelzeichen-Editieroperationen, auch Transpositionen berÃ¼cksichtigt, um eine ungefÃ¤hre Ãœbereinstimmung zu finden (Fuzzy Match).

---

###### Damerau-Levenshtein Beispiel

> **Zeichenfolge 1:** Re<strong>ch</strong>nun<strong>g</strong>  
> **Zeichenfolge 2:** Re<strong>hc</strong>nun  
>   
> **Operation 1:** Transposition â†’ Vertauschen der Zeichen â€**h**â€œ und â€**c**â€œ  
> **Operation 2:** EinfÃ¼gen eines â€**g**â€œ am Ende der Zeichenfolge 2


Da zwei Operationen erforderlich waren, um die beiden WÃ¶rter identisch zu gestalten, **betrÃ¤gt der Abstand 2**. Vereinfacht ausgedrÃ¼ckt zÃ¤hlt jede Operation wie EinfÃ¼gung, LÃ¶schung, Transposition usw. als ein Abstand von â€1â€œ. Mit der Levenshtein-Distanz mÃ¼ssten Sie jedoch drei Korrekturen vornehmen, was einem Abstand von 3 entspricht.

Alle oben genannten Fuzzy-Matching-Algorithmen unterscheiden sich natÃ¼rlich in der Art und Weise, wie die Bearbeitungsdistanz berechnet wird. Dies ist der Grund, warum es keinen FM-Algorithmus gibt, der fÃ¼r alle geeignet ist. Von den drei vorgestellten Algorithmen ist die Levenshtein-Distanz jedoch der am hÃ¤ufigsten verwendete FM-Algorithmus in der Datenverwaltung und Datenwissenschaft.

Empfehlung: In Python kann man die [fuzzywuzzy-Bibliothek](https://github.com/seatgeek/fuzzywuzzy) testen oder einen eigenen Algorithmus implementieren.

---

#### ğŸ§  **Wird Fuzzy Matching fÃ¼r einen Sprachassistenten benÃ¶tigt?**

##### âœ… **Kurzantwort:**

**Ja, in bestimmten Komponenten eines Sprachassistenten kann Fuzzy Matching nÃ¼tzlich sein â€“ aber es ist nicht zwingend erforderlich** und wird **nicht immer eingesetzt**, insbesondere dann nicht, wenn du moderne NLU- oder KI-Modelle verwendest.

---

#### ğŸ“Œ **Wann ist Fuzzy Matching sinnvoll?**

Fuzzy Matching kommt dann zum Einsatz, wenn:

* **Benutzereingaben ungenau, variabel oder fehlerhaft** sind (z.â€¯B. Tippfehler, unterschiedliche Formulierungen).
* **Keine trainierten oder semantischen Modelle verfÃ¼gbar** sind, etwa bei regelbasierten Assistenten.
* Du **einfach strukturierte Sprachbefehle mit statischen Antworten** hast.

##### Typische Einsatzszenarien:

| Anwendung                                                   | Fuzzy Matching notwendig? | Alternative bei moderner Architektur          |
| ----------------------------------------------------------- | ------------------------- | --------------------------------------------- |
| MenÃ¼systeme / Chatbots mit festen Befehlen                  | âœ… Ja                      | Regelbasierte Intenterkennung                 |
| Sprachassistent mit NLU-Modell (z.â€¯B. Rasa)                 | âŒ Eher nein               | Verwendung von ML-basierten Parsers           |
| Wakeword-Erkennung                                          | âŒ Nein                    | Hier nutzt man z.â€¯B. CNNs, kein Text-Matching |
| Dateinamen oder Orte erkennen (â€Spiel *Bohemian Rhapsody*â€œ) | âœ… Ja (optional)           | Levenshtein, Jaro-Winkler, etc.               |

---

#### ğŸ”„ **Beispiel: Fuzzy Matching vs. NLU**

##### Ohne NLU:

```python
if "spiel musik" in user_input:
    do_music()
elif "spiel musick" in user_input:  # Tippfehler!
    do_music()  # geht nicht ohne Fuzzy Matching
```

##### Mit Fuzzy Matching:

```python
from fuzzywuzzy import fuzz
if fuzz.partial_ratio("spiel musik", user_input) > 80:
    do_music()
```

##### Mit NLU (z.â€¯B. Rasa):

```yaml
- intent: play_music
  examples: |
    - spiel musik
    - spiel bitte musik
    - spiele ein lied
    - mach musik an
```

â†’ NLU erkennt die Absicht, auch wenn der Text variiert oder leicht fehlerhaft ist. Kein Fuzzy Matching nÃ¶tig.

---

#### ğŸ§© **Fazit**

| Wenn du ...                            | Dann ...                                      |
| -------------------------------------- | --------------------------------------------- |
| ... nur einfache Textvergleiche machst | Fuzzy Matching kann helfen                    |
| ... moderne NLU-Modelle nutzt          | Fuzzy Matching ist **nicht nÃ¶tig**            |
| ... Intents regelbasiert zuweist       | Fuzzy Matching kann Fehler abfedern           |
| ... mit Sprache (nicht Text) arbeitest | ASR-Fehler kÃ¶nnen durch NLU abgefangen werden |

---

#### ğŸ’¡ Empfehlung fÃ¼r deine Bachelorarbeit:

* **Nutze Fuzzy Matching nur bei Bedarf**, z.â€¯B. fÃ¼r regelbasierte Systeme oder spezifische Trigger (z.â€¯B. Namen, Ortslisten).
* Bei **modernem Aufbau mit Rasa, transformers oder GPT-basierten Komponenten**: Verzichte darauf und verlasse dich auf semantische Modelle.

Ich denke in einer gut ausgearbeiteten Abschlussarbeit erlÃ¤utert man diesen Verzicht auch, weil es sehr viel darÃ¼ber sagt, was in der Theorie ein System alles kÃ¶nnen muss.

### Intent, Entity, Confidence Score

Aus:

[https://www.melibo.de/blog/was-sind-intent-und-entity](https://www.melibo.de/blog/was-sind-intent-und-entity)

---

#### Intent

Intents, zu Deutsch â€Absichtenâ€œ, sind Zwecke oder Ziele, die in den Eingaben eines Kunden zum Ausdruck kommen, um z.B. eine Frage zu einer Retoure zu stellen. Durch die Erkennung der Absicht, die sich in der Kundeneingabe ausdrÃ¼ckt, versucht der KI-Chatbot den richtigen Dialog zu finden und die passende Ausgabe zu wÃ¤hlen. DafÃ¼r nutzen KI-Chatbots maschinelles Lernen, um in natÃ¼rlicher Sprache die vorher definierte Absicht (Intent) zu erkennen. [1] Einfach gesagt, Intents sind Fragen der User:innen, die dem Chatbot zu einem speziellen Thema gestellt werden und der Versuch des KI-Chatbots, die passende Antwort zu erkennen, um das Problem zu lÃ¶sen bzw. die Frage zu beantworten.

---

##### Wie funktionieren Intents?

Bestehende Anbieter wie unter anderem der IBM Watson Assistant [1], Rasa [2] oder Microsoft LUIS [3] basieren alle meist auf dem Prinzip der Intent-Ausgabe. Bevor der Chatbot Intents erkennen kann, mÃ¼ssen erst mal alle Absichten der User:innen definiert werden. HierfÃ¼r ist es wichtig, dass man seine Kundenanfragen erst mal identifiziert und seinen Use-Case richtig versteht. Nachdem der Intent-Katalog erstellt und der Bot online genommen wurde, werden die User:innen dem Chatbot Fragen stellen. Jede Anfrage der User:innen durchlÃ¤uft das sogenannte Intent-Matching, also der Zuordnung der Anfrage aus den gesamten Inhalten des Chatbots. Dabei wird anhand von NLP (Natural Language Processing) ein Confidence-Score berechnet, um anhand von Wahrscheinlichkeiten die passende Antwort auszugeben.

---

#### Confidence-Score

Ein kurzer Exkurs zum Thema Confidence-Scores. Die Confidence-Scores liegen zwischen 0 und 1 und geben an, zu wie viel Prozent der Chatbot ein Intent erkannt hat. Zu jeder gestellten Frage der User:innen berechnet der Chatbot also einen Confidence-Score und versucht auf dieser Grundlage, durch Wahrscheinlichkeiten, die richtige Antwort an die User:innen auszugeben. Die Confidence-Scores sind meistens voreingestellt und liegen zwischen 0,6 und 0,7. Das heiÃŸt, dass der Chatbot Antworten nur dann ausgibt, wenn die Erkennungswahrscheinlichkeit bei mindestens 60 % liegt. Nehmen wir nun als Beispiel an, dass User:innen die Frage stellen â€Was kannst du so?â€œ, um zu erfahren, welche Themen der Chatbot Ã¼berhaupt beantworten kann. Der KI-Chatbot erkennt zu 89 % Prozent den Intent â€Was kannst du?â€œ. In diesem Beispiel gibt der Chatbot die passende Antwort aus und beantwortet somit die Frage.

---

##### Bestandteile eines Intents

Ein Intent besteht also aus dem: Intentnamen, dem User-Input, dem Confidence-Score und einer Antwort. AbhÃ¤ngig von der genutzten Technologie kÃ¶nnen noch weitere Bestandteile dazu kommen, wie Variablen, Actions und Entities.

Ein Beispiel fÃ¼r Intents

(1) User-Input: Frage eines Users

ğŸ™â€â™‚ï¸: â€Vorteile eines Chatbotsâ€œ
ğŸ™â€â™‚ï¸: â€Was kÃ¶nnen Chatbots besonders gut?â€œ
ğŸ™â€â™‚ï¸: â€Warum sollte ich einen Chatbot holen?â€œ

(2) Confidence-Score und (3) Intentnamen â€Vorteile Chatbotâ€œ: Berechnung der Wahrscheinlichkeit anhand vom User-Input

ğŸ§® : 100 % Erkennung des Intents â€Vorteile Chatbotâ€œ

(4) Antwort des Chatbots auf die Frage â€Vorteile Chatbotsâ€œ

ğŸ¤– : Zu den Vorteilen von Chatbots gehÃ¶ren unter anderem und abhÃ¤ngig von der Branche: Automatisierung von Prozessen, wodurch Fehler beim Support reduziert sowie Zeit und Geld eingespart werden kÃ¶nnen. VerkÃ¼rzte Wartezeiten fÃ¼r den Kunden. 24/7 Kundensupport. Effizientere Strukturen. Weniger manueller Aufwand fÃ¼r dich.

---

#### Entity

Im Unterschied zu Intents dienen Entitys oder auch Entities dazu, Informationen der User:innen aus der natÃ¼rlichen Sprache zu extrahieren. Jedes Entity verfÃ¼gt Ã¼ber eine Reihe von Eigenschaften, die mit ihr verbunden sind. Dabei kannst du auf Informationen deines Entities zugreifen. Wie bei einem Intent gibt der Chatbot an, wie hoch der Confidence-Score liegt. Im Unterschied zu Intents liegt der Confidence-Score, aber bei 0 oder 1. UnabhÃ¤ngig davon, ob und wie die Erkennung des Entitys eingestellt ist, haben sogenannte System-Entites immer eine Erkennung von 1. Jedes Entity besitzt einen Wert, einen sogenannten EntitÃ¤tswert. Bei der Erstellung von Entities ist es notwendig, dass neben dem Wert auch Typen definiert werden. Unter Typen versteht man allgemein Synonyme, also WÃ¶rter, die sich ebenfalls auf dasselbe Vorhaben beziehen, es nur anders umschreiben. Je mehr Synonyme ein Entity hat, desto besser die Erkennung des Chatbots [4].

GrundsÃ¤tzlich unterscheiden wir zwischen System-Entities und Customize-Entities. Die System-Entites sind voreingestellt, das heiÃŸt im System bereits enthalten. Darunter fallen etwa Zahlen, Uhrzeiten oder Adressen. Diese Entities sind besonders beliebt und wurden in der Vergangenheit besonders hÃ¤ufig verwendet. Die Customize-Entities dagegen sind selbst definierte Werte, die auf den jeweiligen Use-Case angepasst werden.

---

##### Ein Beispiel fÃ¼r Entities in der Praxis

ğŸ™â€â™‚ï¸: â€Wann kommt mein Produkt Chatbot-Experte in der LandstraÃŸe 5 an?â€œ

---

##### Entities in diesem Beispiel:

Produkt Chatbot Experte (Customize-Entity product_type)
LandstraÃŸe 5 (Sytem-Entity street_adress)

---

##### Vorteile von Entites:

Mit Entities kann man User:innen durch Chat Flows in Form von Buttons navigieren, schnell Synonyme definieren und mehrere Anfragen auf einmal verarbeiten. Die System-Entitys helfen auÃŸerdem dabei, die beliebten Anfragen zum Thema Ort, Zeit und Adresse abzufangen.

[1]: https://cloud.ibm.com/docs/assistant?topic=assistant-intents
[2]: https://rasa.com/open-source/
[3]: https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-utterance
[4]: https://cloud.ibm.com/docs/assistant?topic=assistant-expression-language

---

#### Speech Synthesis Markup Language (SSML)

**SSML (Speech Synthesis Markup Language)** ist eine XML-basierte Auszeichnungssprache, mit der du **Text-to-Speech (TTS)**-Ausgaben prÃ¤zise steuern kannst â€“ also wie ein Sprachsynthesizer den gesprochenen Text betonen, pausieren oder variieren soll.

---

##### ğŸ§  Was kann man mit SSML machen?

Mit SSML kannst du steuern:

* ğŸ—£ **Aussprache** (`<phoneme>`)
* â¸ **Pausen** (`<break>`)
* ğŸµ **TonhÃ¶he, LautstÃ¤rke, Sprechgeschwindigkeit** (`<prosody>`)
* ğŸ“¢ **Sprachstil / Emphasis** (`<emphasis>`, `<voice>`)
* ğŸŒ **Sprache und Stimme** (`<lang>`, `<voice>`)
* ğŸ” **Audioeinspielung** (`<audio>` â€“ je nach Engine)

---

##### âœ… Einfaches Beispiel

```xml
<speak>
  Hallo! <break time="500ms"/> Wie kann ich dir helfen?
</speak>
```

â± FÃ¼gt eine **halbe Sekunde Pause** ein.

---

##### ğŸ“Œ HÃ¤ufig genutzte SSML-Tags (Auswahl)

| Tag          | Funktion                                | Beispiel                                              |
| ------------ | --------------------------------------- | ----------------------------------------------------- |
| `<speak>`    | Wurzel jeder SSML-Ausgabe               | `<speak>Text hier</speak>`                            |
| `<break>`    | FÃ¼gt Pause ein                          | `<break time="300ms"/>`                               |
| `<prosody>`  | Steuert TonhÃ¶he, LautstÃ¤rke, Tempo      | `<prosody pitch="+10%" rate="slow">...</prosody>`     |
| `<emphasis>` | Betonung auf Wort/Satz                  | `<emphasis level="strong">wichtig</emphasis>`         |
| `<phoneme>`  | Definiert Lautschrift                   | `<phoneme alphabet="ipa" ph="ËˆhÃ¦loÊŠ">Hallo</phoneme>` |
| `<lang>`     | Schaltet auf andere Sprache             | `<lang xml:lang="en-US">Hello!</lang>`                |
| `<audio>`    | Spielt Audiodatei ab (nur in Cloud-TTS) | `<audio src="sound.mp3"/>`                            |

Anmerkung: Gute Entwickler kÃ¶nnen auch Mechanismen entwickeln, wie man solche SSML-Tags selbst in Libraries umsetzt, die dies nicht von sich aus unterstÃ¼tzen. SÃ¤tze mÃ¼ssen dann dann unter UmstÃ¤nden aufgeteilt werden, mehrere Audiodateien erstellt und am Ende zusammengefÃ¼gt werden. Dies wÃ¤re aber durchaus ein extrem komplexes und durchaus nicht sicheres und instabiles Prozedere.

---

##### ğŸ™ Beispiel: Mehrsprachiger Text mit Betonung

```xml
<speak>
  Willkommen bei deinem Sprachassistenten. <break time="300ms"/>
  <emphasis level="strong">Heute</emphasis> ist ein schÃ¶ner Tag.
  <lang xml:lang="en-US">The weather is sunny and warm.</lang>
</speak>
```

---

##### âš™ï¸ KompatibilitÃ¤t

| TTS-Engine            | SSML-UnterstÃ¼tzung | Hinweis                               |
| --------------------- | ------------------ | ------------------------------------- |
| **Google TTS**        | âœ… VollstÃ¤ndig      | auch `<audio>`-Einbindung mÃ¶glich     |
| **Amazon Polly**      | âœ… Sehr gut         | unterstÃ¼tzt viele `<voice>`-Stile     |
| **Coqui TTS**         | âš ï¸ Teilweise       | `<break>`, `<prosody>` oft verfÃ¼gbar  |
| **eSpeak / Festival** | âŒ Kaum             | meist keine native SSML-UnterstÃ¼tzung |

---

##### ğŸ›  Tipps fÃ¼r die Praxis

* SSML hilft, die **NatÃ¼rlichkeit und VerstÃ¤ndlichkeit** von Sprachausgaben zu steigern.
* Nutze es gezielt bei langen Dialogen, AufzÃ¤hlungen oder multilingualem Content.
* FÃ¼r lokale Assistenten mit z.â€¯B. `pyttsx3` ist SSML oft **nicht direkt nutzbar** â€“ dort musst du ggf. Pausen und TonhÃ¶he manuell steuern.

---

Ja, **es gibt mehrere Text-to-Speech-LÃ¶sungen, die vollstÃ¤ndig lokal arbeiten**, also **ohne Cloudverbindung** funktionieren. Das ist besonders wichtig fÃ¼r Datenschutz, OfflinefÃ¤higkeit und Embedding in Systeme wie deinen Sprachassistenten.

---

##### ğŸ—£ï¸ **Lokale TTS-Libraries â€“ ohne Cloud**

Hier ist eine Ãœbersicht:

| Library / Tool | Sprache | Lokal | QualitÃ¤t | SSML-UnterstÃ¼tzung     | Bemerkung                              |
| -------------- | ------- | ----- | -------- | ---------------------- | -------------------------------------- |
| **Coqui TTS**  | Python  | âœ…     | â­â­â­â­â˜†    | Teilweise (SSML-light) | Moderne TTS, sehr anpassbar            |
| **eSpeak NG**  | C/C++   | âœ…     | â­â­â˜†â˜†â˜†    | âŒ                      | Extrem leichtgewichtig, viele Sprachen |
| **Festival**   | C++     | âœ…     | â­â­â˜†â˜†â˜†    | âŒ                      | Veraltet, aber stabil                  |
| **RHVoice**    | C++     | âœ…     | â­â­â­â˜†â˜†    | âŒ                      | Gute QualitÃ¤t, Open Source             |
| **pyttsx3**    | Python  | âœ…     | â­â­â˜†â˜†â˜†    | âŒ                      | Wrapper um native Engines              |
| **MaryTTS**    | Java    | âœ…     | â­â­â­â˜†â˜†    | âœ…                      | Alte, aber robuste Plattform mit SSML  |
| **OpenTTS**    | Docker  | âœ…     | â­â­â­â­â˜†    | Ja (je nach Backend)   | Aggregiert Coqui, MaryTTS, etc.        |

---

##### ğŸ” Kurze Details zu den wichtigsten:

###### ğŸ”¸ **Coqui TTS**

* Modernes, Deep-Learning-basiertes System (Fork von Mozilla TTS)
* Offline trainierbar oder fertige Modelle nutzbar
* CLI & Python API
* Stimmen fÃ¼r viele Sprachen, inkl. Deutsch
* EingeschrÃ¤nkte SSML-Ã¤hnliche Features (`<break>`, `rate`, `pitch` per CLI/Args)

ğŸ‘‰ Empfohlen fÃ¼r dein Projekt, wenn du **QualitÃ¤t + LokalitÃ¤t** brauchst

---

###### ğŸ”¸ **eSpeak NG**

* Extrem leicht (\~2MB)
* UnterstÃ¼tzt 100+ Sprachen, aber Robot-Ã¤hnliche Stimme
* Ideal fÃ¼r embedded / low-resource-Systeme

ğŸ‘‰ FÃ¼r Prototypen oder Systeme mit **minimaler Leistung**

---

###### ğŸ”¸ **pyttsx3**

* Python-Wrapper um native TTS-Engines (Windows: SAPI5, macOS: NSSpeechSynthesizer, Linux: espeak)
* Einfach zu benutzen, aber begrenzte Kontrolle Ã¼ber Stimme und Stil
* Keine SSML-UnterstÃ¼tzung, nur rudimentÃ¤res Timing

ğŸ‘‰ Gut fÃ¼r **schnelle Demos** oder klassische Desktop-Sprachsynthese

---

###### ğŸ”¸ **MaryTTS**

* Java-basiert, mit Webserver & REST-API
* UnterstÃ¼tzt echtes SSML
* Deutsche Stimme enthalten
* LÃ¤ngere Startzeit, komplexer Aufbau

ğŸ‘‰ FÃ¼r **akademische oder stabile serverseitige Anwendungen**

---

##### âœ… Empfehlung fÃ¼r Bachelorarbeit

| Ziel                                     | Empfehlung                         |
| ---------------------------------------- | ---------------------------------- |
| Moderne TTS, lokale Nutzung, erweiterbar | **Coqui TTS**                      |
| Einfache Integration, geringe Ressourcen | **pyttsx3** oder **eSpeak NG**     |
| VollstÃ¤ndige SSML-Verarbeitung lokal     | **MaryTTS** oder Coqui (teilweise) |

---

## ğŸ“‘ **Gliederungsvorschlag fÃ¼r die Bachelorarbeit**

Hier ist ein **Vorschlag fÃ¼r die Gliederung deiner Bachelorarbeit**, passend zum Thema *â€Entwicklung eines lokalen Sprachassistentenâ€œ*. Die Gliederung ist modular, logisch aufgebaut und erlaubt sowohl technische Tiefe als auch theoretische Reflexion.

### 1. **Einleitung**

* Motivation und Relevanz des Themas
* Zielsetzung der Arbeit
* Abgrenzung und Umfang
* Aufbau der Arbeit

---

### 2. **Theoretische Grundlagen**

* Sprachassistenten: Definition und Bestandteile
* Ãœberblick Ã¼ber aktuelle Systeme (Alexa, Siri, Mycroft, etc.)
  * Vor- und Nachteile (Datenschutz, Cloudanbindung, keine Smart Home Integration wie bspw. openHAB, usw.)
* Komponenten im Detail:

  * Hotword Detection
  * Automatic Speech Recognition (ASR)
    * ErklÃ¤re ebenfalls am besten STT 
  * Natural Language Understanding (NLU)
    * ErklÃ¤re ebenfalls am besten NLP und warum NLU eine Teilmenge ist. 
  * Text-to-Speech (TTS)
* Datenschutz & Offline-Verarbeitung
* Einordnung in bestehende Arbeiten (State of the Art)

Was nicht schaden kann ist, wenn man hier noch KI-Grundlagen irgendwie erlÃ¤utert und erklÃ¤rt. Man hat ja verschiedene BerÃ¼hrungspunkte, weil man ja auch Modelle trainiert. Eventuell muss man ja auch Begriffe wie `Reinforcement Learning` beschreiben. Spracherkennungssysteme neigen meist zu `Ãœbertraining` (`overfitting`), was wÃ¤re dies denn Ã¼berhaupt erst? Da kann man mit theoretische Grundlagen sehr tief ins Thema einsteigen.

---

### 3. **Anforderungsanalyse**

* Funktionale Anforderungen (z.â€¯B. Hotword, STT, NLU)
* Nicht-funktionale Anforderungen (z.â€¯B. Offline-Betrieb, ModularitÃ¤t)
* Zielplattform (z.â€¯B. Raspberry Pi vs. Mini-PC)
* Auswahlkriterien fÃ¼r Frameworks und Komponenten
* Smart Home Integration (z. B. openHAB)

---

### 4. **Konzept und Architektur**

* Architekturentwurf des Sprachassistenten
* Beschreibung der Systemkomponenten

  * Audioaufnahme & Verarbeitung
  * Modulinteraktion
* Kommunikationsschnittstellen
* Datenflussdiagramm

---

### 5. **Implementierung**

* Projektstruktur & Technologie-Stack
* Beschreibung der wichtigsten Module:

  * Hotword-Erkennung (z.â€¯B. Porcupine)
  * Spracherkennung (z.â€¯B. Whisper/Vosk)
  * Intent-Erkennung (z.â€¯B. Rasa/Regex)
  * Textausgabe (z.â€¯B. Coqui TTS)
* Erweiterbarkeit & Custom Intents
* BeispielablÃ¤ufe / Interaktionen

Ich sage hier nur vielleicht. Man sollte sich nicht zu stark an diesen Libraries orientieren.

---

### 6. **Evaluation**

* Testmethodik (z.â€¯B. definierte Szenarien, Vergleich mit CloudlÃ¶sungen)
* Messkriterien:

  * Erkennungsrate
  * Latenzzeit
  * Ressourcenverbrauch
* Vergleich von Alternativen (z.â€¯B. Whisper vs. Vosk)
* Diskussion der Ergebnisse

Eine Machbarkeitsstudie macht man ja so oder so allgemein. In Bezug auf "Machtbarkeit" kann man ja zeigen, ob ein ehemaliges Szenario von Alexa und openHAB sich nun mit dem neu entwickelten System umsetzen lÃ¤sst. Wenn ja, dann hat man es ja geschafft ein "Konkurrenzprodukt" zu entwickeln, welches cloudunabhÃ¤ngig funktioniert, somit lokal ist, datenschutztechtlich unbedenklich, usw.

---

### 7. **Reflexion und Ausblick**

* Bewertung der Zielerreichung
* Technische und konzeptionelle Herausforderungen
* Verbesserungspotenziale
* Ausblick auf mÃ¶gliche Erweiterungen (z.â€¯B. Dialogmanagement, IoT-Anbindung)

Ich denke eins wird klar sein: Alexa nutzt bspw. verschiedene Skills. Mit diesen Skills lassen sich Apps, Anwendungen, GerÃ¤te von sehr vielen verschiedenen Anbietern integrieren. Auch, weil Skills communitybasiert oder durch Firmen/Organistationen entwickelt werden. HeiÃŸt viele BedienmÃ¶glichkeiten und Szenarien Ã¼ber viele Skills/Anwendungen sind hier natÃ¼rlich nicht integrierbar und am Ende nutzbar. Dies nimmt einen Nutzer auch eine gewisse KomfortabilitÃ¤t und einen gewissen Nutzen. WÃ¤ren aber bspw. manche Skills umsetzbar oder mÃ¼sste man die komplette Architektur des Prototypen umschreiben? KÃ¶nnte man z. B. GerÃ¤te von Somfy, Philips Hue oder Sonos auch direkt anbinden oder geht dies nur Ã¼ber ein Smart Home System wie openHAB?

---

### 8. **Fazit**

* Zusammenfassung der Arbeit
* Wichtigste Erkenntnisse
* PersÃ¶nliche Bewertung

---

### 9. **Anhang**

* CodeauszÃ¼ge
* Konfigurationsdateien
* Screenshots / Ablaufdiagramme
* Testprotokolle

---

### 10. **Literaturverzeichnis**

* Fachliteratur
* Dokumentationen verwendeter Tools
* Onlinequellen mit Zugriffsdatum

---

## Beispielprojekt

Hier ist ein **Beispielprojekt fÃ¼r einen lokalen Sprachassistenten** mit Fokus auf ModularitÃ¤t, VerstÃ¤ndlichkeit und Erweiterbarkeit â€“ ideal als Startpunkt fÃ¼r deine Bachelorarbeit.

Anmerkung: Das ist vom zu entwickelten Prototypen noch meilenweit entfernt.

---

### ğŸ“ Projekt-Setup: `voice_assistant/`

```plaintext
voice_assistant/
â”œâ”€â”€ main.py                       # Einstiegspunkt: orchestration der Komponenten
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml             # Konfiguration (Hotword, STT, NLU, TTS)
â”œâ”€â”€ hotword/
â”‚   â””â”€â”€ detector.py               # Hotword-Erkennung (Porcupine / Precise)
â”œâ”€â”€ stt/
â”‚   â””â”€â”€ transcriber.py            # Speech-to-Text (Whisper / Vosk)
â”œâ”€â”€ nlu/
â”‚   â””â”€â”€ intent_parser.py          # Intent-Erkennung (Rasa / Regex)
â”œâ”€â”€ tts/
â”‚   â””â”€â”€ synthesizer.py            # Text-to-Speech (Coqui TTS)
â”œâ”€â”€ core/
â”‚   â””â”€â”€ actions.py                # Antwortlogik / Aktionshandler
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ audio_utils.py            # Mikrofonaufnahme, Audio-Konvertierung
â”œâ”€â”€ data/
â”‚   â””â”€â”€ intents.yaml              # Trainingsdaten fÃ¼r Intents
â””â”€â”€ requirements.txt              # Python-AbhÃ¤ngigkeiten
```

---

### ğŸ§  KomponentenÃ¼berblick mit Mini-Codebeispielen

#### 1. `main.py`

```python
from hotword.detector import wait_for_hotword
from stt.transcriber import transcribe_audio
from nlu.intent_parser import parse_intent
from tts.synthesizer import speak
from core.actions import handle_intent

def main():
    print("System bereit. Sag das Hotword...")
    while True:
        wait_for_hotword()
        audio = utils.audio_utils.record_input()
        text = transcribe_audio(audio)
        intent = parse_intent(text)
        response = handle_intent(intent)
        speak(response)

if __name__ == "__main__":
    main()
```

---

#### 2. `hotword/detector.py` (z.â€¯B. mit `pvporcupine`)

```python
import pvporcupine
import pyaudio

def wait_for_hotword():
    porcupine = pvporcupine.create(keywords=["computer"])
    audio_stream = pyaudio.PyAudio().open(
        rate=porcupine.sample_rate,
        channels=1,
        format=pyaudio.paInt16,
        input=True,
        frames_per_buffer=porcupine.frame_length,
    )
    while True:
        pcm = audio_stream.read(porcupine.frame_length)
        pcm = list(int.from_bytes(pcm[i:i+2], 'little', signed=True) for i in range(0, len(pcm), 2))
        if porcupine.process(pcm) >= 0:
            break
```

---

#### 3. `stt/transcriber.py` (mit `whisper`)

```python
import whisper

model = whisper.load_model("base")

def transcribe_audio(audio_path="input.wav"):
    result = model.transcribe(audio_path)
    return result["text"]
```

---

#### 4. `nlu/intent_parser.py` (Regel- oder Rasa-basiert)

```python
import yaml

with open("data/intents.yaml", "r") as f:
    intent_data = yaml.safe_load(f)

def parse_intent(text):
    text = text.lower()
    for intent, patterns in intent_data.items():
        for pattern in patterns:
            if pattern in text:
                return intent
    return "unknown"
```

Beispiel fÃ¼r `data/intents.yaml`:

```yaml
play_music:
  - spiel musik
  - mach musik an
  - spiele ein lied
get_time:
  - wie spÃ¤t ist es
  - sag mir die uhrzeit
  - wie viel uhr
```

---

#### 5. `tts/synthesizer.py` (z.â€¯B. mit `pyttsx3` oder `coqui-tts`)

```python
import pyttsx3

engine = pyttsx3.init()

def speak(text):
    engine.say(text)
    engine.runAndWait()
```

---

#### 6. `core/actions.py`

```python
from datetime import datetime

def handle_intent(intent):
    if intent == "get_time":
        return f"Es ist {datetime.now().strftime('%H:%M')} Uhr."
    elif intent == "play_music":
        return "Okay, ich spiele Musik ab."
    else:
        return "Das habe ich leider nicht verstanden."
```

---

### ğŸ“¦ `requirements.txt`

```txt
pvporcupine
pyaudio
whisper
torch
pyttsx3
pyyaml
```

---

### ğŸ› ï¸ Was du jetzt damit machen kannst

* ğŸ”„ Eigene Intents und Aktionen hinzufÃ¼gen
* ğŸ” Komponenten austauschen (z.â€¯B. Whisper â†’ Vosk, pyttsx3 â†’ Coqui TTS)
* ğŸ§ª Performance- oder Vergleichsexperimente implementieren
* ğŸ” Datenschutz-Konzept einbauen (lokale vs. Cloud-Verarbeitung vergleichen)

---

Du hast bereits eine beeindruckend detaillierte Planung fÃ¼r deinen lokalen Sprachassistenten erstellt. Dennoch gibt es einige zusÃ¤tzliche Aspekte, die du in Betracht ziehen kÃ¶nntest, um dein System noch robuster und benutzerfreundlicher zu gestalten:

---

### ğŸ§  Weitere Ãœberlegungen fÃ¼r die Systemplanung

#### 1. **Benutzererkennung und Personalisierung**

* **Stimmerkennung**: Implementiere eine Funktion zur Identifikation von Nutzern anhand ihrer Stimme, um personalisierte Dienste anzubieten.
* **Profilmanagement**: ErmÃ¶gliche die Verwaltung mehrerer Benutzerprofile mit individuellen Einstellungen und PrÃ¤ferenzen.

#### 2. **Kontextbewusstsein**

* **Zeit- und Standortkontext**: BerÃ¼cksichtige Faktoren wie Tageszeit oder den aktuellen Raum, um angemessene Antworten zu liefern.
* **Verlaufsspeicherung**: Speichere vergangene Interaktionen, um Folgefragen besser zu verstehen und zu beantworten.

#### 3. **Multimodale Interaktion**

* **Visuelle RÃ¼ckmeldungen**: Integriere Displays oder LED-Anzeigen, um Informationen zusÃ¤tzlich visuell darzustellen.
* **Touch-Interaktion**: ErgÃ¤nze die Sprachsteuerung durch Touch-Interfaces fÃ¼r eine flexiblere Bedienung.([architektur-online][1])

#### 4. **Fehlertoleranz und Robustheit**

* **Fuzzy Matching**: Implementiere unscharfe Suchalgorithmen, um auch bei ungenauen Eingaben passende Ergebnisse zu liefern.
* **Fallback-Strategien**: Definiere klare Strategien fÃ¼r den Umgang mit nicht erkannten oder missverstandenen Befehlen.([Internet fÃ¼r Architekten][2])

#### 5. **Datenschutz und Sicherheit**

* **Lokale Datenverarbeitung**: Stelle sicher, dass alle Daten lokal verarbeitet werden, um die PrivatsphÃ¤re der Nutzer zu schÃ¼tzen.
* **VerschlÃ¼sselung**: Nutze VerschlÃ¼sselungstechniken fÃ¼r gespeicherte Daten und KommunikationskanÃ¤le.

---

### ğŸ§© Integration in bestehende Systeme

* **Smart-Home-KompatibilitÃ¤t**: Stelle sicher, dass dein Assistent mit gÃ¤ngigen Smart-Home-Plattformen wie openHAB, kompatibel ist.
* **Offene Schnittstellen**: Biete APIs oder andere Schnittstellen an, um die Integration in andere Systeme zu erleichtern.([DIE WELT][3])

---

### ğŸ› ï¸ Entwicklung und Wartung

* **Modularer Aufbau**: Strukturiere dein System modular, um einzelne Komponenten leichter aktualisieren oder austauschen zu kÃ¶nnen.
* **Dokumentation**: Erstelle umfassende Dokumentationen fÃ¼r Entwickler und Endnutzer, um die Bedienung und Weiterentwicklung zu erleichtern.

---

### ğŸ“š WeiterfÃ¼hrende Ressourcen

* **Open-Source-Projekte**: Analysiere bestehende Open-Source-Sprachassistenten wie Mycroft oder Rhasspy, um von deren Architektur und Funktionen zu lernen.
* **Fachliteratur**: Ziehe aktuelle Studien und VerÃ¶ffentlichungen heran, um dein System auf dem neuesten Stand der Technik zu halten.

---

[1]: https://www.architektur-online.com/kolumnen/ki-am-bau-maschinell-planen-und-bauen?utm_source=chatgpt.com "KI am Bau: Maschinell planen und bauen - architektur-online"
[2]: https://internet-fuer-architekten.de/kategorien/fachartikel-und-meldungen/apps-fuer-planer/?utm_source=chatgpt.com "Apps fÃ¼r Planer - Internet fÃ¼r Architekten"
[3]: https://www.welt.de/230560877?utm_source=chatgpt.com "Funkstandards im Smart Home - das Problem mit der Kommunikation"

## Sprachbefehl vs. Konversation

Ein Sprachbefehl ist in der Regel ein Satz und darauf gibt es eine Antwort. FÃ¼hrt man ein GesprÃ¤ch bzw. in einer Konversation, dann sind die beiden GesprÃ¤chspartner mit ihrem Dialog stÃ¤ndig im Wechsel. Mal redet die eine Seite mehrere SÃ¤tze, mal die andere. Mal redet eine Seite mal kÃ¼rzer und die andere auch, mal redet die eine Seite kÃ¼rzer und die andere Seite lÃ¤nger. Ein GesprÃ¤ch bzw. eine Konversation ist damit viel vielschichtiger und komplizierter. Ein Sprachbefehl kann man sich somit als eine Ebene vorstellen. Zu einem Satz gibt es eine Antwort. Je nach Satz variiert die Antwort. Selbst wenn die Sprachantworten eine gewisse Varianz (z. B. durch `random`) aufweist, kann man dies exakt einer Ebenen zuordnen. Bei einem GesprÃ¤ch hingegen entsprecht jeder Dialogwechsel einer neuen Ebene. Da es pro Ebene unter UmstÃ¤nden zu unterschiedlichen, durchaus auch zufÃ¤lligen randomiserten AntwortmÃ¶glichkeiten kommen kann, braucht man auch viele Verzweigungen pro Ebene. Denkbar ist ja auch, dass je nach Verzweigung das GesprÃ¤ch sich vom Kontext her verÃ¤ndert.

Ist so etwas mit einem Sprachassistenten Ã¼berhaupt realisierbar? StÃ¶ren vielleicht lÃ¤ngere Antwortzeiten den GesprÃ¤chfluss?

Absolut â€“ du beschreibst ein typisches **Command-and-Control-System**, das **eindeutige Sprachbefehle** verarbeitet (z.â€¯B. â€Licht im Wohnzimmer anâ€œ) statt eine flieÃŸende Konversation wie mit ChatGPT zu fÃ¼hren.

---

### ğŸ§  Theoretischer Durchlauf: Sprachbefehl statt Konversation

#### ğŸ¯ Ziel:

Verarbeitung von **einzelnen Sprachbefehlen (z.â€¯B. Satz)** â†’ Aktion ausfÃ¼hren â†’ Antwort optional

---

#### ğŸ”„ Systemarchitektur (vereinfachtes Modell)

1. **Mikrofon**
   â†’ Audioaufnahme

2. **Hotword-Erkennung (optional)**
   â†’ z.â€¯B. â€Computerâ€œ oder â€Hey Homeyâ€œ

3. **ASR (Automatic Speech Recognition)**
   â†’ Umwandlung: â€Schalte das Licht im Wohnzimmer anâ€œ â†’ Text

4. **Intent-Erkennung**
   â†’ Mapping des Satzes auf einen definierten Befehl:

   ```json
   {
     "intent": "light_on",
     "location": "wohnzimmer"
   }
   ```

5. **AusfÃ¼hrung**
   â†’ Steuerbefehl an Smart-Home-System senden

6. **(Optional: RÃ¼ckmeldung)**
   â†’ z.â€¯B. â€Licht im Wohnzimmer ist jetzt an.â€œ

---

#### âœ… Was wird benÃ¶tigt?

##### ğŸ—£ Sprachverarbeitung

| Modul            | Tool/Lib                                                        | Lokal mÃ¶glich? |
| ---------------- | --------------------------------------------------------------- | -------------- |
| ASR              | z.â€¯B. [Vosk](https://alphacephei.com/vosk/), Whisper, Coqui STT | âœ…              |
| Intent-Erkennung | Rasa NLU, Snips NLU, RegEx-basiert, Duckling                    | âœ…              |

---

##### ğŸ§© Intent-Erkennung: Wie?

Du brauchst eine Methode, die **vordefinierte SÃ¤tze** erkennt. Es gibt drei Wege:

1. âœ… **Regel- oder Musterbasiert (z.â€¯B. RegEx)**
   Ideal bei wenigen, klaren Befehlen.

   ```python
   re.match(r"schalte das licht im (?P<room>\w+) an", text)
   ```

2. âœ… **Slot-Filling / NLU (z.â€¯B. Snips oder Rasa)**
   Nutzt Trainingsdaten, erkennt flexible Variationen:

   * â€Mach das Wohnzimmerlicht anâ€œ
   * â€Bitte Licht einschalten im Wohnzimmerâ€œ

3. âš ï¸ **LLM / KI-basiert (Whisper â†’ GPT)**
   Overkill fÃ¼r simple Befehle, aber bei hoher FlexibilitÃ¤t denkbar.

---

#### ğŸ› Vorteile dieses Ansatzes

* **Robust** bei klaren Befehlen
* **Geringe Rechenlast** â€“ perfekt fÃ¼r Embedded Devices
* **Besser steuerbar** als ein dialogbasiertes System

---

#### ğŸ”’ Datenschutz und ZuverlÃ¤ssigkeit

* Alles kann **lokal** laufen (z.â€¯B. Raspberry Pi 5)
* Kein Cloudzugriff notwendig
* OfflinefÃ¤hig und DSGVO-konform

---

#### ğŸ§  Fazit

| Aspekt                  | Bewertung                                     |
| ----------------------- | --------------------------------------------- |
| Konzeptionell umsetzbar | âœ… Ja, mit bewÃ¤hrter Technik                   |
| Lokal realisierbar      | âœ… VÃ¶llig ohne Cloud mÃ¶glich                   |
| KomplexitÃ¤t             | âš–ï¸ Mittel â€“ je nach FlexibilitÃ¤t              |
| Erweiterbarkeit         | âœ… Modular mit Intent-Slots                    |
| KonversationsfÃ¤higkeit  | âŒ Nicht erforderlich, bewusst nicht verwendet |

---

### ğŸ§  Ziel: **KonversationsfÃ¤higer Sprachassistent**

Jetzt steigen wir in die **dialogorientierte Sprachinteraktion** ein â€“ also **Konversation statt einzelner Sprachbefehle**. Das ist komplexer, aber auch deutlich flexibler und menschlicher in der Interaktion.

Statt:

> â€Schalte das Licht im Wohnzimmer an.â€œ

Sowas wie:

> **â€Mir ist zu dunkel.â€œ** â†’ â€MÃ¶chtest du das Licht im Wohnzimmer einschalten?â€œ â†’ â€Ja, bitte.â€œ

---

#### ğŸ§© Architektur: Was braucht ein konversationsfÃ¤higes System?

Hier ist der **erweiterte Ablauf** im Vergleich zum â€Befehl-onlyâ€œ-Modell:

1. ğŸ¤ **Mikrofon**
   â†’ Audioaufnahme

2. ğŸ›‘ **Hotword-Erkennung** (wie gehabt)
   â†’ z.â€¯B. â€Hey Lunaâ€œ

3. ğŸ—£ **ASR (Speech-to-Text)**
   â†’ z.â€¯B. Whisper oder Vosk: Sprache â†’ Text

4. ğŸ¤– **Natural Language Understanding (NLU)**
   â†’ Erkennen von Intents + EntitÃ¤ten + Stimmungen
   â†’ z.â€¯B. â€Mir ist zu dunkelâ€œ â†’ Intent: `discomfort_light`

5. ğŸ’¬ **Dialog-Manager (State & Kontext)**
   â†’ Verwaltung des GesprÃ¤chsverlaufs und Kontexts
   â†’ z.â€¯B.: â€Dunkelâ€œ erkannt â†’ nÃ¤chste Aktion: â€Licht anbietenâ€œ
   â†’ Kontext bleibt bestehen: Wenn spÃ¤ter â€Jaâ€œ kommt, wird das Licht eingeschaltet

6. ğŸ”„ **Antwortgenerierung (NLG)**
   â†’ z.â€¯B. Templating (â€MÃ¶chtest du das Licht einschalten?â€œ)
   â†’ oder generativ (Chatbot-Style)

7. ğŸ—£ **TTS (Text-to-Speech)**
   â†’ Lokale Stimme antwortet

8. ğŸ”§ **Aktion ausfÃ¼hren**
   â†’ API-Aufruf, z.â€¯B. Lichtsteuerung

---

#### ğŸ› ï¸ Was brauchst du zusÃ¤tzlich (gegenÃ¼ber Befehlssystem)?

| Funktion                               | Beschreibung                                          | Beispiele / Tools                                  |
| -------------------------------------- | ----------------------------------------------------- | -------------------------------------------------- |
| **Dialogmanager**                      | Merkt sich, was gesagt wurde & wie reagiert wurde     | Rasa Core, OpenDialog, eigene FSM                  |
| **Kontextbezug**                       | Z.â€¯B. â€Mach es hellerâ€œ â†’ Welcher Raum? Welches GerÃ¤t? | Kontexttracking via State                          |
| **EntitÃ¤ten-Tracking**                 | RÃ¤ume, GerÃ¤te, Uhrzeiten, ...                         | Rasa NLU, Snips, Duckling                          |
| **BestÃ¤tigungen / RÃ¼ckfragen**         | Wenn Absicht nicht eindeutig ist                      | â€Meinst du das Wohnzimmerlicht?â€œ                   |
| **Stimmungs- und Intentionserkennung** | FÃ¼r natÃ¼rlichere Antworten                            | â€Du klingst gestresst â€“ soll ich Musik abspielen?â€œ |

---

#### ğŸ” Beispiel-Dialog (mit Kontexthandling)

```text
Nutzer: Mir ist kalt.
Assistent: MÃ¶chtest du die Heizung im Wohnzimmer erhÃ¶hen?
Nutzer: Ja bitte.
â†’ Aktion: Heizung +2Â°C im Wohnzimmer
```

Oder komplexer:

```text
Nutzer: Ich hab Hunger.
Assistent: Soll ich dir ein Rezept vorschlagen oder etwas bestellen?
Nutzer: Vorschlag klingt gut.
Assistent: Wie wÃ¤râ€™s mit Pasta mit Tomaten?
```

---

#### ğŸ§  Wie speichert man Kontext?

Je nach Framework gibtâ€™s diese MÃ¶glichkeiten:

* **Finite-State-Machine**: Einfach, kontrolliert â†’ gut fÃ¼r definierte AblÃ¤ufe
* **Frame-based Systems** (Slots): Ã„hnlich wie Formulare
* **Generativ mit LLMs**: GPT oder Ã¤hnliche Modelle mit Kontextprompting (aufwendig)

---

#### âœ… Lokale Open-Source-Tools fÃ¼r Konversation

| Komponente        | Empfohlene Tools                |
| ----------------- | ------------------------------- |
| NLU + Dialog      | Rasa (NLU + Core), Snips NLU    |
| ASR               | Whisper, Vosk                   |
| TTS               | Coqui TTS, MaryTTS              |
| Intent-Management | YAML/JSON mit FSM / Rasa Domain |
| Memory/State      | Redis, SQLite, oder im RAM      |

---

#### âš–ï¸ Vergleich: Befehl vs. Konversation

| Aspekt             | Sprachbefehl-System  | Konversationssystem                    |
| ------------------ | -------------------- | -------------------------------------- |
| Einfachheit        | âœ… Sehr einfach       | âŒ Komplexer Dialogmanager nÃ¶tig        |
| NatÃ¼rlichkeit      | âŒ EingeschrÃ¤nkt      | âœ… Sehr hoch                            |
| FehleranfÃ¤lligkeit | âœ… Kaum Kontextfehler | âŒ Kontext muss korrekt getrackt werden |
| Ressourcenbedarf   | âœ… Gering             | âŒ HÃ¶her (mehr RAM/CPU)                 |
| Lokale Umsetzung   | âœ… Einfach            | âœ… MÃ¶glich, aber aufwendiger            |

---

#### ğŸ“Œ Fazit

Ein konversationsfÃ¤higer Sprachassistent ist:

* **Deutlich komplexer** (du brauchst Kontext, RÃ¼ckfragen, State)
* **Realistisch umsetzbar lokal**, mit Tools wie **Rasa**, **Coqui**, **Whisper**
* Ideal fÃ¼r smarte Umgebungen, wo auch **implizite WÃ¼nsche** geÃ¤uÃŸert werden (â€Mir ist kaltâ€œ, â€Ich bin mÃ¼deâ€œ, ...)

---

#### Beispiel: Dialog-Engine mit ZustandsÃ¼bergÃ¤ngen (FSM)

Diese Beispiel-Engine in Python simuliert einen **zustandsbasierten Sprachdialog** (Finite-State-Machine):

### ğŸ“ `dialog_engine.py`

```python
class DialogEngine:
    def __init__(self):
        self.state = "START"

    def handle_input(self, user_input):
        if self.state == "START":
            if "hallo" in user_input.lower():
                self.state = "GREETING"
                return "Hallo! Wie kann ich dir helfen?"
            return "Bitte beginne mit einer BegrÃ¼ÃŸung."

        elif self.state == "GREETING":
            if "licht" in user_input.lower():
                self.state = "LIGHT_CONFIRM"
                return "MÃ¶chtest du das Licht einschalten?"
            return "Das habe ich nicht verstanden. MÃ¶chtest du das Licht einschalten?"

        elif self.state == "LIGHT_CONFIRM":
            if user_input.lower() in ["ja", "ja bitte"]:
                self.state = "END"
                return "Licht wird eingeschaltet. Noch etwas?"
            else:
                self.state = "END"
                return "Okay, kein Licht. Noch etwas?"

        elif self.state == "END":
            if "nein" in user_input.lower():
                self.state = "START"
                return "Okay, bis zum nÃ¤chsten Mal!"
            else:
                self.state = "GREETING"
                return "Womit kann ich dir sonst helfen?"

# --- Demo-Nutzung ---
if __name__ == "__main__":
    engine = DialogEngine()
    while True:
        user = input("Du: ")
        reply = engine.handle_input(user)
        print("Assistent:", reply)
```

Hier wird ziemlich deutlich, dass die Varianz nicht groÃŸ ist. Je grÃ¶ÃŸer die Varianz, desto mehr if-Verzweigungen bzw. Verschachtelungen benÃ¶tigt man. Je mehr Ebenen das GesprÃ¤ch hat, desto komplizierter wird das Ganze und man muss deutlich mehr SonderfÃ¤lle berÃ¼cksichtigen.

---

#### Beispiel: Minimaler Konversationsbaum in YAML (z.â€¯B. fÃ¼r Rasa)

Das folgende YAML definiert einen **einfachen Dialog** in einer Struktur, wie Rasa oder ein FSM-Parser sie nutzen kÃ¶nnte:

##### ğŸ“„ `dialogs.yaml`

```yaml
version: "1.0"
dialogs:
  - state: start
    input: "hallo"
    next: greeting
    response: "Hallo! Wie kann ich dir helfen?"

  - state: greeting
    input: "ich will das licht einschalten"
    next: confirm_light
    response: "MÃ¶chtest du das Licht jetzt einschalten?"

  - state: confirm_light
    input: "ja"
    next: light_on
    response: "Okay, das Licht wird eingeschaltet."

  - state: confirm_light
    input: "nein"
    next: end
    response: "Alles klar, kein Licht."

  - state: light_on
    input: "*"
    next: end
    response: "Kann ich sonst noch etwas fÃ¼r dich tun?"

  - state: end
    input: "nein"
    next: start
    response: "Bis spÃ¤ter!"
```

Diese Datei kannst du mit einem simplen YAML-Interpreter oder selbstgeschriebenem FSM-Parser nutzen. Die `*`-Eingabe dient als Platzhalter fÃ¼r â€alles andereâ€œ.

---

Hier ist ein **Minimaler Konversationsbaum** als **einfache textbasierte Baumstruktur** (Diagramm-artig). Dieser stellt denselben Dialogfluss wie im YAML-Beispiel dar â€“ aber in **grafischer Baumform**, wie du ihn dir konzeptionell vorstellen wÃ¼rdest.

##### Beispiel: Minimaler Konversationsbaum â€“ Baumstruktur (Dialoglogik)

```
START
 â””â”€â”€ "hallo"
     â†“
GREETING
 â””â”€â”€ "ich will das licht einschalten"
     â†“
CONFIRM_LIGHT
 â”œâ”€â”€ "ja"
 â”‚    â†“
 â”‚  LIGHT_ON
 â”‚    â””â”€â”€ "*"
 â”‚         â†“
 â”‚       END
 â”‚         â””â”€â”€ "nein" â†’ START
 â”‚
 â””â”€â”€ "nein"
      â†“
    END
      â””â”€â”€ "nein" â†’ START
```

---

##### ğŸ“– ErklÃ¤rung der ZustÃ¤nde:

* **START**
  â†’ Einstiegspunkt, wartet auf BegrÃ¼ÃŸung (â€halloâ€œ)

* **GREETING**
  â†’ Fragt: Was mÃ¶chtest du tun? Erkennt: â€Ich will das Licht einschaltenâ€œ

* **CONFIRM\_LIGHT**
  â†’ Fragt zur Sicherheit: â€MÃ¶chtest du das Licht jetzt einschalten?â€œ

* **LIGHT\_ON**
  â†’ Antwort: â€Okay, Licht wird eingeschaltet.â€œ
  â†’ Danach optionale RÃ¼ckfrage: â€Noch etwas?â€œ

* **END**
  â†’ Beendet GesprÃ¤ch, MÃ¶glichkeit zum Neustart (z.â€¯B. bei â€neinâ€œ)

---

##### ğŸ§© ErweiterungsmÃ¶glichkeiten:

* FÃ¼ge z.â€¯B. weitere Intents wie â€Temperatur Ã¤ndernâ€œ, â€Musik spielenâ€œ, etc. ein
* Baue ZustÃ¤nde fÃ¼r Unsicherheiten oder Wiederholungen ein
* Integriere Slot-Filling (z.â€¯B. Raum, Helligkeit, Uhrzeit)

---

Es gibt Sprachassistenten, die **Ã¼ber reine Sprachbefehle hinausgehen** und tatsÃ¤chlich **mehrstufige Konversationen** ermÃ¶glichen â€“ Ã¤hnlich wie Chatbots. Allerdings ist das **eher die Ausnahme als die Regel**, besonders bei **lokalen oder Open-Source-Systemen**.

Hier ist ein **vergleichender Ãœberblick**, der zeigt, welche Sprachassistenten eher **befehlsgesteuert** agieren und welche eine **dialogorientierte Interaktion** (Konversation) ermÃ¶glichen:

## ğŸ§  Vergleich: Sprachbefehl vs. Konversation bei Sprachassistenten

| Sprachassistent                   | Typ                  | Befehlsgesteuert | KonversationsfÃ¤hig | Konversationsspeicher   | Bemerkungen                                             |
| --------------------------------- | -------------------- | ---------------- | ------------------ | ----------------------- | ------------------------------------------------------- |
| **Amazon Alexa**                  | Cloud                | âœ… Ja             | âš ï¸ Teilweise       | ğŸ” Kontext begrenzt     | Nur mit aktivem Skill/Dialogmodell konversationsfÃ¤hig   |
| **Google Assistant**              | Cloud                | âœ… Ja             | âš ï¸ Teilweise       | ğŸ” Nur kurzzeitig       | Kontext wird oft nach Antwort gelÃ¶scht                  |
| **Apple Siri**                    | Cloud                | âœ… Ja             | âŒ Nein             | âŒ Kein Kontext          | Sehr befehlsfokussiert                                  |
| **Mycroft**                       | Open Source (lokal)  | âœ… Ja             | âš ï¸ MÃ¶glich         | âš ï¸ EntwicklerabhÃ¤ngig   | FSMs und dialog module mÃ¶glich                          |
| **Rhasspy**                       | Open Source (lokal)  | âœ… Ja             | âŒ Nein             | âŒ Kein interner Kontext | Fokus auf Einmal-Befehle                                |
| **Jarvis.ai / ChatGPT + TTS/STT** | Hybrid (lokal/cloud) | âœ… Ja             | âœ… Ja               | âœ… VollstÃ¤ndig           | Nutzt z.â€¯B. Whisper + ChatGPT fÃ¼r KonversationsverlÃ¤ufe |
| **Home Assistant Assist**         | Lokal/Cloud          | âœ… Ja             | âŒ (noch nicht)     | âŒ Kein Dialogspeicher   | Fokus auf Smarthome-Befehle                             |

---

### ğŸ” Interpretation:

* Die meisten Sprachassistenten (vor allem kommerzielle) sind auf **Einzelbefehle** optimiert.
* **Echte Konversationen** wie bei einem Chatbot erfordern:

  * **Kontextspeicherung** Ã¼ber mehrere Turns hinweg
  * **Dialog-Management** (z.â€¯B. mit Zustandsautomaten oder Intenterkennung + Slot-Filling)
  * ggf. **LLM-UnterstÃ¼tzung** (Large Language Models)
* Systeme wie **ChatGPT + TTS/STT-Wrapper** oder eigene FSM-basierte Dialogsysteme bieten die **flexibelste Form echter Konversation**.

---

### ğŸ§© Fazit:

> Sprachassistenten wie **Siri, Alexa, Rhasspy oder Mycroft** sind primÃ¤r auf **sprachgesteuerte Einzelbefehle** ausgelegt. FÃ¼r **kontextbasierte GesprÃ¤che** brauchst du entweder ein System wie **ChatGPT**, ein eigenes LLM-Backend â€“ oder du erweiterst Open-Source-LÃ¶sungen wie **Mycroft** mit Dialogmanagementlogik.

## Bonustipp

### ASR/STT

Um verbal ASR oder STT richtig zu beschreiben, empfiehlt es sich Worte wie "dekodieren" und "konvertieren" zu verwenden, weil es somit einfach technischer beschrieben ist:

> Automatic Speech Recognition (ASR) bzw. Speech-to-Text (STT) ist eine Kombination von Prozessen und Software, welches menschliche Sprache dekodiert und es in einen digitalen Text konvertiert.

### TTS

Um Verbal TTS zu beschreiben empfiehlt es sich ebenfalls Worte wie "kodieren" und "konvertieren" zu verwenden:

> Text-to-Speech ist eine Kombination von Prozessen und Software, die einen digitalen Text in synthetische menschliche Sprache kodiert und konvertiert.

### STTS und TTSS

Bei cloudbasierten Services spricht man auch gerne von `STTS` und `TTSS`, also von `Speech-to-Text-Services` und `Text-to-Speech-Services`.

---

Ein koversationsfÃ¤higer Sprachassistent wÃ¼rde wahrscheinlich den Rahmen einer Abschlussarbeit ordentlich spregen. KÃ¶nnte aber in Anschluss zu einer guten vorangegangenen Abschlussarbeit ein sehr spannendes Thema fÃ¼r eine Masterarbeit werden.
