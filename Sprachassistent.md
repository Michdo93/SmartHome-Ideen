# Bachelorarbeit: Entwicklung eines modularen, datenschutzfreundlichen und lokalen Sprachassistenten

---

## M√∂gliche Titelvorschl√§ge

- "Entwicklung eines datenschutzfreundlichen Sprachassistenten auf Basis freier Software"
- "Vergleich von ASR-Engines f√ºr Offline-Sprachassistenten"
- "Modularer Sprachassistent f√ºr den lokalen Einsatz: Architektur und Prototyp"
- "Sprachverarbeitung ohne Cloud: Konzeption und Evaluation eines lokalen Sprachassistenten"
- "Custom Wakeword Detection in Embedded Systems: Architektur, Training und Einsatz"
- "Open Source Sprachassistent: Ein systemischer Vergleich freier Komponenten"
- ...

---

## Hintergrund

Sprachassistenten wie Alexa, Siri oder Google Assistant haben sich im Alltag vieler Menschen etabliert. Allerdings sind diese Systeme oft cloudgebunden, datenschutzrechtlich bedenklich und schwer individualisierbar. Dies er√∂ffnet spannende M√∂glichkeiten f√ºr die Entwicklung eines eigenen, lokal laufenden Sprachassistenten mit freier Wahl der Komponenten.

---

### Problemstellung

- Cloudverbindungen k√∂nnen auch √∂fter mal gest√∂rt oder abgebrochen sein.
  - Ein lokaler Sprachassistent w√ºrde zumindest lokal funktionierende Befehle bearbeiten und lokal verf√ºgbare Informationen zur Verf√ºgung stellen k√∂nnen.
- Datenschutzrechtlich h√∂rt ein Sprachassistent dauerhaft mit und wartet, bis ein sog. Hotword (auch trigger word oder wake word) erkannt wird.
  - Ist wirklich garantiert, dass nur Daten aufgezeichnet und an eine Cloud gesendet werden, wenn ein Hotword verwendet wurde oder lauscht das System dauerhaft mit?
  - Anmerkung: Deutsche Begriffe sind Aktivierungswort, Aufwachwort, Aufwachbefel, Triggerwort, usw.
- Sprachassistenten sind oft nur beschr√§nkt individualisierbar, Anwendungen werden eingestellt oder eingeschr√§nkt.
  - Google Nest erm√∂glicht es oft bspw. Smart Home Ger√§te zwar anzubinden, diese k√∂nnen dar√ºber aber nicht gesteuert werden, sondern meist nur Zust√§nde abgefragt werden.
  - Amazon hat bspw. ihre Policy ge√§ndert und seitdem wird das openHAB Skill aus Sicherheitsgr√ºnden nicht mehr unterst√ºtzt, damit w√§re ein Smart Home, welches √ºber openHAB gesteuert wurde nicht mehr bedienbar.

---

## Ziel der Arbeit

Ziel der Arbeit ist die Entwicklung und prototypische Umsetzung eines lokalen Sprachassistenzsystems auf Basis freier Software und Hardware. Der Fokus liegt auf Modularit√§t, Erweiterbarkeit und Datenschutzfreundlichkeit. Es sollen verschiedene Komponenten (Spracherkennung, Hotword-Erkennung, Intent-Parsing, Text-to-Speech) integriert und ggf. evaluiert werden. Es muss eine Anbindung zur openHAB REST API m√∂glich sein.

---

## Teilziele

- Aufbau einer modularen Architektur f√ºr Sprachassistenz
- Integration von Hotword-Detection, ASR (Automatic Speech Recognition, hier: Speech-to-Text), NLU (Natural Language Understanding, hier: Intent-Erkennung ist eine Teilmenge des Natural Language Processings), TTS (Text-to-Speech)
- (Optional) Vergleich verschiedener frei verf√ºgbarer ASR/NLU/TTS-Frameworks hinsichtlich:
  - Latenz
  - Genauigkeit
  - Ressourcenverbrauch
- Anpassbarkeit durch neue Triggerw√∂rter und Intents
- Betrachtung datenschutzrechtlicher Implikationen (lokal vs. Cloud)
- Anbindung an die openHAB REST API
  - Triggersatz soll bspw.
    - Command an ein openHAB Item schicken und dar√ºber dann ein Ger√§t steuern
    - State von einem openHAB Item abfragen und per Text-to-Speech diesen Systemzustand ausgeben
- eigene REST API zur Verf√ºgung stellen
  - ein Endpunkt k√∂nnte zum Beispiel genutzt werden, dass man einen Text schickt, der dann per Text-to-Speech ausgegeben wird.
  - ...
- (Optional) Einbindung einfacher Aktionen (z.‚ÄØB. Timer/Wecker setzen, Terminerinnerungen speichern & Termine vorlesen, Wetter abfragen, "das Internet durchsuchen")-
- (Optional) Einbindung von Ger√§ten unabh√§ngig von Smart Home Systemen
  - Zum Beispiel die Python Library `SoCo` (Sonos Controller)
    - Man k√∂nnte die `play_uri`-Funktion nutzen, dass die Text-to-Speech-Ausgabe nicht √ºber einen Lautsprecher am entwickelten Ger√§t erfolgt, sondern √ºber einen Sonos-Lautsprecher. Entsprechend muss dies konfigurierbar und eintsellbar sein.

---

## M√∂gliche Forschungsschwerpunkte

- Vergleich von Offline-STT-Engines: `Whisper`, `Vosk`, `DeepSpeech`
- Evaluation von NLU-Systemen: `Rasa`, `Snips`, Transformer-basierte L√∂sungen
- Training eigener Hotword-Modelle mit `Porcupine`, `Mycroft Precise`
- Einfluss der Hardware auf Performance (z.‚ÄØB. Raspberry Pi vs. Jetson Nano)
- Kombination regelbasierter und ML-basierter Dialogmodelle
- Lokaler Datenschutz vs. Cloud-Performance

---

## Eingesetzte Technologien (Auswahl)

- Programmiersprache: **Python**
- Hotword-Erkennung: `Porcupine`, `Mycroft Precise`
- Speech-to-Text: `Whisper`, `Vosk`, `DeepSpeech`
- NLU: `Rasa`, `Snips`, eigene ML-Modelle mit `transformers`
- TTS: `Coqui TTS`, `eSpeak`, `Festival`
- Plattform: **Raspberry Pi 5**, ggf. Mini-PC

(Unten sieht man, dass es hier verschiedene Alternativen zur Auswahl gibt. Auch mit Erfahrungen w√§hrend der Bearbeitung darf hier nat√ºrlich die Technologien getauscht werden.)

---

## Voraussetzungen

- Interesse an Sprachverarbeitung, KI und Embedded Systems
- Gute Kenntnisse in Python
- Grundverst√§ndnis maschinellen Lernens und Linux-Systemen
- Motivation zur Arbeit mit Open-Source-Technologien

---

## Betreuung & Rahmen

- Diese Arbeit eignet sich besonders f√ºr Studierende der Informatik, Medieninformatik oder verwandter Studieng√§nge.
- Umfang: Bachelorarbeit (ca. 16 Wochen Vollzeit)
- Bei Interesse ist eine Weiterentwicklung zur Masterarbeit denkbar.

---

## Beispielaufbau & Vorgehensweise

Ein eigener Sprachassistent ist ein spannendes Projekt ‚Äì technisch anspruchsvoll, aber mit vielen Freiheiten in der Gestaltung. Unten findest du eine systematische √úbersicht √ºber alle ben√∂tigten **Komponenten**: Hardware, Software, KI-Module und die empfohlene Toolchain.

---

### üîß **1. Hardware**

#### ‚úÖ *Mindestanforderungen (f√ºr lokale Verarbeitung)*:

* **Mini-PC oder Einplatinenrechner**:

  * *Raspberry Pi 5 (2GB/4GB/8GB/16GB)* ‚Äì Einsteigerfreundlich, f√ºr einfache Anwendungen ausreichend.
  * *Alternativen*: NVIDIA Jetson Nano (wenn du KI lokal trainieren willst), oder ein Mini-PC wie Intel NUC.
* **Mikrofon**:

  * *USB-Mikrofon* mit guter Aufnahmequalit√§t (z.‚ÄØB. Samson Go Mic, ReSpeaker 2-Mic HAT f√ºr Raspberry Pi).
  * *Array-Mikrofone* (f√ºr bessere Erkennung und Ger√§uschunterdr√ºckung).
* **Lautsprecher**:

  * Per Klinke oder USB anschlie√übar. Auch kleine Bluetooth-Speaker sind m√∂glich.

> üîÅ Empfehlung: Beginne mit einem USB-Mikrofon + Pi 5 oder Mini-PC zum Testen, bevor du in Mikrofon-Arrays oder Jetson investierst.

---

### üß† **2. Software-Komponenten & Architektur**

Die Sprachassistenz umfasst mehrere Stufen:

| **Schritt**                   | **Funktion**                           | **Empfohlene Tools/Libs**                                        |
| ----------------------------- | -------------------------------------- | ---------------------------------------------------------------- |
| **1. Hotword Detection**      | Erkennung des Aktivierungswortes       | `Porcupine`, `Snowboy`, `Mycroft Precise`, `Picovoice`           |
| **2. Spracherkennung (ASR)**  | Umwandlung von Sprache in Text         | `Vosk`, `Whisper`, `DeepSpeech`, Google STT (Cloud)              |
| **3. NLU (Intent-Erkennung)** | Bedeutung/Text in Aktion umwandeln     | `Rasa`, `Snips NLU`, eigene Modelle mit `Transformers`           |
| **4. Antwortgenerierung**     | Aktion ausf√ºhren oder Antwort erzeugen | Eigene Logik / GPT-Anbindung / Regelbasiert                      |
| **5. Text-zu-Sprache (TTS)**  | Text in Sprache umwandeln              | `Coqui TTS`, `Festival`, `eSpeak`, Google TTS, `ResponsiveVoice` |

Neben der Sprachassistenz ben√∂tigt man meiner Meinung nach `Flask` f√ºr die `REST API` und f√ºr eine `Weboberfl√§che` des Sprachassistenzen. Wahrscheinlich konfiguriert man √ºber die Weboberfl√§che Triggers√§tze. Entsprechend wird auch eine Datenbank (`SQLite` mit `SQLAlchemy`-Anbindung) f√ºr Passw√∂rter und Regeln ben√∂tigt.

#### Beispielvergleich

Die Auswahl von verschiedenen Libraries kann ja Vor- und Nachteile hervorheben.

##### Hotword Detection

###### ‚úÖ Vergleich: **Lokale Hotword-Erkennungssysteme**

| **Library / System** | **Sprache / API** | **Lokal** | **Cloudfrei**     | **Status**           | **Besonderheiten**                                      |
| -------------------- | ----------------- | --------- | ----------------- | -------------------- | ------------------------------------------------------- |
| üîπ `Porcupine`       | Python 3 / C      | ‚úÖ         | ‚úÖ\*               | ‚úÖ aktiv              | Sehr effizient, Wakeword-Modelle mit Lizenz generierbar |
| üîπ `Snowboy`         | Python 2 / C++    | ‚úÖ         | ‚ö†Ô∏è (f√ºr Training) | ‚ùå eingestellt (2020) | Exzellente Erkennung, kein Custom Training mehr         |
| üîπ `Mycroft Precise` | Python 3          | ‚úÖ         | ‚úÖ                 | ‚úÖ aktiv (2024 Forks) | Open-Source, trainierbar mit eigenem Datensatz          |
| üî∏ `Picovoice`       | Python 3 / Web    | ‚ö†Ô∏è teils  | ‚ùå                 | ‚ö†Ô∏è deprecated        | Picovoice war Suite, nun nur noch Porcupine aktiv       |

---

###### üß† Klarstellungen:

* **Porcupine** (by Picovoice):

  * Sehr **effizient**, l√§uft sogar auf Mikrocontrollern.
  * Man kann eigene Wakewords **lokal erzeugen**, aber dazu braucht man evtl. die **Picovoice Console** (Web).
  * Die Laufzeit selbst ist **vollst√§ndig offline**.
  * Lizenzmodell: Kostenlos f√ºr Einzelpersonen / nicht-kommerzielle Zwecke.

* **Snowboy**:

  * Fr√ºher sehr popul√§r f√ºr lokale Wakeword-Erkennung.
  * Leider **nicht mehr gepflegt**.
  * Eigene Wakewords waren nur √ºber eine Web-Oberfl√§che trainierbar, die inzwischen offline ist.

* **Mycroft Precise**:

  * **Open Source und lokal trainierbar** mit eigenem Datensatz.
  * Python 3-kompatibel, kann auf Linux / Raspberry Pi laufen.
  * Aktive Community / Forks, auch 2024 noch in Benutzung.
  * Funktioniert gut f√ºr DIY- oder Datenschutzprojekte.

* **Picovoice SDK**:

  * Die **gesamte SDK-Suite (Speech-to-Text etc.)** wurde eingeschr√§nkt, empfohlen wird jetzt direkt **Porcupine**.
  * **Nicht mehr aktiv entwickelt** als Komplettpaket.

---

###### üìå Fazit / Empfehlung:

| Einsatzziel                              | Empfehlung                         |
| ---------------------------------------- | ---------------------------------- |
| Lokaler Assistent mit Custom Hotword     | üîπ **Mycroft Precise**             |
| Minimaler Stromverbrauch (z.‚ÄØB. Pi Zero) | üîπ **Porcupine**                   |
| Forschung / Training eigener Wakewords   | üîπ **Mycroft Precise**             |
| Veraltete Tools vermeiden                | ‚ùå Kein Snowboy, kein Picovoice SDK |

---

Dies ist jetzt nur eine kleine √úbersicht, was man machen kann im Vergleich. Am besten Quellen ranziehen (z. B. Webseite, Doku, usw.). Man vergleicht am Besten `Python 3 / Python 2` (`Python 2` ist ein Ausschlusskriterium). In der auf NodeJS-basierenden Software `MagicMirror` wird f√ºr die Spracherkennung `Snowboy` eingesetzt. Dort ist als `Hotword` bereits `MagicMirror` vortrainiert. Ein neues Hotword kann nicht mehr trainiert werden, weil `Snowboy` deprecated ist und der Service f√ºr das Training meines Wissens sogar eingestellt wurde. Selbstverst√§ndlich schaut man sich an, ob etwas cloudbasiert funktioniert, lokal oder teilweise cloudbasiert ist. Ebenfalls wichtig ist ja, ob etwas kostenlos ist oder nicht. Hin und wieder gibt es ja auch Abomodelle, dass man vielleicht 1000 Requests kostenlos hat oder Centbetr√§ge f√ºr einzelne Requests zahlt, usw. Manche Libraries nutzen `C`, `C++` im Background und es wird nur ein `Python-Wrapper` verwendet. Dies bedeutet, dass das Training performanter und hardwaren√§her ist.

##### ASR/TTS

Es gibt extrem viele Libraries f√ºr `Text-to-Speech`. Die meisten ben√∂tigen eine Cloud. Also wirklich fast alle. Dies ist ein klarer Nachteil. Manche sind immer noch auf Basis von `Python 2`. Einige unterst√ºtzen sowohl `Python 2`, als auch `Python 3`. Manche gehen nat√ºrlich nur mit `Python 3`. Ergo kann man auch hier alles was `Python 3` nutzt als Vorteil ansehen. Das Ergebnis, bzw. die Sprachqualit√§t von cloudbasierten Libraries ist ganz klar besser. Die lokal laufenden Systeme klingen im Deutschen so, als w√ºrde ein Amerikaner Deutsch reden und dann klingt es sehr technisch, mechanisch nach einem Roboter, w√§hrend cloudbasierte Ergebnisse meist einem menschlichen Sprecher sehr nahe kommt. Je nachdem dauert die Umwandlung lange. Bei cloudbasierten L√∂sungen gibt es welche, die sehr schnell gute Ergebnisse zur√ºckliefern. Cloudbasiert und damit viel Rechenleistung bedeutet aber tats√§chlich nicht zwangsl√§ufig, dass es schnell geht, bis die Datei generiert wird. Auch hier gibt es m√∂glicherweise Latzenzen. Lokal laufende L√∂sungen haben zwar keine Latzen zwecks Netzwerkauslastung oder schlechter Internetverbindung, sind aber oft auch langsam, weil die Rechenkapazit√§t geringer ist, als in einer Cloud. Man kann vergleichen, welche Datenformate generiert werden (`wav`, `mp3`, usw.). Ich denke f√ºr die meisten Szenarien ist es egal, welches Audioformat generiert wird. Wenn man aber bspw. Sonos-Lautsprecher verwenden m√∂chte, dann kann es sein, dass nicht jedes Audioformat unterst√ºtz wird. Vor- und Nachteile in der Bewertung kann es sein, ob Sprecherstimmen (verschiedene Frauen- oder M√§nnerstimmen) konfigurierbar sind, ob Sprachen konfigurierbar sind oder ob `Speech Synthesis Markup Language` (`SSML`) unterst√ºtzt wird. `SSML` ist ein ganz klarer Vorteil und gerade im Vergleich zu `Amazon Alexa`, w√§re es vielleicht sogar sehr sinnvoll, da `Alexa` dies unterst√ºtzt. 

Es k√∂nnen auch weitere Kritieren noch f√ºr einen Vergleich herangezogen werden.

Anmerkung: Es gibt verschiedene Libraries, die im Hintergrund aber bspw. denselben Service wie `Google STT` verwenden. Das hei√üt, viele Libraries stellen zu manch einer Library kaum eine echte Alternative dar.

##### NLP

Selbstverst√§ndlich kommt hier auch wieder `Python 2` vs. `Python 3` vor, wenn man die Libraries vergleicht. Da es hier um KI geht, ist auch entscheidend, ob `CPU` oder `GPU` verwendet werden kann. Gemessen an der schwachen Leistung des Endger√§ts (z. B. Raspberry Pi), w√ºrde es hier auch Sinn ergeben, ob man eine `TPU`-Unterst√ºtzung oder andere Coprozessoren verwenden k√∂nnte. Es gibt auch KIs, die √ºber eine Cloud trainiert werden. Bei manchen KIs, in einer Cloud l√§dt man Daten hoch, trainiert sie, l√§dt Daten herunter und kann die trainierten Daten zu seiner Anwendung hinzuf√ºgen. Dies w√§re dann zumindest bedingt offline und lokalf√§hig, weil man nur beim Training eine Internetverbindung zwingend ben√∂tigt hat. Es gibt KIs, die sind leichtgewichtig und besonders gut f√ºr `Raspberry Pi's` geeignet.

##### Antwortgenerierung

Ich denke dies kann man mehrschichtig aufbauen. Als erstes nutzt man z. B. die regelbasierten Antworten, wenn dort keine Regel auftaucht k√∂nnen Websuche oder KIs angebunden sein, usw. Hier kann man auch Vor- und Nachteile diskutieren. Eine eigene KI zu trainieren edeutet, dass man sehr vielseitig trainieren m√ºsste und immens viele Daten br√§uchte. Ein `GPT` (`Generative Pre-trained Transformer`) bedeutet ja in der Regel immer eine Cloud-L√∂sung (es gibt auch Docker-Container, die man lokal laufen lassen und trainieren kann).

### Speech-to-Text

Nicht selten sind es auch dieselben Cloud-Dienstleister, wie bei `Text-to-Speech`. Ich behaupte, es sind auch sehr vegleichbare und √§hnliche Kriterien, sowie dieselben Vor- und Nachteile.

---

### üß† **3. KI-Modelle: Wof√ºr und wie trainieren?**

#### Du brauchst KI f√ºr:

* **Hotword Detection** (Custom Wakewords)
* **Intent-Erkennung** (z.‚ÄØB. ‚ÄúWie wird das Wetter morgen?‚Äù)
* (Optional) **Dialog-Management** (Kontext verstehen)
  * Man kann dies Regelbasiert machen. Empfehlenswert w√§re eine Oberfl√§che, bei der ich Triggers√§tze hinzuf√ºge. Sogenannte `WENN-DANN`-Regel sind im Smart Home Gang und G√§be. Man w√§hlt bspw. `WENN` aus und schreibt einen Triggersatz, klickt dann auf `DANN` und w√§hlt bspw. `openHAB Command` und sendet einen `Command` an ein openHAB Item oder man w√§hlt bspw. `openHAB State` und macht eine Statusabfrage von einem openHAB Item.
  * Grunds√§tzlich kann man es so aufbauen, dass sobald ein Kontext nicht verstanden wird, eine KI versucht zu verstehen, was gemeint ist oder eine Websuche machen, sobald etwas erkannt wird, was in keiner Regel hinterlegt ist.
  * Man kann auch im Regelsystem direkt integrieren, dass z. B. Termineerinnerungen gesetzt werden sollen, das Web durchsucht werden soll, usw. Man kann ja auch absichtlich Triggers√§tze bauen, die nicht zwangsl√§ufig mit Smart Home zu tun haben m√ºssen.

##### Modelle:

* Hotword-Erkennung ‚Üí kleine CNNs oder vortrainierte Modelle (Snowboy, Porcupine).
* Intent-Erkennung ‚Üí NLP-Modelle wie BERT oder einfache Klassifikatoren (z.‚ÄØB. `scikit-learn`, `spaCy`, `Rasa`).
* F√ºr Antwortgenerierung kannst du GPT lokal oder per API (OpenAI, HuggingFace) einbinden.

##### Training:

* Hotword-Erkennung: ca. 20‚Äì50 Audiobeispiele pro Wakeword.
* Intent-Erkennung: Beispiel-Intents in YAML/JSON, evtl. Feintuning mit `Rasa` oder `Transformers`.

> üí° F√ºr viele Anwendungsf√§lle ist kein eigenes Training notwendig, da es gute **pretrained Modelle** gibt.

---

### üíª **4. Entwicklungsumgebung**

#### üî§ **Programmiersprache:**

* **Python**: Klare Empfehlung, da fast alle relevanten Frameworks hier existieren.
* **Alternative** (falls Performance kritisch ist): Rust oder C++ (z.‚ÄØB. f√ºr Low-Level Audio-Handling).

#### üß∞ **Empfohlene Frameworks**:

* `speech_recognition` (Wrapper f√ºr verschiedene STT-Systeme)
* `Rasa` (NLU/Dialogmanagement)
* `transformers` von HuggingFace (f√ºr moderne NLP)
* `PyTorch` oder `TensorFlow` (f√ºr eigene Modelle)
* `Flask` oder `FastAPI`
  * Ich empfehle `Flask`, da man hier nicht nur eine `REST API` bauen kann, sondern auch eine Weboberfl√§che. √úber die Weboberfl√§che kann man ja bspw. Triggers√§tze konfigurieren. 
* `pyaudio` oder `sounddevice` (f√ºr Mikrofonzugriff)
* `python-openhab-rest-client` (f√ºr openHAB)
* `SQLAlchemy` (f√ºr Datenbanken)

Es wird noch eine genauere Planung ben√∂tigt, wie ein solches System funktonieren soll.

---

### üß± **5. Architektur√ºbersicht (Ablauf)**

```plaintext
[Mikrofon] ‚Üí [Hotword Detection] ‚Üí [Spracherkennung (ASR)] ‚Üí [Intent-Erkennung (NLU)]
            ‚Üí [Antwort (Aktion/Dialog)] ‚Üí [Text-To-Speech] ‚Üí [Lautsprecher]
```

Was hier in der Architektur nicht drinnen ist, ist was mit den Intents passiert. Mit diesen kann ich ja zum Beispiel openHAB steuern oder Zust√§nde aus openHAB abfragen. Ich k√∂nnte ja auch andere Systeme anbinden. Au√üerdem muss ja gar keine Antwort verbal erfolgen. Bei einer `WENN-DANN`-Regel h√§tte ich hier, dass auf eine spezielle Spracheingabe eine spezielle Sprachausgabe erfolgen w√ºrde. Ich kann ja auch etwas anderes machen und erhalte dann eine Sprachausgabe oder ich habe eine Spracheingabe ohne Sprachausgabe und mache damit irgendetwas v√∂llig anderes (z. B. Ger√§te bedienen).

Es fehlt zum Beispiel, ob es eine Benutzeroberfl√§che gibt, mit der man `WENN-DANN`-Regel konfiguriert. Es fehlt eine Architekturbetrachtung, ob Datenbanken oder andere Systeme angebunden sind. Es fehlt eine Architekturbetrachtung, wie ein KI-Model angebunden ist. Wie wird was √ºberhaupt trainiert?

Dies bedarf noch einer genaueren Planung.

#### Beispiel-Stack (lokal lauff√§hig):

* **Hotword**: Porcupine (Wakeword: ‚ÄûHey Kiro‚Äú)
* **ASR**: Vosk (lokal) oder Whisper
* **NLU**: Rasa (mit YAML-Intents)
* **TTS**: Coqui TTS
* **Logik**: Python-Controller, der Aktionen oder Antworten triggert

---

### üß™ **6. Zus√§tzliche Tipps**

* **Modular entwickeln**: Trenne jeden Schritt in eigene Komponenten (Hotword, STT, NLU, TTS).
* **Offline-f√§higkeit pr√ºfen**: Whisper, Vosk, Rasa, Coqui k√∂nnen lokal laufen.
* **Sicherheit**: Wenn du eine Cloud-L√∂sung einsetzt (z.‚ÄØB. OpenAI API), achte auf Datenschutz.
* **Trigger/Custom Intents**: Rasa erlaubt einfache Erweiterung mit neuen ‚ÄûStories‚Äú oder ‚ÄûIntents‚Äú ohne tiefes KI-Training.

Anmerkung: Cloud- oder Web-Anbindungen k√∂nnen ja das System jederzeit erweitern. In Abgrenzung zu g√§ngigen Sprachassistenzen l√§uft dieser ja trotzdem lokal. Sollte es Internetprobleme geben, dann verzichtet man nur auf einen Teil der Funktionalit√§ten, aber die Basisfunktionen gehen noch, w√§hrend bei einem typischen Sprachassistent ohne Internetverbindung nichts mehr funktionieren w√ºrde.

---

### üöÄ Einstiegsvorschlag (f√ºr Prototypen)

1. **Raspberry Pi 5 + USB-Mikrofon**
2. Hotword: `Porcupine` (kostenloser Wakeword-Engine)
3. STT: `Whisper` oder `Vosk`
4. NLU: `Rasa` (mit einfachen Intents)
5. TTS: `Coqui TTS` oder Google TTS API
6. Alles orchestriert mit `Python`

---

## Hintergrundwissen

### Unterschied Chatbot vs. Sprachassistent

Prinzipiell gilt:

```
Chateingabe ‚âô Spracheingabe (Speech-to-Text)
Chatausgabe ‚âô Sprachausgabe (Text-to-Speech)
```

Entspricht der Aufbau von einem Chatbot wirklich dem Aufbau eines Sprachassistenten? Nicht wirklich. Sie funktionieren sehr √§hnlich. Ein Gro√üteil der Architektur kann gleich aufgebaut werden. Man kann auch `Fuzzy Matching` verwenden. Es wird `NLP` in beiden Systemen angewandt. Auch ein Sprachassistent ben√∂tigt dringend ein `Intent`. Am Beispiel `Fuzzy Matching` wird deutlich, dass dies in einem Chatbot sehr viel notwendiger sein wird, als in einem Sprachassistenten. Tippfehler kann man bspw. mit `Fuzzy Matching` korrigieren, nicht geh√∂rte oder verrauschte Worte, k√∂nnen damit jedoch nicht korrigiert werden. Bei einem Chatbot kann es mal entstehen, dass einzelne Buchstaben in einem Wort fehlen oder verdreht sind, bei einer Spracherkennung hingegen wird ein Wort entweder erkannt oder nicht erkannt.

### Fuzzy Matching

Das **Fuzzy Matching** kann man im deutschen als unscharfe Suche betiteln. Es ist eine Klasse von String-Matching-Algorithmen, mit der man bestimmte Zeichenketten (Strings) in einer l√§ngeren Zeichenkette oder einem Text suchen bzw. finden k√∂nnen sollte.  Es wird daher auch oft als Fuzzy-Suche oder Fuzzy-String-Suche betitelt.

Aus:

[https://www.klippa.com/de/blog/informativ/fuzzy-matching-de/](https://www.klippa.com/de/blog/informativ/fuzzy-matching-de/)

Mit Fuzzy-Matching kann ich darauf reagieren, wenn z. B. nur 80% eines Wortes/Satzes richtig erkannt wurde. Hier mal Beispiele.

```
Gesucht: Bestellnummer

Richtig w√§ren:
Bestellnr.
Bestelnummer
Bestellnumer
Bestellnummmer
...
```

Also m√∂gliche Tippfehler oder auch Abk√ºrzungen sollen ja darufhin deuten, dass ein- und dasselbe Worte gemeint ist. In Zusammenhang mit Synonymen w√ºrde man durch Fuzzy Matching dem Benutzer so extrem viele Eingabem√∂glichkeiten erm√∂glichen. Man formuliert ja nicht nur S√§tze um, man vertippt sich auch mal oder k√ºrzt einzelne W√∂rter ab.

**Fuzzy Matching** nimmt also Korrekturen vor. Dies k√∂nnten im allgemeinen die nachfolgenden sein:

* **Einf√ºgen** ‚Äì Hinzuf√ºgen eines Buchstabens zur Vervollst√§ndigung des Wortes (z. B. `Rechnun` wird zu `Rechnung`)
* **L√∂schen** ‚Äì Entfernen eines Buchstabens aus einem Wort (z. B. `Rechnnung` wird zu `Rechnung`)
* **Substitution** ‚Äì Vertauschen eines Buchstabens, um ein Wort zu korrigieren (z. B. `Technung` wird zu `Rechnung`)
* **Transposition** ‚Äì Vertauschen von Buchstaben, um ein Wort zu korrigieren (z. B. `Rehcnung` wird zu `Rechnung`)

Jeder Korrektur, die durchgef√ºhrt werden muss, wird eine ‚ÄûBearbeitungsdistanz‚Äú von 1 zugeschrieben. Die Bearbeitungsdistanzen beeinflussen die oben erw√§hnte Trefferquote. Wenn Sie beispielsweise eine Zeichenfolge mit 11 Zeichen haben und 2 Korrekturen vornehmen m√ºssen, betr√§gt die endg√ºltige Trefferquote **81,81 %**.

```
Berechnung: 100%- 2 / 11= 81.81%  
```

Neben diesen Korrekturen kann **Fuzzy Matching** auch verwendet werden, um Zeichensetzungen, zus√§tzliche W√∂rter und fehlende Leerzeichen in Zeichenketten oder Texten zu korrigieren.

---

#### Fuzzy-Matching-Algorithmen

Fuzzy Matching f√§llt in die Kategorie der Methoden, f√ºr die es keinen spezifischen Algorithmus gibt, der alle Szenarien und Anwendungsf√§lle abdeckt. Daher werden wir einige der am h√§ufigsten verwendeten und zuverl√§ssigsten Fuzzy-Matching-Algorithmen f√ºr die Suche nach ungef√§hren Daten√ºbereinstimmungen behandeln:

* Levenshtein-Distanz (LD)
* Hamming-Distanz (HD)
* Damerau-Levenshtein

---

##### Levenshtein-Distanz

Die **Levenshtein-Distanz (LD)** ist eine Fuzzy-Matching-Technik, die zwei Zeichenfolgen beim Vergleich und der Suche nach einer √úbereinstimmung ber√ºcksichtigt. Je h√∂her der Wert der Levenshtein-Distanz ist, desto weiter sind die beiden Zeichenfolgen oder ‚ÄûBegriffe‚Äú von einer identischen √úbereinstimmung entfernt.

Wie erhalten wir nun den Wert der Levenshtein-Distanz? Die LD zwischen den beiden Zeichenfolgen entspricht der Anzahl der √Ñnderungen, die erforderlich sind, um eine Zeichenfolge in die andere umzuwandeln. F√ºr die LD gelten das Einf√ºgen, L√∂schen und Ersetzen eines einzelnen Zeichens als Bearbeitungsoperationen.

Nehmen wir an, Sie m√∂chten die LD zwischen ‚ÄûRechnungsnummer‚Äú und ‚ÄûRechnungs-Nr.‚Äú messen. Der Abstand zwischen den beiden Begriffen ist ‚Äû1 x u‚Äú, ‚Äû2 x m‚Äú und ‚Äû1 x e‚Äú, was einem Abstand von 4 entsprechen w√ºrde. Warum? Weil Sie diese Zeichen hinzuf√ºgen m√ºssten, um eine √úbereinstimmung zu erreichen. Siehe die Beispiele unten.

---

###### Levenshtein-Abstand Beispiel

> **Rechnungnummer** ‚Üí Rechnung**s**nummer (Einf√ºgung von ‚Äû**s**‚Äú) ‚Äì Abstand: 1  
> **Rechnung numr** ‚Üí Rechnungsnu**m**m**e**r (Einf√ºgung von ‚Äû**m**‚Äú & ‚Äû**e**‚Äú) ‚Äì Abstand: 2  
> **Rechnung nr** ‚Üí Rechnungsn**u****m****m****e**r (Einf√ºgung von ‚Äû**u, m, m, e**‚Äú) ‚Äì Abstand: 4

---

##### Hamming-Distanz

Die **Hamming-Distanz (HD)** unterscheidet sich nicht allzu sehr von der Levenshtein-Distanz. Die Hamming-Distanz wird h√§ufig verwendet, um den Abstand zwischen zwei gleich langen Textabschnitten zu berechnen.

Die HD-Methode basiert auf der **ASCII**-Tabelle (American Standard Code for Information Interchange). Zur Berechnung des Abstandswertes verwendet der Hamming-Distanz-Algorithmus die Tabelle, um den Bin√§rcode zu bestimmen, der jedem Buchstaben in den Zeichenketten zugeordnet ist.

---

###### Hamming-Abstand-Beispiel

Nehmen wir die folgenden Textzeichenfolgen ‚ÄûNumber‚Äú und ‚ÄûLumber‚Äú als Beispiel. Wenn wir versuchen, den HD zwischen den Zeichenfolgen zu bestimmen, ist der Abstand nicht 1, wie es mit dem Levenshtein-Algorithmus der Fall w√§re. Stattdessen w√ºrde er 10 betragen. Das liegt daran, dass die ASCII-Tabelle einen Bin√§rcode von **(1001110)** f√ºr den Buchstaben **N** und **(1001100)** f√ºr den Buchstaben **L** anzeigt.

Beispielrechnung:

> **D** = N ‚Äì L = 1001110 ‚Äì 1001100 = **10**

---

##### Damerau-Levenshtein

Das Damerau-Levenshtein-Verfahren misst auch den Abstand zwischen zwei W√∂rtern, indem es die erforderlichen √Ñnderungen misst, die vorgenommen werden m√ºssen, um ein Wort an das andere anzupassen. Diese √Ñnderungen h√§ngen von der Anzahl der Operationen ab, wie z. B. Einf√ºgung, L√∂schung oder Ersetzung eines einzelnen Zeichens oder Transposition zweier benachbarter Zeichen.

Hier unterscheidet sich die Damerau-Levenshtein-Distanz von der regul√§ren Levenshtein-Distanz, da sie zus√§tzlich zu den Einzelzeichen-Editieroperationen, auch Transpositionen ber√ºcksichtigt, um eine ungef√§hre √úbereinstimmung zu finden (Fuzzy Match).

---

###### Damerau-Levenshtein Beispiel

> **Zeichenfolge 1:** Re<strong>ch</strong>nun<strong>g</strong>  
> **Zeichenfolge 2:** Re<strong>hc</strong>nun  
>   
> **Operation 1:** Transposition ‚Üí Vertauschen der Zeichen ‚Äû**h**‚Äú und ‚Äû**c**‚Äú  
> **Operation 2:** Einf√ºgen eines ‚Äû**g**‚Äú am Ende der Zeichenfolge 2


Da zwei Operationen erforderlich waren, um die beiden W√∂rter identisch zu gestalten, **betr√§gt der Abstand 2**. Vereinfacht ausgedr√ºckt z√§hlt jede Operation wie Einf√ºgung, L√∂schung, Transposition usw. als ein Abstand von ‚Äû1‚Äú. Mit der Levenshtein-Distanz m√ºssten Sie jedoch drei Korrekturen vornehmen, was einem Abstand von 3 entspricht.

Alle oben genannten Fuzzy-Matching-Algorithmen unterscheiden sich nat√ºrlich in der Art und Weise, wie die Bearbeitungsdistanz berechnet wird. Dies ist der Grund, warum es keinen FM-Algorithmus gibt, der f√ºr alle geeignet ist. Von den drei vorgestellten Algorithmen ist die Levenshtein-Distanz jedoch der am h√§ufigsten verwendete FM-Algorithmus in der Datenverwaltung und Datenwissenschaft.

Empfehlung: In Python kann man die [fuzzywuzzy-Bibliothek](https://github.com/seatgeek/fuzzywuzzy) testen oder einen eigenen Algorithmus implementieren.

---

#### üß† **Wird Fuzzy Matching f√ºr einen Sprachassistenten ben√∂tigt?**

##### ‚úÖ **Kurzantwort:**

**Ja, in bestimmten Komponenten eines Sprachassistenten kann Fuzzy Matching n√ºtzlich sein ‚Äì aber es ist nicht zwingend erforderlich** und wird **nicht immer eingesetzt**, insbesondere dann nicht, wenn du moderne NLU- oder KI-Modelle verwendest.

---

#### üìå **Wann ist Fuzzy Matching sinnvoll?**

Fuzzy Matching kommt dann zum Einsatz, wenn:

* **Benutzereingaben ungenau, variabel oder fehlerhaft** sind (z.‚ÄØB. Tippfehler, unterschiedliche Formulierungen).
* **Keine trainierten oder semantischen Modelle verf√ºgbar** sind, etwa bei regelbasierten Assistenten.
* Du **einfach strukturierte Sprachbefehle mit statischen Antworten** hast.

##### Typische Einsatzszenarien:

| Anwendung                                                   | Fuzzy Matching notwendig? | Alternative bei moderner Architektur          |
| ----------------------------------------------------------- | ------------------------- | --------------------------------------------- |
| Men√ºsysteme / Chatbots mit festen Befehlen                  | ‚úÖ Ja                      | Regelbasierte Intenterkennung                 |
| Sprachassistent mit NLU-Modell (z.‚ÄØB. Rasa)                 | ‚ùå Eher nein               | Verwendung von ML-basierten Parsers           |
| Wakeword-Erkennung                                          | ‚ùå Nein                    | Hier nutzt man z.‚ÄØB. CNNs, kein Text-Matching |
| Dateinamen oder Orte erkennen (‚ÄûSpiel *Bohemian Rhapsody*‚Äú) | ‚úÖ Ja (optional)           | Levenshtein, Jaro-Winkler, etc.               |

---

#### üîÑ **Beispiel: Fuzzy Matching vs. NLU**

##### Ohne NLU:

```python
if "spiel musik" in user_input:
    do_music()
elif "spiel musick" in user_input:  # Tippfehler!
    do_music()  # geht nicht ohne Fuzzy Matching
```

##### Mit Fuzzy Matching:

```python
from fuzzywuzzy import fuzz
if fuzz.partial_ratio("spiel musik", user_input) > 80:
    do_music()
```

##### Mit NLU (z.‚ÄØB. Rasa):

```yaml
- intent: play_music
  examples: |
    - spiel musik
    - spiel bitte musik
    - spiele ein lied
    - mach musik an
```

‚Üí NLU erkennt die Absicht, auch wenn der Text variiert oder leicht fehlerhaft ist. Kein Fuzzy Matching n√∂tig.

---

#### üß© **Fazit**

| Wenn du ...                            | Dann ...                                      |
| -------------------------------------- | --------------------------------------------- |
| ... nur einfache Textvergleiche machst | Fuzzy Matching kann helfen                    |
| ... moderne NLU-Modelle nutzt          | Fuzzy Matching ist **nicht n√∂tig**            |
| ... Intents regelbasiert zuweist       | Fuzzy Matching kann Fehler abfedern           |
| ... mit Sprache (nicht Text) arbeitest | ASR-Fehler k√∂nnen durch NLU abgefangen werden |

---

#### üí° Empfehlung f√ºr deine Bachelorarbeit:

* **Nutze Fuzzy Matching nur bei Bedarf**, z.‚ÄØB. f√ºr regelbasierte Systeme oder spezifische Trigger (z.‚ÄØB. Namen, Ortslisten).
* Bei **modernem Aufbau mit Rasa, transformers oder GPT-basierten Komponenten**: Verzichte darauf und verlasse dich auf semantische Modelle.

Ich denke in einer gut ausgearbeiteten Abschlussarbeit erl√§utert man diesen Verzicht auch, weil es sehr viel dar√ºber sagt, was in der Theorie ein System alles k√∂nnen muss.

### Intent, Entity, Confidence Score

Aus:

[https://www.melibo.de/blog/was-sind-intent-und-entity](https://www.melibo.de/blog/was-sind-intent-und-entity)

---

#### Intent

Intents, zu Deutsch ‚ÄûAbsichten‚Äú, sind Zwecke oder Ziele, die in den Eingaben eines Kunden zum Ausdruck kommen, um z.B. eine Frage zu einer Retoure zu stellen. Durch die Erkennung der Absicht, die sich in der Kundeneingabe ausdr√ºckt, versucht der KI-Chatbot den richtigen Dialog zu finden und die passende Ausgabe zu w√§hlen. Daf√ºr nutzen KI-Chatbots maschinelles Lernen, um in nat√ºrlicher Sprache die vorher definierte Absicht (Intent) zu erkennen. [1] Einfach gesagt, Intents sind Fragen der User:innen, die dem Chatbot zu einem speziellen Thema gestellt werden und der Versuch des KI-Chatbots, die passende Antwort zu erkennen, um das Problem zu l√∂sen bzw. die Frage zu beantworten.

---

##### Wie funktionieren Intents?

Bestehende Anbieter wie unter anderem der IBM Watson Assistant [1], Rasa [2] oder Microsoft LUIS [3] basieren alle meist auf dem Prinzip der Intent-Ausgabe. Bevor der Chatbot Intents erkennen kann, m√ºssen erst mal alle Absichten der User:innen definiert werden. Hierf√ºr ist es wichtig, dass man seine Kundenanfragen erst mal identifiziert und seinen Use-Case richtig versteht. Nachdem der Intent-Katalog erstellt und der Bot online genommen wurde, werden die User:innen dem Chatbot Fragen stellen. Jede Anfrage der User:innen durchl√§uft das sogenannte Intent-Matching, also der Zuordnung der Anfrage aus den gesamten Inhalten des Chatbots. Dabei wird anhand von NLP (Natural Language Processing) ein Confidence-Score berechnet, um anhand von Wahrscheinlichkeiten die passende Antwort auszugeben.

---

#### Confidence-Score

Ein kurzer Exkurs zum Thema Confidence-Scores. Die Confidence-Scores liegen zwischen 0 und 1 und geben an, zu wie viel Prozent der Chatbot ein Intent erkannt hat. Zu jeder gestellten Frage der User:innen berechnet der Chatbot also einen Confidence-Score und versucht auf dieser Grundlage, durch Wahrscheinlichkeiten, die richtige Antwort an die User:innen auszugeben. Die Confidence-Scores sind meistens voreingestellt und liegen zwischen 0,6 und 0,7. Das hei√üt, dass der Chatbot Antworten nur dann ausgibt, wenn die Erkennungswahrscheinlichkeit bei mindestens 60 % liegt. Nehmen wir nun als Beispiel an, dass User:innen die Frage stellen ‚ÄûWas kannst du so?‚Äú, um zu erfahren, welche Themen der Chatbot √ºberhaupt beantworten kann. Der KI-Chatbot erkennt zu 89 % Prozent den Intent ‚ÄûWas kannst du?‚Äú. In diesem Beispiel gibt der Chatbot die passende Antwort aus und beantwortet somit die Frage.

---

##### Bestandteile eines Intents

Ein Intent besteht also aus dem: Intentnamen, dem User-Input, dem Confidence-Score und einer Antwort. Abh√§ngig von der genutzten Technologie k√∂nnen noch weitere Bestandteile dazu kommen, wie Variablen, Actions und Entities.

Ein Beispiel f√ºr Intents

(1) User-Input: Frage eines Users

üôé‚Äç‚ôÇÔ∏è: ‚ÄûVorteile eines Chatbots‚Äú
üôé‚Äç‚ôÇÔ∏è: ‚ÄûWas k√∂nnen Chatbots besonders gut?‚Äú
üôé‚Äç‚ôÇÔ∏è: ‚ÄûWarum sollte ich einen Chatbot holen?‚Äú

(2) Confidence-Score und (3) Intentnamen ‚ÄûVorteile Chatbot‚Äú: Berechnung der Wahrscheinlichkeit anhand vom User-Input

üßÆ : 100 % Erkennung des Intents ‚ÄûVorteile Chatbot‚Äú

(4) Antwort des Chatbots auf die Frage ‚ÄûVorteile Chatbots‚Äú

ü§ñ : Zu den Vorteilen von Chatbots geh√∂ren unter anderem und abh√§ngig von der Branche: Automatisierung von Prozessen, wodurch Fehler beim Support reduziert sowie Zeit und Geld eingespart werden k√∂nnen. Verk√ºrzte Wartezeiten f√ºr den Kunden. 24/7 Kundensupport. Effizientere Strukturen. Weniger manueller Aufwand f√ºr dich.

---

#### Entity

Im Unterschied zu Intents dienen Entitys oder auch Entities dazu, Informationen der User:innen aus der nat√ºrlichen Sprache zu extrahieren. Jedes Entity verf√ºgt √ºber eine Reihe von Eigenschaften, die mit ihr verbunden sind. Dabei kannst du auf Informationen deines Entities zugreifen. Wie bei einem Intent gibt der Chatbot an, wie hoch der Confidence-Score liegt. Im Unterschied zu Intents liegt der Confidence-Score, aber bei 0 oder 1. Unabh√§ngig davon, ob und wie die Erkennung des Entitys eingestellt ist, haben sogenannte System-Entites immer eine Erkennung von 1. Jedes Entity besitzt einen Wert, einen sogenannten Entit√§tswert. Bei der Erstellung von Entities ist es notwendig, dass neben dem Wert auch Typen definiert werden. Unter Typen versteht man allgemein Synonyme, also W√∂rter, die sich ebenfalls auf dasselbe Vorhaben beziehen, es nur anders umschreiben. Je mehr Synonyme ein Entity hat, desto besser die Erkennung des Chatbots [4].

Grunds√§tzlich unterscheiden wir zwischen System-Entities und Customize-Entities. Die System-Entites sind voreingestellt, das hei√üt im System bereits enthalten. Darunter fallen etwa Zahlen, Uhrzeiten oder Adressen. Diese Entities sind besonders beliebt und wurden in der Vergangenheit besonders h√§ufig verwendet. Die Customize-Entities dagegen sind selbst definierte Werte, die auf den jeweiligen Use-Case angepasst werden.

---

##### Ein Beispiel f√ºr Entities in der Praxis

üôé‚Äç‚ôÇÔ∏è: ‚ÄûWann kommt mein Produkt Chatbot-Experte in der Landstra√üe 5 an?‚Äú

---

##### Entities in diesem Beispiel:

Produkt Chatbot Experte (Customize-Entity product_type)
Landstra√üe 5 (Sytem-Entity street_adress)

---

##### Vorteile von Entites:

Mit Entities kann man User:innen durch Chat Flows in Form von Buttons navigieren, schnell Synonyme definieren und mehrere Anfragen auf einmal verarbeiten. Die System-Entitys helfen au√üerdem dabei, die beliebten Anfragen zum Thema Ort, Zeit und Adresse abzufangen.

[1]: https://cloud.ibm.com/docs/assistant?topic=assistant-intents
[2]: https://rasa.com/open-source/
[3]: https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-utterance
[4]: https://cloud.ibm.com/docs/assistant?topic=assistant-expression-language

---

#### Speech Synthesis Markup Language (SSML)

**SSML (Speech Synthesis Markup Language)** ist eine XML-basierte Auszeichnungssprache, mit der du **Text-to-Speech (TTS)**-Ausgaben pr√§zise steuern kannst ‚Äì also wie ein Sprachsynthesizer den gesprochenen Text betonen, pausieren oder variieren soll.

---

##### üß† Was kann man mit SSML machen?

Mit SSML kannst du steuern:

* üó£ **Aussprache** (`<phoneme>`)
* ‚è∏ **Pausen** (`<break>`)
* üéµ **Tonh√∂he, Lautst√§rke, Sprechgeschwindigkeit** (`<prosody>`)
* üì¢ **Sprachstil / Emphasis** (`<emphasis>`, `<voice>`)
* üåç **Sprache und Stimme** (`<lang>`, `<voice>`)
* üîÅ **Audioeinspielung** (`<audio>` ‚Äì je nach Engine)

---

##### ‚úÖ Einfaches Beispiel

```xml
<speak>
  Hallo! <break time="500ms"/> Wie kann ich dir helfen?
</speak>
```

‚è± F√ºgt eine **halbe Sekunde Pause** ein.

---

##### üìå H√§ufig genutzte SSML-Tags (Auswahl)

| Tag          | Funktion                                | Beispiel                                              |
| ------------ | --------------------------------------- | ----------------------------------------------------- |
| `<speak>`    | Wurzel jeder SSML-Ausgabe               | `<speak>Text hier</speak>`                            |
| `<break>`    | F√ºgt Pause ein                          | `<break time="300ms"/>`                               |
| `<prosody>`  | Steuert Tonh√∂he, Lautst√§rke, Tempo      | `<prosody pitch="+10%" rate="slow">...</prosody>`     |
| `<emphasis>` | Betonung auf Wort/Satz                  | `<emphasis level="strong">wichtig</emphasis>`         |
| `<phoneme>`  | Definiert Lautschrift                   | `<phoneme alphabet="ipa" ph="Ààh√¶lo ä">Hallo</phoneme>` |
| `<lang>`     | Schaltet auf andere Sprache             | `<lang xml:lang="en-US">Hello!</lang>`                |
| `<audio>`    | Spielt Audiodatei ab (nur in Cloud-TTS) | `<audio src="sound.mp3"/>`                            |

Anmerkung: Gute Entwickler k√∂nnen auch Mechanismen entwickeln, wie man solche SSML-Tags selbst in Libraries umsetzt, die dies nicht von sich aus unterst√ºtzen. S√§tze m√ºssen dann dann unter Umst√§nden aufgeteilt werden, mehrere Audiodateien erstellt und am Ende zusammengef√ºgt werden. Dies w√§re aber durchaus ein extrem komplexes und durchaus nicht sicheres und instabiles Prozedere.

---

##### üéô Beispiel: Mehrsprachiger Text mit Betonung

```xml
<speak>
  Willkommen bei deinem Sprachassistenten. <break time="300ms"/>
  <emphasis level="strong">Heute</emphasis> ist ein sch√∂ner Tag.
  <lang xml:lang="en-US">The weather is sunny and warm.</lang>
</speak>
```

---

##### ‚öôÔ∏è Kompatibilit√§t

| TTS-Engine            | SSML-Unterst√ºtzung | Hinweis                               |
| --------------------- | ------------------ | ------------------------------------- |
| **Google TTS**        | ‚úÖ Vollst√§ndig      | auch `<audio>`-Einbindung m√∂glich     |
| **Amazon Polly**      | ‚úÖ Sehr gut         | unterst√ºtzt viele `<voice>`-Stile     |
| **Coqui TTS**         | ‚ö†Ô∏è Teilweise       | `<break>`, `<prosody>` oft verf√ºgbar  |
| **eSpeak / Festival** | ‚ùå Kaum             | meist keine native SSML-Unterst√ºtzung |

---

##### üõ† Tipps f√ºr die Praxis

* SSML hilft, die **Nat√ºrlichkeit und Verst√§ndlichkeit** von Sprachausgaben zu steigern.
* Nutze es gezielt bei langen Dialogen, Aufz√§hlungen oder multilingualem Content.
* F√ºr lokale Assistenten mit z.‚ÄØB. `pyttsx3` ist SSML oft **nicht direkt nutzbar** ‚Äì dort musst du ggf. Pausen und Tonh√∂he manuell steuern.

---

Ja, **es gibt mehrere Text-to-Speech-L√∂sungen, die vollst√§ndig lokal arbeiten**, also **ohne Cloudverbindung** funktionieren. Das ist besonders wichtig f√ºr Datenschutz, Offlinef√§higkeit und Embedding in Systeme wie deinen Sprachassistenten.

---

##### üó£Ô∏è **Lokale TTS-Libraries ‚Äì ohne Cloud**

Hier ist eine √úbersicht:

| Library / Tool | Sprache | Lokal | Qualit√§t | SSML-Unterst√ºtzung     | Bemerkung                              |
| -------------- | ------- | ----- | -------- | ---------------------- | -------------------------------------- |
| **Coqui TTS**  | Python  | ‚úÖ     | ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ    | Teilweise (SSML-light) | Moderne TTS, sehr anpassbar            |
| **eSpeak NG**  | C/C++   | ‚úÖ     | ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ    | ‚ùå                      | Extrem leichtgewichtig, viele Sprachen |
| **Festival**   | C++     | ‚úÖ     | ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ    | ‚ùå                      | Veraltet, aber stabil                  |
| **RHVoice**    | C++     | ‚úÖ     | ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ    | ‚ùå                      | Gute Qualit√§t, Open Source             |
| **pyttsx3**    | Python  | ‚úÖ     | ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ    | ‚ùå                      | Wrapper um native Engines              |
| **MaryTTS**    | Java    | ‚úÖ     | ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ    | ‚úÖ                      | Alte, aber robuste Plattform mit SSML  |
| **OpenTTS**    | Docker  | ‚úÖ     | ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ    | Ja (je nach Backend)   | Aggregiert Coqui, MaryTTS, etc.        |

---

##### üîç Kurze Details zu den wichtigsten:

###### üî∏ **Coqui TTS**

* Modernes, Deep-Learning-basiertes System (Fork von Mozilla TTS)
* Offline trainierbar oder fertige Modelle nutzbar
* CLI & Python API
* Stimmen f√ºr viele Sprachen, inkl. Deutsch
* Eingeschr√§nkte SSML-√§hnliche Features (`<break>`, `rate`, `pitch` per CLI/Args)

üëâ Empfohlen f√ºr dein Projekt, wenn du **Qualit√§t + Lokalit√§t** brauchst

---

###### üî∏ **eSpeak NG**

* Extrem leicht (\~2MB)
* Unterst√ºtzt 100+ Sprachen, aber Robot-√§hnliche Stimme
* Ideal f√ºr embedded / low-resource-Systeme

üëâ F√ºr Prototypen oder Systeme mit **minimaler Leistung**

---

###### üî∏ **pyttsx3**

* Python-Wrapper um native TTS-Engines (Windows: SAPI5, macOS: NSSpeechSynthesizer, Linux: espeak)
* Einfach zu benutzen, aber begrenzte Kontrolle √ºber Stimme und Stil
* Keine SSML-Unterst√ºtzung, nur rudiment√§res Timing

üëâ Gut f√ºr **schnelle Demos** oder klassische Desktop-Sprachsynthese

---

###### üî∏ **MaryTTS**

* Java-basiert, mit Webserver & REST-API
* Unterst√ºtzt echtes SSML
* Deutsche Stimme enthalten
* L√§ngere Startzeit, komplexer Aufbau

üëâ F√ºr **akademische oder stabile serverseitige Anwendungen**

---

##### ‚úÖ Empfehlung f√ºr Bachelorarbeit

| Ziel                                     | Empfehlung                         |
| ---------------------------------------- | ---------------------------------- |
| Moderne TTS, lokale Nutzung, erweiterbar | **Coqui TTS**                      |
| Einfache Integration, geringe Ressourcen | **pyttsx3** oder **eSpeak NG**     |
| Vollst√§ndige SSML-Verarbeitung lokal     | **MaryTTS** oder Coqui (teilweise) |

---

## üìë **Gliederungsvorschlag f√ºr die Bachelorarbeit**

Hier ist ein **Vorschlag f√ºr die Gliederung deiner Bachelorarbeit**, passend zum Thema *‚ÄûEntwicklung eines lokalen Sprachassistenten‚Äú*. Die Gliederung ist modular, logisch aufgebaut und erlaubt sowohl technische Tiefe als auch theoretische Reflexion.

### 1. **Einleitung**

* Motivation und Relevanz des Themas
* Zielsetzung der Arbeit
* Abgrenzung und Umfang
* Aufbau der Arbeit

---

### 2. **Theoretische Grundlagen**

* Sprachassistenten: Definition und Bestandteile
* √úberblick √ºber aktuelle Systeme (Alexa, Siri, Mycroft, etc.)
  * Vor- und Nachteile (Datenschutz, Cloudanbindung, keine Smart Home Integration wie bspw. openHAB, usw.)
* Komponenten im Detail:

  * Hotword Detection
  * Automatic Speech Recognition (ASR)
    * Erkl√§re ebenfalls am besten STT 
  * Natural Language Understanding (NLU)
    * Erkl√§re ebenfalls am besten NLP und warum NLU eine Teilmenge ist. 
  * Text-to-Speech (TTS)
* Datenschutz & Offline-Verarbeitung
* Einordnung in bestehende Arbeiten (State of the Art)

Was nicht schaden kann ist, wenn man hier noch KI-Grundlagen irgendwie erl√§utert und erkl√§rt. Man hat ja verschiedene Ber√ºhrungspunkte, weil man ja auch Modelle trainiert. Eventuell muss man ja auch Begriffe wie `Reinforcement Learning` beschreiben. Spracherkennungssysteme neigen meist zu `√úbertraining` (`overfitting`), was w√§re dies denn √ºberhaupt erst? Da kann man mit theoretische Grundlagen sehr tief ins Thema einsteigen.

---

### 3. **Anforderungsanalyse**

* Funktionale Anforderungen (z.‚ÄØB. Hotword, STT, NLU)
* Nicht-funktionale Anforderungen (z.‚ÄØB. Offline-Betrieb, Modularit√§t)
* Zielplattform (z.‚ÄØB. Raspberry Pi vs. Mini-PC)
* Auswahlkriterien f√ºr Frameworks und Komponenten
* Smart Home Integration (z. B. openHAB)

---

### 4. **Konzept und Architektur**

* Architekturentwurf des Sprachassistenten
* Beschreibung der Systemkomponenten

  * Audioaufnahme & Verarbeitung
  * Modulinteraktion
* Kommunikationsschnittstellen
* Datenflussdiagramm

---

### 5. **Implementierung**

* Projektstruktur & Technologie-Stack
* Beschreibung der wichtigsten Module:

  * Hotword-Erkennung (z.‚ÄØB. Porcupine)
  * Spracherkennung (z.‚ÄØB. Whisper/Vosk)
  * Intent-Erkennung (z.‚ÄØB. Rasa/Regex)
  * Textausgabe (z.‚ÄØB. Coqui TTS)
* Erweiterbarkeit & Custom Intents
* Beispielabl√§ufe / Interaktionen

Ich sage hier nur vielleicht. Man sollte sich nicht zu stark an diesen Libraries orientieren.

---

### 6. **Evaluation**

* Testmethodik (z.‚ÄØB. definierte Szenarien, Vergleich mit Cloudl√∂sungen)
* Messkriterien:

  * Erkennungsrate
  * Latenzzeit
  * Ressourcenverbrauch
* Vergleich von Alternativen (z.‚ÄØB. Whisper vs. Vosk)
* Diskussion der Ergebnisse

Eine Machbarkeitsstudie macht man ja so oder so allgemein. In Bezug auf "Machtbarkeit" kann man ja zeigen, ob ein ehemaliges Szenario von Alexa und openHAB sich nun mit dem neu entwickelten System umsetzen l√§sst. Wenn ja, dann hat man es ja geschafft ein "Konkurrenzprodukt" zu entwickeln, welches cloudunabh√§ngig funktioniert, somit lokal ist, datenschutztechtlich unbedenklich, usw.

---

### 7. **Reflexion und Ausblick**

* Bewertung der Zielerreichung
* Technische und konzeptionelle Herausforderungen
* Verbesserungspotenziale
* Ausblick auf m√∂gliche Erweiterungen (z.‚ÄØB. Dialogmanagement, IoT-Anbindung)

Ich denke eins wird klar sein: Alexa nutzt bspw. verschiedene Skills. Mit diesen Skills lassen sich Apps, Anwendungen, Ger√§te von sehr vielen verschiedenen Anbietern integrieren. Auch, weil Skills communitybasiert oder durch Firmen/Organistationen entwickelt werden. Hei√üt viele Bedienm√∂glichkeiten und Szenarien √ºber viele Skills/Anwendungen sind hier nat√ºrlich nicht integrierbar und am Ende nutzbar. Dies nimmt einen Nutzer auch eine gewisse Komfortabilit√§t und einen gewissen Nutzen. W√§ren aber bspw. manche Skills umsetzbar oder m√ºsste man die komplette Architektur des Prototypen umschreiben? K√∂nnte man z. B. Ger√§te von Somfy, Philips Hue oder Sonos auch direkt anbinden oder geht dies nur √ºber ein Smart Home System wie openHAB?

---

### 8. **Fazit**

* Zusammenfassung der Arbeit
* Wichtigste Erkenntnisse
* Pers√∂nliche Bewertung

---

### 9. **Anhang**

* Codeausz√ºge
* Konfigurationsdateien
* Screenshots / Ablaufdiagramme
* Testprotokolle

---

### 10. **Literaturverzeichnis**

* Fachliteratur
* Dokumentationen verwendeter Tools
* Onlinequellen mit Zugriffsdatum

---

## Beispielprojekt

Hier ist ein **Beispielprojekt f√ºr einen lokalen Sprachassistenten** mit Fokus auf Modularit√§t, Verst√§ndlichkeit und Erweiterbarkeit ‚Äì ideal als Startpunkt f√ºr deine Bachelorarbeit.

Anmerkung: Das ist vom zu entwickelten Prototypen noch meilenweit entfernt.

---

### üìÅ Projekt-Setup: `voice_assistant/`

```plaintext
voice_assistant/
‚îú‚îÄ‚îÄ main.py                       # Einstiegspunkt: orchestration der Komponenten
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ settings.yaml             # Konfiguration (Hotword, STT, NLU, TTS)
‚îú‚îÄ‚îÄ hotword/
‚îÇ   ‚îî‚îÄ‚îÄ detector.py               # Hotword-Erkennung (Porcupine / Precise)
‚îú‚îÄ‚îÄ stt/
‚îÇ   ‚îî‚îÄ‚îÄ transcriber.py            # Speech-to-Text (Whisper / Vosk)
‚îú‚îÄ‚îÄ nlu/
‚îÇ   ‚îî‚îÄ‚îÄ intent_parser.py          # Intent-Erkennung (Rasa / Regex)
‚îú‚îÄ‚îÄ tts/
‚îÇ   ‚îî‚îÄ‚îÄ synthesizer.py            # Text-to-Speech (Coqui TTS)
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îî‚îÄ‚îÄ actions.py                # Antwortlogik / Aktionshandler
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ audio_utils.py            # Mikrofonaufnahme, Audio-Konvertierung
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ intents.yaml              # Trainingsdaten f√ºr Intents
‚îî‚îÄ‚îÄ requirements.txt              # Python-Abh√§ngigkeiten
```

---

### üß† Komponenten√ºberblick mit Mini-Codebeispielen

#### 1. `main.py`

```python
from hotword.detector import wait_for_hotword
from stt.transcriber import transcribe_audio
from nlu.intent_parser import parse_intent
from tts.synthesizer import speak
from core.actions import handle_intent

def main():
    print("System bereit. Sag das Hotword...")
    while True:
        wait_for_hotword()
        audio = utils.audio_utils.record_input()
        text = transcribe_audio(audio)
        intent = parse_intent(text)
        response = handle_intent(intent)
        speak(response)

if __name__ == "__main__":
    main()
```

---

#### 2. `hotword/detector.py` (z.‚ÄØB. mit `pvporcupine`)

```python
import pvporcupine
import pyaudio

def wait_for_hotword():
    porcupine = pvporcupine.create(keywords=["computer"])
    audio_stream = pyaudio.PyAudio().open(
        rate=porcupine.sample_rate,
        channels=1,
        format=pyaudio.paInt16,
        input=True,
        frames_per_buffer=porcupine.frame_length,
    )
    while True:
        pcm = audio_stream.read(porcupine.frame_length)
        pcm = list(int.from_bytes(pcm[i:i+2], 'little', signed=True) for i in range(0, len(pcm), 2))
        if porcupine.process(pcm) >= 0:
            break
```

---

#### 3. `stt/transcriber.py` (mit `whisper`)

```python
import whisper

model = whisper.load_model("base")

def transcribe_audio(audio_path="input.wav"):
    result = model.transcribe(audio_path)
    return result["text"]
```

---

#### 4. `nlu/intent_parser.py` (Regel- oder Rasa-basiert)

```python
import yaml

with open("data/intents.yaml", "r") as f:
    intent_data = yaml.safe_load(f)

def parse_intent(text):
    text = text.lower()
    for intent, patterns in intent_data.items():
        for pattern in patterns:
            if pattern in text:
                return intent
    return "unknown"
```

Beispiel f√ºr `data/intents.yaml`:

```yaml
play_music:
  - spiel musik
  - mach musik an
  - spiele ein lied
get_time:
  - wie sp√§t ist es
  - sag mir die uhrzeit
  - wie viel uhr
```

---

#### 5. `tts/synthesizer.py` (z.‚ÄØB. mit `pyttsx3` oder `coqui-tts`)

```python
import pyttsx3

engine = pyttsx3.init()

def speak(text):
    engine.say(text)
    engine.runAndWait()
```

---

#### 6. `core/actions.py`

```python
from datetime import datetime

def handle_intent(intent):
    if intent == "get_time":
        return f"Es ist {datetime.now().strftime('%H:%M')} Uhr."
    elif intent == "play_music":
        return "Okay, ich spiele Musik ab."
    else:
        return "Das habe ich leider nicht verstanden."
```

---

### üì¶ `requirements.txt`

```txt
pvporcupine
pyaudio
whisper
torch
pyttsx3
pyyaml
```

---

### üõ†Ô∏è Was du jetzt damit machen kannst

* üîÑ Eigene Intents und Aktionen hinzuf√ºgen
* üîç Komponenten austauschen (z.‚ÄØB. Whisper ‚Üí Vosk, pyttsx3 ‚Üí Coqui TTS)
* üß™ Performance- oder Vergleichsexperimente implementieren
* üîê Datenschutz-Konzept einbauen (lokale vs. Cloud-Verarbeitung vergleichen)

---
